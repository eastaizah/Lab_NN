# Lab 02: Primera Red Neuronal

## Descripci√≥n

En este laboratorio construimos nuestra primera red neuronal completa conectando m√∫ltiples capas de neuronas. Implementamos forward propagation y exploramos diferentes arquitecturas.

## Objetivos de Aprendizaje

Al completar este laboratorio, podr√°s:

1. ‚úÖ Construir redes neuronales con m√∫ltiples capas
2. ‚úÖ Implementar forward propagation
3. ‚úÖ Entender c√≥mo fluyen los datos a trav√©s de la red
4. ‚úÖ Dise√±ar arquitecturas para diferentes problemas
5. ‚úÖ Calcular el n√∫mero de par√°metros de una red
6. ‚úÖ Comprender el problema de la linealidad

## Contenido

### üìñ Teor√≠a (`teoria.md`)

Fundamentos te√≥ricos completos:
- Arquitectura de redes neuronales
- Capas: entrada, ocultas, salida
- Forward propagation
- Dimensiones de matrices
- Inicializaci√≥n de pesos
- Redes profundas vs anchas
- El problema sin funciones de activaci√≥n

### üíª Pr√°ctica (`practica.ipynb`)

Jupyter Notebook interactivo con:
- Construcci√≥n de redes multicapa
- Experimentaci√≥n con arquitecturas
- Visualizaci√≥n de activaciones
- Ejercicios pr√°cticos
- Demostraci√≥n del problema de linealidad

### üîß C√≥digo de Ejemplo (`codigo/red_neuronal.py`)

Implementaci√≥n completa:
- Clase `CapaDensa`: Capa individual
- Clase `RedNeuronal`: Red completa
- Funci√≥n `visualizar_activaciones()`: Visualizaci√≥n de flujo de datos
- M√∫ltiples ejemplos de uso

## C√≥mo Usar Este Laboratorio

### Prerequisitos

Completa primero el [Lab 01: Introducci√≥n a las Neuronas](../Lab01_Introduccion_Neuronas/)

### Opci√≥n 1: Jupyter Notebook (Recomendado)

```bash
cd Lab02_Primera_Red_Neuronal
jupyter notebook practica.ipynb
```

### Opci√≥n 2: Script Python

```bash
python codigo/red_neuronal.py
```

### Opci√≥n 3: Estudio Guiado

1. Lee `teoria.md` para comprender los conceptos
2. Abre y ejecuta `practica.ipynb`
3. Completa los ejercicios
4. Experimenta modificando arquitecturas
5. Revisa `codigo/red_neuronal.py` como referencia

## Conceptos Clave

### Arquitectura de Red

```
[n_entrada] ‚Üí [n_capa1] ‚Üí [n_capa2] ‚Üí ... ‚Üí [n_salida]
```

**Ejemplo**: `[784, 128, 64, 10]`
- 784 caracter√≠sticas de entrada
- 128 neuronas en capa oculta 1
- 64 neuronas en capa oculta 2
- 10 clases de salida

### Forward Propagation

```python
activacion = X
for cada capa:
    activacion = capa.forward(activacion)
return activacion
```

### N√∫mero de Par√°metros

Para cada capa:
```
par√°metros = (n_entradas √ó n_neuronas) + n_neuronas
```

## Ejemplos Incluidos

### 1. Red Simple
Red b√°sica de 2 capas para entender el flujo de datos.

### 2. Red para MNIST
Arquitectura t√≠pica para clasificaci√≥n de d√≠gitos: `[784, 128, 64, 10]`

### 3. Comparaci√≥n de Arquitecturas
Diferentes configuraciones y su impacto en par√°metros.

### 4. Visualizaci√≥n
Gr√°ficos mostrando c√≥mo se transforman los datos.

### 5. Profunda vs Ancha
Comparaci√≥n de diferentes estrategias de dise√±o.

## Ejercicios

### Ejercicio 2.1: Seguimiento de Dimensiones
Traza las dimensiones de los tensores a trav√©s de la red.

### Ejercicio 2.2: Contar Par√°metros
Calcula manualmente los par√°metros de una red `[10, 20, 15, 5]`.

### Ejercicio 2.3: Dise√±o de Arquitectura (Desaf√≠o)
Dise√±a dos redes con ~10,000 par√°metros pero arquitecturas diferentes.

## Notas Importantes

‚ö†Ô∏è **Limitaci√≥n Sin Activaciones**: Sin funciones de activaci√≥n no lineales, cualquier red profunda es matem√°ticamente equivalente a una red de una sola capa.

üí° **Por qu√© Importa**:
- Capas m√∫ltiples solo son √∫tiles con no-linealidad
- En Lab 03 a√±adiremos funciones de activaci√≥n
- Entonces veremos el verdadero poder de las redes profundas

## Visualizaciones

El notebook incluye visualizaciones que muestran:
- C√≥mo cambia la dimensionalidad en cada capa
- Valores de activaci√≥n por neurona
- Comparaci√≥n de diferentes arquitecturas

## Decisiones de Dise√±o

### ¬øCu√°ntas capas?
- **Problemas simples**: 1-2 capas ocultas
- **Problemas complejos**: 3-5+ capas ocultas
- **Deep Learning**: 10-100+ capas (con t√©cnicas especiales)

### ¬øCu√°ntas neuronas por capa?
- Generalmente, disminuir hacia la salida
- Depende de la complejidad del problema
- Experimentaci√≥n es clave

### Inicializaci√≥n de Pesos
- ‚ùå No todo ceros (simetr√≠a)
- ‚ùå No valores muy grandes (gradientes explotan)
- ‚úÖ Valores peque√±os aleatorios
- ‚úÖ Xavier/He initialization (veremos en Lab 05)

## Pr√≥ximo Paso

Una vez completes este laboratorio, contin√∫a con:

üëâ **[Lab 03: Funciones de Activaci√≥n](../Lab03_Funciones_Activacion/)**

Aprenderemos sobre ReLU, Sigmoid, Tanh y Softmax, que permitir√°n a nuestras redes aprender patrones no lineales.

## Recursos Adicionales

- [Visualizing Neural Networks](http://playground.tensorflow.org/)
- [CS231n: Neural Networks Part 1](https://cs231n.github.io/neural-networks-1/)
- [Deep Learning Book - Chapter 6](https://www.deeplearningbook.org/contents/mlp.html)

## Preguntas Frecuentes

**P: ¬øM√°s capas siempre es mejor?**  
R: No necesariamente. M√°s capas pueden llevar a overfitting y son m√°s dif√≠ciles de entrenar. El balance es importante.

**P: ¬øC√≥mo s√© cu√°ntas neuronas usar?**  
R: Es parte del "arte" del ML. Se determina mediante experimentaci√≥n y validaci√≥n. Generalmente se empieza con valores est√°ndar y se ajusta.

**P: ¬øPor qu√© la red sin activaci√≥n es lineal?**  
R: Porque la composici√≥n de funciones lineales es lineal. Necesitamos no-linealidad para resolver problemas complejos.

**P: ¬øPuedo tener capas de diferentes tama√±os?**  
R: ¬°S√≠! No hay restricci√≥n. Puedes aumentar, disminuir, o mantener el tama√±o entre capas seg√∫n tu necesidad.
