# Gu√≠a de Laboratorio: Primera Red Neuronal Multicapa

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Primera Red Neuronal Multicapa  
**C√≥digo:** Lab 02  
**Duraci√≥n:** 2-3 horas  
**Nivel:** B√°sico-Intermedio  

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender arquitectura de redes neuronales multicapa
2. Implementar forward propagation desde cero
3. Dise√±ar arquitecturas para diferentes problemas
4. Calcular n√∫mero de par√°metros en una red
5. Entender flujo de datos a trav√©s de capas
6. Implementar redes usando programaci√≥n orientada a objetos
7. Visualizar activaciones y transformaciones
8. Reconocer limitaciones sin funciones de activaci√≥n
9. Aplicar buenas pr√°cticas de inicializaci√≥n

## üìö Prerrequisitos

### Conocimientos

- Completar Lab 01
- Python intermedio (clases, funciones, NumPy)
- √Ålgebra lineal b√°sica
- Comprensi√≥n de conceptos de labs anteriores

### Software

- Python 3.8+
- NumPy 1.19+
- Matplotlib 3.0+
- Jupyter Notebook (recomendado)

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` - Marco te√≥rico completo
- `README.md` - Visi√≥n general del laboratorio

## üìñ Introducci√≥n

En Lab 01 aprendimos sobre **neuronas individuales**. Ahora conectaremos m√∫ltiples neuronas en **capas** y m√∫ltiples capas en **redes neuronales** completas.

### Contexto del Problema

Las neuronas individuales tienen limitaciones:
- Solo patrones linealmente separables
- No pueden resolver XOR
- Capacidad representacional limitada

Para problemas reales necesitamos **redes neuronales profundas** con m√∫ltiples capas.

### Enfoque con Redes Neuronales

```
ENTRADA ‚Üí [CAPA OCULTA 1] ‚Üí [CAPA OCULTA 2] ‚Üí [SALIDA]
```

Arquitectura t√≠pica para MNIST:
```
[784 entradas] ‚Üí [128 neuronas] ‚Üí [64 neuronas] ‚Üí [10 salidas]
```

### Conceptos Fundamentales

**Forward Propagation:** C√°lculo secuencial de salidas capa por capa.

**Dimensiones:** Crucial entender (batch, features) @ (features, neurons) = (batch, neurons)

**Par√°metros:** Para cada capa = (n_entradas √ó n_neuronas) + n_neuronas

### Aplicaciones Pr√°cticas

- Visi√≥n por computadora (ResNet, VGG)
- PLN (BERT, GPT)
- Sistemas de recomendaci√≥n
- Diagn√≥stico m√©dico
- Trading algor√≠tmico

### Motivaci√≥n Hist√≥rica

De perceptrones simples (1958) a redes profundas modernas con billones de par√°metros.

## üî¨ Parte 1: Construyendo Tu Primera Red (45 min)

### 1.1 Dos Capas Conectadas

```python
import numpy as np

# Arquitectura: 3 ‚Üí 4 ‚Üí 2
X = np.array([[1, 2, 3]])
W1 = np.random.randn(3, 4) * 0.01
b1 = np.zeros(4)
a1 = X @ W1 + b1

W2 = np.random.randn(4, 2) * 0.01
b2 = np.zeros(2)
salida = a1 @ W2 + b2

print(f"Salida: {salida.shape}")  # (1, 2)
```

### 1.2 Red Completa para MNIST

```python
# Red [784, 128, 64, 10]
X = np.random.randn(32, 784)  # batch de 32 im√°genes

W1 = np.random.randn(784, 128) * 0.01
b1 = np.zeros(128)
a1 = X @ W1 + b1  # (32, 128)

W2 = np.random.randn(128, 64) * 0.01
b2 = np.zeros(64)
a2 = a1 @ W2 + b2  # (32, 64)

W3 = np.random.randn(64, 10) * 0.01
b3 = np.zeros(10)
salida = a2 @ W3 + b3  # (32, 10)
```

### Actividades

1. Crear red [10, 20, 15, 5] y verificar dimensiones
2. Calcular par√°metros de [784, 256, 128, 10]
3. Experimentar con diferentes batch sizes

## üî¨ Parte 2: Programaci√≥n Orientada a Objetos (45 min)

### 2.1 Clase CapaDensa

```python
class CapaDensa:
    def __init__(self, n_entradas, n_neuronas):
        self.pesos = np.random.randn(n_entradas, n_neuronas) * 0.01
        self.biases = np.zeros(n_neuronas)
    
    def forward(self, entradas):
        self.salida = entradas @ self.pesos + self.biases
        return self.salida
```

### 2.2 Clase RedNeuronal

```python
class RedNeuronal:
    def __init__(self, arquitectura):
        self.capas = []
        for i in range(len(arquitectura) - 1):
            capa = CapaDensa(arquitectura[i], arquitectura[i+1])
            self.capas.append(capa)
    
    def forward(self, X):
        activacion = X
        for capa in self.capas:
            activacion = capa.forward(activacion)
        return activacion
```

### Actividades

1. Implementar m√©todo contar_parametros()
2. Agregar m√©todo resumen()
3. Visualizar activaciones por capa

## üî¨ Parte 3: Dise√±o de Arquitecturas (40 min)

### 3.1 Arquitecturas para Problemas Diferentes

**Clasificaci√≥n Binaria:**
```python
red_spam = RedNeuronal([5000, 256, 64, 1])
```

**Clasificaci√≥n Multiclase:**
```python
red_mnist = RedNeuronal([784, 512, 256, 128, 10])
```

**Regresi√≥n:**
```python
red_precios = RedNeuronal([20, 64, 32, 16, 1])
```

### 3.2 Profundas vs Anchas

```python
profunda = RedNeuronal([100, 80, 60, 40, 20, 10])
ancha = RedNeuronal([100, 500, 10])
```

### 3.3 Limitaci√≥n Sin Activaci√≥n

```python
# Demostraci√≥n: Red de 2 capas = Red de 1 capa
h1 = X @ W1 + b1
salida = h1 @ W2 + b2

# Equivalente:
W_combinado = W1 @ W2
b_combinado = b1 @ W2 + b2
salida_equivalente = X @ W_combinado + b_combinado
```

**Conclusi√≥n:** Sin activaci√≥n no lineal, red profunda = red de 1 capa

### Actividades

1. Dise√±ar red para 50 clases con 10 caracter√≠sticas
2. Comparar n√∫mero de par√°metros de diferentes arquitecturas
3. Demostrar equivalencia matem√°tica

## üî¨ Parte 4: Aplicaciones Pr√°cticas (40 min)

### 4.1 Dataset Sint√©tico

```python
def generar_datos(n=1000, features=20, clases=5):
    X = np.random.randn(n, features)
    y = np.random.randint(0, clases, n)
    return X, y

X, y = generar_datos()
red = RedNeuronal([20, 64, 32, 5])
predicciones = red.forward(X)
```

### 4.2 Comparaci√≥n Batch Sizes

```python
import time
for batch_size in [1, 10, 50, 100]:
    start = time.time()
    _ = red.forward(X[:batch_size])
    print(f"Batch {batch_size}: {time.time()-start:.4f}s")
```

### 4.3 Estad√≠sticas de Activaciones

```python
def analizar_activaciones(red, X):
    _ = red.forward(X)
    for i, capa in enumerate(red.capas):
        print(f"Capa {i}: mean={capa.salida.mean():.4f}, "
              f"std={capa.salida.std():.4f}")
```

### Actividades

1. Generar datos sint√©ticos y hacer predicciones
2. Medir throughput con diferentes batch sizes
3. Analizar distribuci√≥n de activaciones

## üìä An√°lisis Final de Rendimiento

### Comparaci√≥n de Implementaciones

En esta secci√≥n comparar√°s diferentes enfoques de implementaci√≥n para entender las ventajas de cada uno.

**Criterios de comparaci√≥n:**
- Velocidad de ejecuci√≥n
- Uso de memoria
- Claridad del c√≥digo
- Mantenibilidad

### M√©tricas de Desempe√±o

Mide y compara:
- Tiempo de forward pass
- Escalabilidad con tama√±o de datos
- Eficiencia computacional

## üéØ EJERCICIOS PROPUESTOS

### Ejercicio 1: Dise√±o de Arquitectura (B√°sico)

Dise√±a red para diagn√≥stico m√©dico:
- 50 caracter√≠sticas de entrada
- 3 diagn√≥sticos posibles
- Calcula par√°metros totales

### Ejercicio 2: Comparaci√≥n de Arquitecturas (Intermedio)

Para MNIST, dise√±a:
- Red poco profunda (2 capas ocultas)
- Red profunda (5+ capas ocultas)
- Ambas con ~100,000 par√°metros

Compara tiempo de forward pass y distribuci√≥n de par√°metros.

### Ejercicio 3: Visualizaci√≥n Avanzada (Intermedio)

Implementa:
- Visualizaci√≥n de arquitectura como gr√°fico
- Heatmap de pesos por capa
- Distribuci√≥n de activaciones

### Ejercicio 4: Optimizaci√≥n de Inicializaci√≥n (Avanzado)

Compara:
- Todos ceros
- Random peque√±o
- Xavier
- He

Analiza varianza de activaciones en red profunda.

### Ejercicio 5: Mini Framework (Proyecto)

Crea framework con:
- Clase Layer base
- DenseLayer, ActivationLayer
- Sequential para encadenar
- M√©todos add(), forward(), summary()
- Guardado/carga de par√°metros

## üìù Entregables

### 1. C√≥digo Implementado (60%)

**Requisitos m√≠nimos:**
- Implementaciones completas y funcionales
- C√≥digo limpio y bien documentado
- Pruebas y validaci√≥n
- Manejo apropiado de errores

### 2. Notebook de Experimentaci√≥n (25%)

**Debe incluir:**
- Experimentos con diferentes configuraciones
- Visualizaciones claras
- An√°lisis de resultados
- Comparaciones y conclusiones

### 3. Reporte T√©cnico (15%)

**Secciones:**
1. Introducci√≥n y objetivos
2. Metodolog√≠a
3. Resultados experimentales
4. An√°lisis y discusi√≥n
5. Conclusiones

**Extensi√≥n:** 3-5 p√°ginas

### Formato de Entrega

```
Lab02_Entrega/
‚îú‚îÄ‚îÄ codigo/
‚îÇ   ‚îî‚îÄ‚îÄ [archivos .py]
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ experimentos.ipynb
‚îú‚îÄ‚îÄ reporte/
‚îÇ   ‚îî‚îÄ‚îÄ reporte_lab02.pdf
‚îî‚îÄ‚îÄ README.md
```

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Concebir (25%)

**Comprender el problema:**
- Identificar requisitos y restricciones
- Analizar alternativas de soluci√≥n
- Reconocer implicaciones de decisiones de dise√±o

### Dise√±ar (25%)

**Planificar soluciones:**
- Dise√±ar arquitecturas apropiadas
- Estructurar c√≥digo eficientemente
- Considerar escalabilidad y mantenibilidad

### Implementar (30%)

**Construcci√≥n:**
- C√≥digo funcional y correcto
- Implementaci√≥n eficiente
- Documentaci√≥n adecuada
- Pruebas comprehensivas

### Operar (20%)

**Validaci√≥n y an√°lisis:**
- Experimentaci√≥n sistem√°tica
- An√°lisis cr√≠tico de resultados
- Visualizaciones informativas
- Conclusiones fundamentadas

### R√∫brica Detallada

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|---------------|---------------------|-------------------|
| **Implementaci√≥n** | Impecable, eficiente, documentado | Funcional con docs | B√°sico funcional | Con errores |
| **Experimentaci√≥n** | An√°lisis profundo | Completo | B√°sico | Incompleto |
| **Documentaci√≥n** | Excelente | Buena | B√°sica | Pobre |
| **Comprensi√≥n** | Dominio total | Buen entendimiento | Comprensi√≥n b√°sica | Comprensi√≥n limitada |

## üìö Referencias Adicionales

### Libros

1. **"Deep Learning" - Goodfellow, Bengio, Courville**
   - Cap√≠tulos relevantes para este lab
   - www.deeplearningbook.org

2. **"Neural Networks and Deep Learning" - Michael Nielsen**
   - neuralnetworksanddeeplearning.com

### Recursos Online

1. **CS231n: Stanford**
   - http://cs231n.stanford.edu/

2. **3Blue1Brown: Neural Networks**
   - Videos educativos excelentes

3. **TensorFlow Playground**
   - https://playground.tensorflow.org/

### Documentaci√≥n

- NumPy: https://numpy.org/doc/
- Matplotlib: https://matplotlib.org/
- Python: https://docs.python.org/3/

## üéì Notas Finales

### Conceptos Clave para Recordar

1. **Arquitectura:** Capas de entrada, ocultas y salida
2. **Forward Propagation:** C√°lculo secuencial capa por capa
3. **Dimensiones:** (batch, features) @ (features, neurons) = (batch, neurons)
4. **Par√°metros:** Pesos y biases aprendibles
5. **Inicializaci√≥n:** Nunca ceros, usar random/Xavier/He
6. **Limitaci√≥n:** Sin activaci√≥n, red profunda = 1 capa
7. **Dise√±o:** Balance profundidad vs anchura
8. **Eficiencia:** Batch processing y vectorizaci√≥n

### Preparaci√≥n para el Siguiente Lab

**Lab 03: Funciones de Activaci√≥n**

Aprender√°s:
- ReLU, Sigmoid, Tanh, Softmax
- Por qu√© son necesarias
- C√≥mo implementarlas
- Derivadas para backpropagation

Prep√°rate repasando c√°lculo diferencial b√°sico.

### Consejos de Estudio

1. **Implementa desde cero** - No uses frameworks todav√≠a
2. **Visualiza** - Dibuja y grafica para entender
3. **Experimenta** - Prueba diferentes configuraciones
4. **Debug sistem√°ticamente** - Verifica paso a paso
5. **Documenta** - Anota hallazgos y experimentos

### Soluci√≥n de Problemas Comunes

**Errores de dimensiones:**
- Verifica shape de todas las matrices
- Usa print(variable.shape) liberalmente

**Resultados inesperados:**
- Verifica inicializaci√≥n
- Asegura reproducibilidad con seed
- Revisa cada paso del c√°lculo

**C√≥digo lento:**
- Usa vectorizaci√≥n de NumPy
- Evita loops innecesarios
- Procesa en batches

### Certificaci√≥n de Completitud

Has completado exitosamente Lab 02 cuando puedas:

- [ ] Comprender arquitectura de redes neuronales multicapa
- [ ] Implementar forward propagation desde cero
- [ ] Dise√±ar arquitecturas para diferentes problemas
- [ ] Calcular n√∫mero de par√°metros en una red
- [ ] Entender flujo de datos a trav√©s de capas
- [ ] Implementar redes usando programaci√≥n orientada a objetos
- [ ] Visualizar activaciones y transformaciones
- [ ] Reconocer limitaciones sin funciones de activaci√≥n
- [ ] Aplicar buenas pr√°cticas de inicializaci√≥n

**¬°Felicitaciones!** Contin√∫a con el siguiente laboratorio.

---

**¬øPreguntas?** Revisa teor√≠a, experimenta, y consulta referencias.

**¬°√âxito en tu aprendizaje! üöÄ**
