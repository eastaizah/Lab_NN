# Gu√≠a de Laboratorio: Primera Red Neuronal Multicapa

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Primera Red Neuronal Multicapa  
**C√≥digo:** Lab 02  
**Duraci√≥n:** 2-3 horas  
**Nivel:** B√°sico-Intermedio  

---

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender la arquitectura de redes neuronales multicapa y el papel de cada tipo de capa
2. Implementar forward propagation desde cero siguiendo el flujo de datos capa a capa
3. Dise√±ar arquitecturas apropiadas para diferentes tipos de problemas
4. Calcular el n√∫mero de par√°metros aprendibles en cualquier red
5. Entender y rastrear el flujo de dimensiones (shapes) a trav√©s de las capas
6. Implementar redes neuronales usando programaci√≥n orientada a objetos
7. Visualizar y analizar las activaciones intermedias de cada capa
8. Reconocer y demostrar las limitaciones sin funciones de activaci√≥n no lineal
9. Aplicar buenas pr√°cticas de inicializaci√≥n de pesos

---

## üìö Prerrequisitos

### Conocimientos

- **Lab 01 completado**: Neuronas individuales, producto punto, vectorizaci√≥n con NumPy
- Python intermedio: clases, m√©todos, herencia, comprensi√≥n de listas
- √Ålgebra lineal b√°sica: vectores, matrices, multiplicaci√≥n matricial
- NumPy b√°sico: arrays, operaciones matriciales, broadcasting

### Software

- Python 3.8+
- NumPy 1.19+
- Matplotlib 3.0+
- Jupyter Notebook (recomendado)

### Material de Lectura

Antes de comenzar este laboratorio:
- `teoria.md` ‚Äî Marco te√≥rico completo sobre arquitecturas de redes neuronales
- `README.md` ‚Äî Visi√≥n general del laboratorio y estructura de archivos
- Repasa la secci√≥n 2 de Lab01 (capas de neuronas y operaciones matriciales)

---

## üìñ Introducci√≥n

En el Lab 01 aprendiste a construir **neuronas individuales**: unidades computacionales que calculan una suma ponderada de sus entradas. Ahora daremos el siguiente paso natural: conectar m√∫ltiples neuronas en **capas** y m√∫ltiples capas en **redes neuronales profundas**.

### Contexto del Problema

Las neuronas individuales, aunque poderosas para operaciones simples, tienen una limitaci√≥n fundamental: solo pueden aprender **patrones linealmente separables**. En otras palabras, solo pueden resolver problemas donde los datos de distintas clases se separan con una l√≠nea recta (o hiperplano).

Esta limitaci√≥n se ilustra perfectamente con el problema **XOR**:

```
 (0,0)‚Üí0    (1,1)‚Üí0  ‚Üê Clase 0 (puntos en diagonal)
 (0,1)‚Üí1    (1,0)‚Üí1  ‚Üê Clase 1 (puntos en diagonal)
```

No existe ninguna l√≠nea recta que separe perfectamente estas dos clases. Se necesita una **red neuronal multicapa** para resolverlo.

### Enfoque con Redes Neuronales

Una red neuronal multicapa organiza las neuronas en capas conectadas:

```
DATOS           CAPA OCULTA 1    CAPA OCULTA 2    SALIDA
[x‚ÇÅ]  ‚îÄ‚îÄ‚îê       [h‚ÇÅ‚ÇÅ]           [h‚ÇÇ‚ÇÅ]            [y‚ÇÅ]
[x‚ÇÇ]  ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí   [h‚ÇÅ‚ÇÇ]  ‚îÄ‚îÄ‚Üí     [h‚ÇÇ‚ÇÇ]  ‚îÄ‚îÄ‚Üí       [y‚ÇÇ]
[x‚ÇÉ]  ‚îÄ‚îÄ‚îò       [h‚ÇÅ‚ÇÉ]           [h‚ÇÇ‚ÇÉ]            ...
                [h‚ÇÅ‚ÇÑ]
```

Arquitectura t√≠pica para clasificaci√≥n de d√≠gitos MNIST:
```
[784 p√≠xeles] ‚Üí [128 neuronas] ‚Üí [64 neuronas] ‚Üí [10 clases]
```

Cada flecha `‚Üí` representa una capa de conexiones ponderadas (pesos + biases).

### Conceptos Fundamentales

**1. Forward Propagation (Propagaci√≥n hacia adelante):**

El proceso de calcular la salida de la red, capa por capa:

$$\mathbf{z}^{(l)} = \mathbf{a}^{(l-1)} \cdot \mathbf{W}^{(l)} + \mathbf{b}^{(l)}$$

$$\mathbf{a}^{(l)} = f\left(\mathbf{z}^{(l)}\right)$$

Donde:
- $\mathbf{a}^{(0)} = \mathbf{X}$ (datos de entrada)
- $\mathbf{W}^{(l)}$ es la matriz de pesos de la capa $l$
- $\mathbf{b}^{(l)}$ es el vector de biases de la capa $l$
- $f$ es la funci√≥n de activaci√≥n (en este lab, identidad)

**2. Dimensiones de Tensores:**

Para un batch de $N$ muestras con $d$ caracter√≠sticas:

| Tensor | Shape | Descripci√≥n |
|--------|-------|-------------|
| X (entrada) | $(N, d)$ | N muestras, d caracter√≠sticas cada una |
| W (pesos) | $(d_{in}, d_{out})$ | Conexiones entre capas |
| b (biases) | $(d_{out},)$ | Un bias por neurona |
| a (activaci√≥n) | $(N, d_{out})$ | N muestras, $d_{out}$ activaciones |

**Regla fundamental**: Si la entrada tiene shape $(N, m)$ y los pesos son $(m, k)$, la salida tiene shape $(N, k)$.

**3. N√∫mero de Par√°metros:**

Para cada capa densa:

$$\text{par√°metros} = (n_{entradas} \times n_{neuronas}) + n_{neuronas}$$

Para la red MNIST completa:
- Capa 1: $(784 \times 128) + 128 = 100{,}480$
- Capa 2: $(128 \times 64) + 64 = 8{,}256$
- Capa 3: $(64 \times 10) + 10 = 650$
- **Total: 109,386 par√°metros aprendibles**

### Aplicaciones Pr√°cticas

Las redes neuronales multicapa son la base de:
- **Visi√≥n por computadora**: ResNet, VGG, EfficientNet (reconocimiento de im√°genes)
- **Procesamiento de lenguaje**: BERT, GPT (traducci√≥n, resumen, chatbots)
- **Sistemas de recomendaci√≥n**: Netflix, Spotify, YouTube
- **Diagn√≥stico m√©dico**: detecci√≥n de tumores, clasificaci√≥n de radiograf√≠as
- **Finanzas**: predicci√≥n de mercados, detecci√≥n de fraude

### Motivaci√≥n Hist√≥rica

El perceptr√≥n simple de Rosenblatt (1958) era una sola neurona. En 1969, Minsky y Papert demostraron matem√°ticamente que no pod√≠a resolver XOR. Esto provoc√≥ el primer "invierno de la IA". En los 80s, Rumelhart et al. desarrollaron el algoritmo de backpropagation, que permiti√≥ entrenar redes multicapa y superar las limitaciones del perceptr√≥n simple ‚Äî dando inicio a la era del deep learning.

---

## üî¨ Parte 1: Construyendo Tu Primera Red Multicapa (35 min)

### 1.1 Introducci√≥n Conceptual: ¬øC√≥mo se conectan las capas?

**¬øQu√© hacemos?** Conectar m√∫ltiples capas de neuronas de forma secuencial.

**¬øPor qu√© lo hacemos?** Cada capa transforma la representaci√≥n de los datos. Las capas tempranas aprenden caracter√≠sticas simples (bordes, colores) y las capas posteriores combinan estas en conceptos complejos (formas, objetos).

**Analog√≠a:** Imagina un equipo de an√°lisis de texto:
- **Capa 1** (analista de palabras): identifica palabras individuales
- **Capa 2** (analista de frases): agrupa palabras en frases con significado
- **Capa 3** (analista de sentimiento): determina si el mensaje es positivo o negativo

Cada nivel depende del nivel anterior y agrega una nueva capa de comprensi√≥n.

**¬øQu√© resultados esperar?** Un tensor de salida cuyo shape depende de la arquitectura definida.

### 1.2 Dos Capas Conectadas Manualmente

Empecemos con lo m√°s b√°sico: dos capas conectadas sin clases:

```python
import numpy as np

# Arquitectura: 3 entradas ‚Üí 4 neuronas ocultas ‚Üí 2 salidas
print("=" * 50)
print("RED DE DOS CAPAS: 3 ‚Üí 4 ‚Üí 2")
print("=" * 50)

# Datos de entrada (1 muestra con 3 caracter√≠sticas)
X = np.array([[1.0, 2.0, 3.0]])  # Shape: (1, 3)

# Capa 1: 3 entradas ‚Üí 4 neuronas
W1 = np.random.randn(3, 4) * 0.01  # Shape: (3, 4)
b1 = np.zeros(4)                    # Shape: (4,)
a1 = X @ W1 + b1                   # Shape: (1, 4)

print(f"\nüìê Capa 1:")
print(f"   Entrada X: {X.shape}")
print(f"   Pesos W1: {W1.shape}")
print(f"   Salida a1: {a1.shape}")

# Capa 2: 4 neuronas ‚Üí 2 salidas
W2 = np.random.randn(4, 2) * 0.01  # Shape: (4, 2)
b2 = np.zeros(2)                    # Shape: (2,)
salida = a1 @ W2 + b2              # Shape: (1, 2)

print(f"\nüìê Capa 2:")
print(f"   Entrada a1: {a1.shape}")
print(f"   Pesos W2: {W2.shape}")
print(f"   Salida final: {salida.shape}")
print(f"\nüî¢ Resultado: {salida}")
```

**Actividad 1.1**: Ejecuta el c√≥digo anterior. Verifica manualmente que las shapes de cada operaci√≥n son correctas. ¬øQu√© ocurre si intentas usar W1 con shape `(4, 3)` en vez de `(3, 4)`?

**Actividad 1.2**: Modifica el c√≥digo para una arquitectura `[5, 8, 6, 3]`. Traza las shapes en cada paso.

### 1.3 Red Completa para MNIST

Ahora implementemos la arquitectura cl√°sica para clasificar d√≠gitos:

```python
# Red [784, 128, 64, 10] ‚Äî Arquitectura para MNIST
np.random.seed(42)  # Reproducibilidad

print("=" * 60)
print("RED NEURONAL PARA MNIST: [784, 128, 64, 10]")
print("=" * 60)

# Simular un batch de 32 im√°genes 28x28 aplanadas
X = np.random.randn(32, 784)  # Shape: (32, 784)

# Capa 1: 784 ‚Üí 128
W1 = np.random.randn(784, 128) * 0.01
b1 = np.zeros(128)
a1 = X @ W1 + b1  # Shape: (32, 128)

# Capa 2: 128 ‚Üí 64
W2 = np.random.randn(128, 64) * 0.01
b2 = np.zeros(64)
a2 = a1 @ W2 + b2  # Shape: (32, 64)

# Capa 3: 64 ‚Üí 10
W3 = np.random.randn(64, 10) * 0.01
b3 = np.zeros(10)
salida = a2 @ W3 + b3  # Shape: (32, 10)

print(f"\n{'Tensor':<15} {'Shape':<15} {'Descripci√≥n'}")
print("-" * 55)
print(f"{'X':<15} {str(X.shape):<15} {'32 im√°genes, 784 p√≠xeles cada una'}")
print(f"{'a1':<15} {str(a1.shape):<15} {'32 activaciones de 128 neuronas'}")
print(f"{'a2':<15} {str(a2.shape):<15} {'32 activaciones de 64 neuronas'}")
print(f"{'salida':<15} {str(salida.shape):<15} {'32 vectores de 10 scores de clase'}")

# Contar par√°metros
params_c1 = 784 * 128 + 128
params_c2 = 128 * 64 + 64
params_c3 = 64 * 10 + 10
print(f"\nüìä Par√°metros:")
print(f"   Capa 1: {params_c1:,}")
print(f"   Capa 2: {params_c2:,}")
print(f"   Capa 3: {params_c3:,}")
print(f"   TOTAL:  {params_c1 + params_c2 + params_c3:,}")
```

**Actividad 1.3**: Crea y traza la red `[10, 20, 15, 5]`. Verifica dimensiones paso a paso.

**Actividad 1.4**: Calcula manualmente el n√∫mero de par√°metros de `[784, 256, 128, 10]` y verifica con c√≥digo.

**Actividad 1.5**: Experimenta con diferentes batch sizes (1, 8, 32, 64). ¬øCambia el n√∫mero de par√°metros?

**Actividad 1.6**: ¬øQu√© sucede si el batch size es 1? Verifica que la red funciona igual para una sola muestra.

### Preguntas de Reflexi√≥n

**Pregunta 1.1 (Concebir):** ¬øPor qu√© conectamos capas en secuencia en lugar de conectar todas las neuronas directamente a la salida?

**Pregunta 1.2 (Dise√±ar):** Para un problema con 100 caracter√≠sticas de entrada y 5 clases de salida, ¬øc√≥mo dise√±ar√≠as la arquitectura? ¬øQu√© factores considerar√≠as?

**Pregunta 1.3 (Implementar):** ¬øPor qu√© la shape de los pesos W entre dos capas debe ser `(n_capa_anterior, n_capa_siguiente)` y no al rev√©s?

**Pregunta 1.4 (Operar):** Si la red tiene 32 millones de par√°metros, ¬øcu√°nta memoria RAM necesita solo para almacenar los pesos (en MB), asumiendo float32 (4 bytes por n√∫mero)?

---

## üî¨ Parte 2: Programaci√≥n Orientada a Objetos (40 min)

### 2.1 Introducci√≥n Conceptual: ¬øPor qu√© usar clases?

**¬øQu√© hacemos?** Encapsular la l√≥gica de capas y redes en clases reutilizables.

**¬øPor qu√© lo hacemos?** El c√≥digo procedimental (como en la Parte 1) se vuelve inmanejable para redes grandes. Las clases permiten:
- **Encapsulamiento**: cada capa maneja sus propios par√°metros
- **Reutilizaci√≥n**: crear cualquier arquitectura con las mismas clases
- **Mantenibilidad**: modificar una capa sin afectar el resto
- **Extensibilidad**: agregar nuevas funcionalidades f√°cilmente

**Analog√≠a:** Piensa en construir con bloques LEGO. Cada `CapaDensa` es un tipo de bloque estandarizado que puedes apilar en cualquier configuraci√≥n, y la `RedNeuronal` es el conjunto ensamblado.

**¬øQu√© resultados esperar?** Clases que generalizan el proceso de forward propagation para cualquier arquitectura.

### 2.2 Clase CapaDensa

```python
import numpy as np

class CapaDensa:
    """
    Capa densa (fully connected) de neuronas artificiales.
    
    Una capa densa conecta cada neurona de la capa anterior
    con cada neurona de esta capa a trav√©s de pesos aprendibles.
    
    Args:
        n_entradas: N√∫mero de caracter√≠sticas de entrada
        n_neuronas: N√∫mero de neuronas en esta capa
        seed: Semilla aleatoria para reproducibilidad
    """
    
    def __init__(self, n_entradas, n_neuronas, seed=None):
        if seed is not None:
            np.random.seed(seed)
        
        # Validaciones
        assert n_entradas > 0, "n_entradas debe ser positivo"
        assert n_neuronas > 0, "n_neuronas debe ser positivo"
        
        self.n_entradas = n_entradas
        self.n_neuronas = n_neuronas
        
        # Inicializaci√≥n de pesos: valores peque√±os aleatorios
        # Multiplicamos por 0.01 para evitar saturaci√≥n en activaciones
        self.pesos = np.random.randn(n_entradas, n_neuronas) * 0.01
        self.biases = np.zeros(n_neuronas)
        
        # Almac√©n para forward pass (√∫til para debugging y backprop)
        self.entradas = None
        self.salida = None
        
        print(f"‚úÖ CapaDensa creada: {n_entradas} ‚Üí {n_neuronas} "
              f"({self.contar_parametros():,} par√°metros)")
    
    def forward(self, entradas):
        """
        Propagaci√≥n hacia adelante.
        
        Calcula: salida = entradas @ pesos + biases
        
        Args:
            entradas: Array (batch_size, n_entradas)
        
        Returns:
            salida: Array (batch_size, n_neuronas)
        """
        # Validar dimensiones de entrada
        assert entradas.shape[1] == self.n_entradas, \
            f"Shape esperado: (batch, {self.n_entradas}), recibido: {entradas.shape}"
        
        self.entradas = entradas
        self.salida = np.dot(entradas, self.pesos) + self.biases
        return self.salida
    
    def contar_parametros(self):
        """Retorna el n√∫mero total de par√°metros aprendibles."""
        return self.pesos.size + self.biases.size
    
    def resumen(self):
        """Imprime informaci√≥n detallada de la capa."""
        print(f"\nüìã Capa Densa:")
        print(f"   Forma de pesos: {self.pesos.shape}")
        print(f"   Forma de biases: {self.biases.shape}")
        print(f"   Par√°metros totales: {self.contar_parametros():,}")
        print(f"   Media de pesos: {self.pesos.mean():.6f}")
        print(f"   Std de pesos: {self.pesos.std():.6f}")
    
    def __repr__(self):
        return f"CapaDensa({self.n_entradas} ‚Üí {self.n_neuronas})"


# Ejemplo de uso
capa = CapaDensa(784, 128, seed=42)
capa.resumen()

# Procesar un batch
X = np.random.randn(32, 784)
salida = capa.forward(X)
print(f"\nüîÑ Forward pass: {X.shape} ‚Üí {salida.shape}")
```

**Actividad 2.1**: Crea una capa con 10 entradas y 5 neuronas. ¬øCu√°ntos par√°metros tiene? Verifica con `contar_parametros()`.

**Actividad 2.2**: Modifica la clase para que `forward()` tambi√©n imprima las estad√≠sticas de salida (media, std). ¬øCambia esto la funcionalidad principal?

### 2.3 Clase RedNeuronal

```python
class RedNeuronal:
    """
    Red neuronal multicapa con arquitectura flexible.
    
    Implementa una red fully-connected donde el usuario
    especifica el n√∫mero de neuronas en cada capa.
    
    Args:
        arquitectura: Lista con n√∫mero de neuronas por capa
                      ej: [784, 128, 64, 10]
    """
    
    def __init__(self, arquitectura, seed=None):
        assert len(arquitectura) >= 2, "Necesitas al menos entrada y salida"
        
        self.arquitectura = arquitectura
        self.capas = []
        
        # Crear capas densas entre cada par de dimensiones adyacentes
        for i in range(len(arquitectura) - 1):
            n_in = arquitectura[i]
            n_out = arquitectura[i + 1]
            capa = CapaDensa(n_in, n_out, seed=seed)
            self.capas.append(capa)
        
        print(f"\nüèóÔ∏è  Red Neuronal creada:")
        print(f"   Arquitectura: {arquitectura}")
        print(f"   Capas: {len(self.capas)}")
        print(f"   Total par√°metros: {self.contar_parametros():,}")
    
    def forward(self, X):
        """
        Forward propagation a trav√©s de todas las capas.
        
        Procesa los datos secuencialmente capa por capa.
        
        Args:
            X: Datos de entrada (batch_size, n_entrada)
        
        Returns:
            activacion: Salida final (batch_size, n_salida)
        """
        activacion = X
        for capa in self.capas:
            activacion = capa.forward(activacion)
        return activacion
    
    def contar_parametros(self):
        """Cuenta todos los par√°metros de la red."""
        return sum(capa.contar_parametros() for capa in self.capas)
    
    def resumen(self):
        """Imprime la arquitectura completa de la red."""
        print("\n" + "=" * 60)
        print("RESUMEN DE LA RED NEURONAL")
        print("=" * 60)
        total = 0
        for i, capa in enumerate(self.capas):
            params = capa.contar_parametros()
            total += params
            print(f"  Capa {i+1}: {capa.n_entradas:5d} ‚Üí {capa.n_neuronas:5d} "
                  f"| {params:10,} par√°metros")
        print("-" * 60)
        print(f"  TOTAL:                        {total:10,} par√°metros")
        print("=" * 60)
    
    def analizar_activaciones(self, X):
        """Analiza estad√≠sticas de activaciones por capa."""
        print("\nüìä An√°lisis de Activaciones:")
        activacion = X
        for i, capa in enumerate(self.capas):
            activacion = capa.forward(activacion)
            print(f"  Capa {i+1}: mean={activacion.mean():.4f}, "
                  f"std={activacion.std():.4f}, "
                  f"min={activacion.min():.4f}, "
                  f"max={activacion.max():.4f}")


# Ejemplo de uso
red = RedNeuronal([784, 128, 64, 10], seed=42)
red.resumen()

X = np.random.randn(32, 784)
salida = red.forward(X)
print(f"\nüî¢ Salida final: {salida.shape}")

red.analizar_activaciones(X)
```

**Actividad 2.3**: Crea redes con las siguientes arquitecturas y compara su n√∫mero de par√°metros:
   - `[100, 50, 10]`
   - `[100, 200, 50, 10]`
   - `[100, 500, 10]`

**Actividad 2.4**: Implementa un m√©todo `get_activaciones_intermedias(X)` que retorne las activaciones de cada capa (no solo la final).

**Actividad 2.5**: Agrega un m√©todo `guardar_pesos(filepath)` y `cargar_pesos(filepath)` usando `np.save` y `np.load`.

**Actividad 2.6**: Crea una funci√≥n `test_red(arquitectura)` que verifique que la red produce las shapes correctas con un batch de 16 muestras.

### Preguntas de Reflexi√≥n

**Pregunta 2.1 (Concebir):** ¬øCu√°l es la ventaja de definir la arquitectura como una lista `[784, 128, 64, 10]` en lugar de crear cada capa manualmente?

**Pregunta 2.2 (Dise√±ar):** En el m√©todo `forward()`, guardamos el estado intermedio de cada capa. ¬øPor qu√© esto es importante para el entrenamiento (aunque a√∫n no lo implementemos)?

**Pregunta 2.3 (Implementar):** ¬øPor qu√© el loop `for capa in self.capas` en `forward()` es correcto para conectar capas en secuencia? Traza mentalmente el flujo de datos.

**Pregunta 2.4 (Operar):** Si necesitas procesar 1 mill√≥n de im√°genes, ¬øpor qu√© es m√°s eficiente procesar en batches de 64 que de una en una?

---

## üî¨ Parte 3: Inicializaci√≥n y Sus Efectos (35 min)

### 3.1 Introducci√≥n Conceptual: ¬øPor qu√© importa la inicializaci√≥n?

**¬øQu√© hacemos?** Estudiar diferentes estrategias para inicializar los pesos de la red.

**¬øPor qu√© lo hacemos?** La inicializaci√≥n de pesos determina el punto de partida del entrenamiento. Una mala inicializaci√≥n puede:
- Hacer que todas las neuronas aprendan lo mismo (**problema de simetr√≠a**)
- Causar que las se√±ales se desvanezcan o exploten al propagarse (**gradientes inestables**)
- Ralentizar enormemente el entrenamiento o impedir la convergencia

**Analog√≠a:** Si quieres explorar un laberinto, ¬øprefieres empezar en el centro (buena inicializaci√≥n) o pegado a una pared (mala inicializaci√≥n)? El punto de partida afecta cu√°nto tardar√°s en encontrar la salida.

**¬øQu√© resultados esperar?** Distribuciones de activaciones diferentes seg√∫n la estrategia de inicializaci√≥n. La buena inicializaci√≥n mantiene la varianza estable entre capas.

### 3.2 El Problema de los Ceros

```python
def demostrar_problema_simetria():
    """Demuestra por qu√© inicializar en cero es problem√°tico."""
    
    print("=" * 60)
    print("PROBLEMA DE SIMETR√çA CON PESOS EN CERO")
    print("=" * 60)
    
    # Red con todos los pesos en cero
    X = np.random.randn(5, 3)
    W_cero = np.zeros((3, 4))
    b_cero = np.zeros(4)
    
    salida_cero = X @ W_cero + b_cero
    
    print("\n‚ùå Con pesos en cero:")
    print(f"   Todas las salidas son cero: {np.all(salida_cero == 0)}")
    print(f"   Salidas √∫nicas: {np.unique(salida_cero)}")
    print(f"   ‚Üí Ninguna neurona aprende caracter√≠sticas diferentes")
    
    # Red con pesos aleatorios peque√±os
    W_rand = np.random.randn(3, 4) * 0.01
    salida_rand = X @ W_rand + b_cero
    
    print("\n‚úÖ Con pesos aleatorios peque√±os:")
    print(f"   Media: {salida_rand.mean():.6f}")
    print(f"   Std: {salida_rand.std():.6f}")
    print(f"   ‚Üí Cada neurona produce valores distintos")

demostrar_problema_simetria()
```

### 3.3 Comparaci√≥n de Estrategias de Inicializaci√≥n

```python
def comparar_inicializaciones(n_entradas=100, n_neuronas=100, n_capas=5):
    """
    Compara c√≥mo distintas inicializaciones afectan la varianza
    de activaciones en redes profundas.
    """
    import matplotlib.pyplot as plt
    
    X = np.random.randn(1000, n_entradas)
    
    estrategias = {
        'Muy peque√±os (√ó0.001)': lambda n, m: np.random.randn(n, m) * 0.001,
        'Peque√±os (√ó0.01)':      lambda n, m: np.random.randn(n, m) * 0.01,
        'Xavier/Glorot':         lambda n, m: np.random.randn(n, m) * np.sqrt(1.0/n),
        'He (para ReLU)':        lambda n, m: np.random.randn(n, m) * np.sqrt(2.0/n),
    }
    
    print("=" * 65)
    print("COMPARACI√ìN DE ESTRATEGIAS DE INICIALIZACI√ìN")
    print(f"Red: [{n_entradas}] √ó {n_capas} capas de {n_neuronas}")
    print("=" * 65)
    print(f"\n{'Estrategia':<25} | " + " | ".join([f"Capa{i+1:2d}" for i in range(n_capas)]))
    print("-" * 65)
    
    for nombre, init_fn in estrategias.items():
        activacion = X.copy()
        stds = []
        
        for _ in range(n_capas):
            W = init_fn(activacion.shape[1], n_neuronas)
            b = np.zeros(n_neuronas)
            activacion = activacion @ W + b
            stds.append(activacion.std())
        
        std_str = " | ".join([f"{s:6.4f}" for s in stds])
        print(f"{nombre:<25} | {std_str}")
    
    print("\nüí° Interpretaci√≥n:")
    print("   - Muy peque√±os: varianza se desvanece ‚Üí neuronas inactivas")
    print("   - Muy grandes: varianza explota ‚Üí gradientes inestables")
    print("   - Xavier: mantiene varianza estable para activaciones lineales/tanh")
    print("   - He: mantiene varianza estable para ReLU")

comparar_inicializaciones()
```

**Actividad 3.1**: Ejecuta `comparar_inicializaciones()` y anota cu√°l estrategia mantiene la varianza m√°s estable entre capas.

**Actividad 3.2**: Modifica la funci√≥n para probar con 10 capas en lugar de 5. ¬øQu√© le ocurre a la varianza con la inicializaci√≥n muy peque√±a?

**Actividad 3.3**: Implementa la inicializaci√≥n **Glorot Uniforme**: `W = uniform(-‚àö(6/(n+m)), ‚àö(6/(n+m)))`. Compara con Xavier Gaussiano.

**Actividad 3.4**: Verifica que dos redes con la misma `seed` producen exactamente las mismas salidas.

### Preguntas de Reflexi√≥n

**Pregunta 3.1 (Concebir):** ¬øPor qu√© el "problema de simetr√≠a" impide que una red con pesos iguales aprenda caracter√≠sticas diversas?

**Pregunta 3.2 (Dise√±ar):** Si sabes que usar√°s ReLU como activaci√≥n (pr√≥ximo lab), ¬øqu√© inicializaci√≥n elegir√≠as y por qu√©?

**Pregunta 3.3 (Implementar):** ¬øPor qu√© multiplicamos los pesos por `sqrt(2/n)` en la inicializaci√≥n He en lugar de simplemente usar `0.01`?

**Pregunta 3.4 (Operar):** En producci√≥n, ¬øpor qu√© es importante fijar una `seed` aleatoria antes de inicializar una red?

---

## üî¨ Parte 4: Dise√±o de Arquitecturas (35 min)

### 4.1 Introducci√≥n Conceptual: ¬øC√≥mo dise√±ar una red?

**¬øQu√© hacemos?** Dise√±ar arquitecturas de redes neuronales apropiadas para diferentes tipos de problemas.

**¬øPor qu√© lo hacemos?** No existe una arquitectura "perfecta" universal. El dise√±o depende de:
- N√∫mero y tipo de caracter√≠sticas de entrada
- Tipo de problema (clasificaci√≥n binaria, multiclase, regresi√≥n)
- Cantidad de datos disponibles
- Restricciones de tiempo y memoria

**Reglas pr√°cticas de dise√±o:**
1. El n√∫mero de neuronas de entrada = n√∫mero de caracter√≠sticas
2. El n√∫mero de neuronas de salida depende del problema
3. Las capas ocultas generalmente se reducen gradualmente hacia la salida
4. M√°s capas = m√°s capacidad, pero tambi√©n m√°s dif√≠cil de entrenar

**¬øQu√© resultados esperar?** Arquitecturas funcionales con conteo de par√°metros verificado.

### 4.2 Arquitecturas para Diferentes Problemas

```python
# Problema 1: Clasificaci√≥n Binaria (spam/no-spam)
# Entrada: 5000 features (bag of words)
# Salida: 1 probabilidad (spam o no)
red_spam = RedNeuronal([5000, 256, 64, 1])
red_spam.resumen()

# Problema 2: Clasificaci√≥n Multiclase (MNIST: 10 d√≠gitos)
# Entrada: 784 p√≠xeles
# Salida: 10 scores de clase
red_mnist = RedNeuronal([784, 512, 256, 128, 10])
red_mnist.resumen()

# Problema 3: Regresi√≥n (predicci√≥n de precios)
# Entrada: 20 caracter√≠sticas de la casa
# Salida: 1 valor continuo (precio)
red_precios = RedNeuronal([20, 64, 32, 16, 1])
red_precios.resumen()

# Problema 4: Clasificaci√≥n de emociones (5 clases)
# Entrada: 1000 features de audio
# Salida: 5 probabilidades de emoci√≥n
red_emociones = RedNeuronal([1000, 256, 128, 64, 5])
red_emociones.resumen()
```

**Actividad 4.1**: Dise√±a una arquitectura para clasificar 50 tipos de flores con 30 caracter√≠sticas cada una. Justifica tu elecci√≥n.

### 4.3 Redes Profundas vs. Anchas

```python
def comparar_profunda_vs_ancha():
    """
    Compara el n√∫mero de par√°metros de redes profundas vs anchas
    con similar capacidad.
    """
    print("=" * 60)
    print("REDES PROFUNDAS vs ANCHAS")
    print("=" * 60)
    
    arquitecturas = {
        "Muy profunda":  [100, 80, 60, 40, 20, 10, 5],
        "Profunda":      [100, 64, 32, 16, 5],
        "Equilibrada":   [100, 200, 100, 5],
        "Ancha":         [100, 500, 5],
        "Muy ancha":     [100, 1000, 5],
    }
    
    print(f"\n{'Nombre':<15} {'Arquitectura':<35} {'Par√°metros':>12}")
    print("-" * 65)
    
    for nombre, arq in arquitecturas.items():
        red = RedNeuronal(arq)
        params = red.contar_parametros()
        arq_str = " ‚Üí ".join(map(str, arq))
        print(f"{nombre:<15} {arq_str:<35} {params:>12,}")

comparar_profunda_vs_ancha()
```

**Actividad 4.2**: Dise√±a dos redes con aproximadamente el mismo n√∫mero de par√°metros (~50,000) pero arquitecturas muy distintas (una profunda, una ancha). Compara sus tiempos de forward pass.

### 4.4 La Limitaci√≥n Matem√°tica Sin Activaci√≥n

Esta es una de las demostraciones m√°s importantes del laboratorio:

```python
def demostrar_colapso_lineal():
    """
    Demuestra matem√°ticamente que una red sin activaciones
    no lineales se reduce a una sola transformaci√≥n lineal.
    """
    print("=" * 60)
    print("DEMOSTRACI√ìN: RED PROFUNDA = RED DE 1 CAPA (sin activaci√≥n)")
    print("=" * 60)
    
    np.random.seed(42)
    X = np.random.randn(5, 3)
    
    # Red de 2 capas sin activaci√≥n
    W1 = np.random.randn(3, 4) * 0.1
    b1 = np.random.randn(4) * 0.1
    W2 = np.random.randn(4, 2) * 0.1
    b2 = np.random.randn(2) * 0.1
    
    # Forward pass con 2 capas
    h1 = X @ W1 + b1          # Capa 1
    salida_2capas = h1 @ W2 + b2  # Capa 2
    
    # Equivalente matem√°tico (1 sola transformaci√≥n lineal):
    # h1 @ W2 + b2
    # = (X @ W1 + b1) @ W2 + b2
    # = X @ W1 @ W2 + b1 @ W2 + b2
    W_equivalente = W1 @ W2
    b_equivalente = b1 @ W2 + b2
    salida_1capa = X @ W_equivalente + b_equivalente
    
    print(f"\nüìä Red de 2 capas (W1={W1.shape}, W2={W2.shape}):")
    print(f"   Par√°metros: {W1.size + b1.size + W2.size + b2.size}")
    
    print(f"\nüìä Equivalente de 1 capa (W={W_equivalente.shape}):")
    print(f"   Par√°metros: {W_equivalente.size + b_equivalente.size}")
    
    print(f"\n‚úÖ ¬øSon id√©nticas las salidas?")
    son_iguales = np.allclose(salida_2capas, salida_1capa)
    print(f"   np.allclose(salida_2capas, salida_1capa) = {son_iguales}")
    
    print("\n‚ö†Ô∏è  CONCLUSI√ìN FUNDAMENTAL:")
    print("   Sin activaci√≥n no lineal, una red profunda es ID√âNTICA")
    print("   a una red de 1 sola capa. Las capas adicionales no")
    print("   aportan capacidad representacional adicional.")
    print("   ‚Üí Por esto necesitamos funciones de activaci√≥n (Lab 03)!")

demostrar_colapso_lineal()
```

**Actividad 4.3**: Extiende la demostraci√≥n a 3 capas lineales. ¬øPuedes reducirlas a 1 capa equivalente?

**Actividad 4.4**: Dise√±a una red para el problema XOR y verifica (con la demostraci√≥n) que sin activaci√≥n no puede resolverlo.

**Actividad 4.5**: Investiga el **Teorema de Aproximaci√≥n Universal**: ¬øcu√°ntas neuronas ocultas necesita una red de 1 capa para aproximar cualquier funci√≥n continua?

### Preguntas de Reflexi√≥n

**Pregunta 4.1 (Concebir):** Si las redes sin activaci√≥n son equivalentes a transformaciones lineales, ¬øcu√°l es la utilidad de estudiarlas en este laboratorio?

**Pregunta 4.2 (Dise√±ar):** ¬øCu√°ndo preferir√≠as una red ancha sobre una profunda? ¬øHay ventajas computacionales?

**Pregunta 4.3 (Implementar):** En la demostraci√≥n del colapso lineal, ¬øpor qu√© la red de 2 capas tiene M√ÅS par√°metros que la equivalente de 1 capa pero hace lo mismo?

**Pregunta 4.4 (Operar):** En producci√≥n, si entrenas una red sin activaciones y observas que no mejora, ¬øc√≥mo diagnosticar√≠as si el problema es la falta de no-linealidad?

---

## üî¨ Parte 5: Aplicaciones Pr√°cticas y Visualizaci√≥n (35 min)

### 5.1 Introducci√≥n Conceptual: Visualizando el Flujo de Datos

**¬øQu√© hacemos?** Analizar c√≥mo los datos se transforman al pasar por cada capa de la red.

**¬øPor qu√© lo hacemos?** Entender las transformaciones intermedias es fundamental para:
- Diagnosticar problemas en el entrenamiento
- Verificar que la red est√° aprendiendo representaciones √∫tiles
- Detectar problemas de inicializaci√≥n (activaciones en cero o saturadas)
- Interpretar qu√© "aprende" cada capa

**¬øQu√© resultados esperar?** Gr√°ficas que muestran c√≥mo cambia la distribuci√≥n de activaciones capa por capa.

### 5.2 Visualizaci√≥n de Activaciones

```python
import matplotlib.pyplot as plt

def visualizar_transformaciones(red, X, titulo="Transformaciones por Capa"):
    """
    Visualiza c√≥mo se transforman los datos en cada capa de la red.
    
    Args:
        red: Instancia de RedNeuronal
        X: Datos de entrada (batch_size, n_entradas)
        titulo: T√≠tulo del gr√°fico
    """
    n_capas = len(red.capas)
    fig, axes = plt.subplots(1, n_capas + 1, figsize=(4 * (n_capas + 1), 4))
    
    # Graficar entrada
    axes[0].hist(X.ravel(), bins=50, edgecolor='black', alpha=0.7, color='steelblue')
    axes[0].set_title("Entrada\n" + f"shape={X.shape}", fontsize=10)
    axes[0].set_xlabel("Valor de activaci√≥n")
    axes[0].set_ylabel("Frecuencia")
    axes[0].grid(True, alpha=0.3)
    
    # Graficar activaciones por capa
    activacion = X
    for i, capa in enumerate(red.capas):
        activacion = capa.forward(activacion)
        axes[i+1].hist(activacion.ravel(), bins=50, edgecolor='black', 
                       alpha=0.7, color='darkorange')
        axes[i+1].set_title(
            f"Capa {i+1}\nshape={activacion.shape}\n"
            f"Œº={activacion.mean():.3f}, œÉ={activacion.std():.3f}",
            fontsize=9
        )
        axes[i+1].set_xlabel("Valor de activaci√≥n")
        axes[i+1].grid(True, alpha=0.3)
    
    plt.suptitle(titulo, fontsize=13, fontweight='bold')
    plt.tight_layout()
    plt.savefig('transformaciones_capas.png', dpi=100, bbox_inches='tight')
    plt.show()
    print("‚úÖ Gr√°fico guardado como 'transformaciones_capas.png'")


# Ejemplo
np.random.seed(42)
red = RedNeuronal([784, 256, 128, 64, 10])
X = np.random.randn(500, 784)
visualizar_transformaciones(red, X)
```

### 5.3 Dataset Sint√©tico y An√°lisis

```python
def experimento_datos_sinteticos():
    """
    Genera datos sint√©ticos y analiza el comportamiento de la red.
    """
    def generar_datos(n=1000, features=20, clases=5, seed=42):
        np.random.seed(seed)
        X = np.random.randn(n, features)
        y = np.random.randint(0, clases, n)
        return X, y
    
    X, y = generar_datos()
    red = RedNeuronal([20, 64, 32, 5])
    
    predicciones = red.forward(X)
    
    print("=" * 55)
    print("EXPERIMENTO CON DATOS SINT√âTICOS")
    print("=" * 55)
    print(f"Datos: {X.shape} | Clases: {np.unique(y)}")
    print(f"Predicciones (sin entrenar): {predicciones.shape}")
    print(f"\nEstad√≠sticas de predicciones:")
    for clase in range(5):
        print(f"  Clase {clase}: media={predicciones[:, clase].mean():.4f}")
    
    # An√°lisis de batch processing
    import time
    print("\n‚è±Ô∏è  An√°lisis de batch processing:")
    for batch_size in [1, 10, 50, 100, 500, 1000]:
        start = time.time()
        for _ in range(100):  # 100 pasadas
            _ = red.forward(X[:batch_size])
        elapsed = (time.time() - start) / 100
        throughput = batch_size / elapsed
        print(f"  Batch {batch_size:5d}: {elapsed*1000:7.3f} ms/pasada "
              f"| {throughput:,.0f} muestras/seg")

experimento_datos_sinteticos()
```

**Actividad 5.1**: Ejecuta `visualizar_transformaciones()` con diferentes arquitecturas. ¬øCambia la distribuci√≥n de activaciones?

**Actividad 5.2**: Modifica la funci√≥n de visualizaci√≥n para mostrar un heatmap de la matriz de pesos de cada capa.

**Actividad 5.3**: Crea un experimento que compare el tiempo de forward pass de una red ancha vs una profunda con el mismo n√∫mero de par√°metros.

**Actividad 5.4**: Implementa una funci√≥n que detecte si alguna capa tiene activaciones con varianza cercana a cero (posible problema de inicializaci√≥n).

### Preguntas de Reflexi√≥n

**Pregunta 5.1 (Concebir):** ¬øQu√© informaci√≥n te proporciona la distribuci√≥n de activaciones de una capa?

**Pregunta 5.2 (Dise√±ar):** Si todas las activaciones de una capa son pr√°cticamente cero, ¬øqu√© problema podr√≠a estar ocurriendo y c√≥mo lo corregir√≠as?

**Pregunta 5.3 (Implementar):** ¬øPor qu√© es importante analizar las activaciones ANTES de entrenar la red?

**Pregunta 5.4 (Operar):** En un sistema en producci√≥n, ¬øqu√© m√©tricas monitorear√≠as durante el forward pass para detectar problemas?

---

## üìä An√°lisis Final de Rendimiento

### Benchmark de Implementaciones

En esta secci√≥n medir√°s el rendimiento de diferentes enfoques de implementaci√≥n para entender las ventajas de la vectorizaci√≥n con NumPy.

**Fundamento:** La multiplicaci√≥n matricial vectorizada de NumPy aprovecha bibliotecas BLAS/LAPACK optimizadas en C/Fortran y puede usar instrucciones SIMD del procesador, siendo √≥rdenes de magnitud m√°s r√°pida que loops en Python puro.

```python
import time
import numpy as np

def benchmark_implementaciones():
    """
    Compara el rendimiento de diferentes implementaciones
    de forward pass.
    """
    print("\n" + "=" * 65)
    print("BENCHMARK: COMPARACI√ìN DE IMPLEMENTACIONES")
    print("=" * 65)
    
    configuraciones = [
        (100, 50, 500),
        (784, 128, 1000),
        (1000, 256, 2000),
    ]
    
    for n_entrada, n_neuronas, batch_size in configuraciones:
        print(f"\nüìê Config: {n_entrada}‚Üí{n_neuronas}, batch={batch_size}")
        
        X = np.random.randn(batch_size, n_entrada)
        W = np.random.randn(n_entrada, n_neuronas) * 0.01
        b = np.zeros(n_neuronas)
        
        # M√©todo 1: NumPy @ operator (vectorizado)
        start = time.perf_counter()
        for _ in range(100):
            Y1 = X @ W + b
        t_numpy = (time.perf_counter() - start) / 100
        
        # M√©todo 2: np.dot (vectorizado)
        start = time.perf_counter()
        for _ in range(100):
            Y2 = np.dot(X, W) + b
        t_dot = (time.perf_counter() - start) / 100
        
        # M√©todo 3: Loop por muestras (no vectorizado)
        if batch_size <= 500:
            start = time.perf_counter()
            for _ in range(10):
                Y3 = np.array([np.dot(X[i], W) + b for i in range(batch_size)])
            t_loop = (time.perf_counter() - start) / 10
        else:
            Y3, t_loop = Y1, None
        
        print(f"   @ operator:    {t_numpy*1000:.4f} ms")
        print(f"   np.dot:        {t_dot*1000:.4f} ms")
        if t_loop:
            print(f"   Loop Python:   {t_loop*1000:.4f} ms")
            print(f"   üöÄ Aceleraci√≥n: {t_loop/t_numpy:.1f}x m√°s r√°pido con vectorizaci√≥n")
        
        assert np.allclose(Y1, Y2), "¬°Los resultados no coinciden!"

benchmark_implementaciones()
```

### An√°lisis de Escalabilidad

```python
def analizar_escalabilidad_red():
    """
    Analiza c√≥mo escala el costo computacional con el tama√±o de la red.
    """
    import time
    import matplotlib.pyplot as plt
    
    print("\n" + "=" * 55)
    print("AN√ÅLISIS DE ESCALABILIDAD")
    print("=" * 55)
    
    batch_size = 128
    n_neuronas_lista = [16, 32, 64, 128, 256, 512, 1024]
    tiempos = []
    
    print(f"\n{'N. Neuronas':<15} {'Tiempo(ms)':<15} {'Par√°metros'}")
    print("-" * 45)
    
    for n in n_neuronas_lista:
        red = RedNeuronal([n, n, n, n])  # 3 capas de n neuronas
        X = np.random.randn(batch_size, n)
        
        # Medir tiempo
        start = time.perf_counter()
        for _ in range(200):
            _ = red.forward(X)
        elapsed = (time.perf_counter() - start) / 200
        tiempos.append(elapsed)
        
        print(f"{n:<15} {elapsed*1000:<15.3f} {red.contar_parametros():,}")
    
    # Visualizar
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    ax1.plot(n_neuronas_lista, np.array(tiempos)*1000, 'o-', 
             linewidth=2, markersize=8, color='steelblue')
    ax1.set_xlabel('N√∫mero de neuronas por capa')
    ax1.set_ylabel('Tiempo por forward pass (ms)')
    ax1.set_title('Tiempo de Forward Pass vs. Tama√±o de Capa')
    ax1.grid(True, alpha=0.3)
    
    ax2.loglog(n_neuronas_lista, np.array(tiempos)*1000, 'o-',
               linewidth=2, markersize=8, color='darkorange')
    ax2.set_xlabel('N√∫mero de neuronas por capa (escala log)')
    ax2.set_ylabel('Tiempo (ms, escala log)')
    ax2.set_title('Escalabilidad (log-log)')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('escalabilidad_red.png', dpi=100, bbox_inches='tight')
    plt.show()
    
    print("\nüí° Complejidad: O(N¬≤) por capa (N = n√∫mero de neuronas)")
    print("   El costo crece cuadr√°ticamente con el tama√±o de la red")

analizar_escalabilidad_red()
```

---

## üéØ EJERCICIOS PROPUESTOS

### Ejercicio 1: Seguimiento de Dimensiones (B√°sico)

**Objetivo:** Consolidar el entendimiento de c√≥mo fluyen los datos.

**Tareas:**
1. Para la red `[5, 8, 6, 3]`, traza manualmente las shapes de cada tensor
2. Verifica con c√≥digo que las shapes son correctas
3. Calcula el n√∫mero total de par√°metros a mano y verifica con `contar_parametros()`

```python
# Esqueleto de soluci√≥n
def analizar_dimensiones_red(arquitectura, batch_size=4):
    """
    Traza las dimensiones de todos los tensores en la red.
    
    Args:
        arquitectura: Lista con neuronas por capa ej: [5, 8, 6, 3]
        batch_size: N√∫mero de muestras en el batch
    """
    print(f"Red: {arquitectura}, Batch size: {batch_size}")
    print("-" * 50)
    
    # Tu c√≥digo aqu√≠: crear la red y analizar shapes
    # Pista: usa red.capas para acceder a las capas
    pass

analizar_dimensiones_red([5, 8, 6, 3], batch_size=4)
```

### Ejercicio 2: Diagn√≥stico de Red (Intermedio)

**Objetivo:** Implementar herramientas de diagn√≥stico que analicen el estado de una red.

**Tareas:**
1. Implementa `diagnostico_red(red, X)` que detecte:
   - Capas con activaciones en cero o casi cero (std < 0.001)
   - Capas con activaciones explosivas (std > 100)
   - Porcentaje de activaciones negativas
2. Prueba con redes con diferentes inicializaciones

```python
def diagnostico_red(red, X):
    """
    Analiza la salud de la red detectando problemas comunes.
    
    Returns:
        dict: Diccionario con estad√≠sticas por capa y alertas
    """
    # Tu c√≥digo aqu√≠
    pass

# Prueba con diferentes inicializaciones
# ¬øQu√© problemas detecta el diagn√≥stico?
```

### Ejercicio 3: Visualizaci√≥n de Arquitectura (Intermedio)

**Objetivo:** Crear una visualizaci√≥n gr√°fica de la arquitectura de la red.

**Tareas:**
1. Implementa `graficar_arquitectura(arquitectura)` que dibuje:
   - Cada capa como una columna de c√≠rculos
   - Las conexiones entre capas (representativas, no todas)
   - El n√∫mero de par√°metros por capa
2. Usa matplotlib para la visualizaci√≥n

```python
import matplotlib.pyplot as plt
import matplotlib.patches as patches

def graficar_arquitectura(arquitectura, max_neuronas_mostradas=8):
    """
    Visualiza la arquitectura de una red neuronal.
    
    Args:
        arquitectura: Lista con n√∫mero de neuronas por capa
        max_neuronas_mostradas: M√°ximo de neuronas a dibujar por capa
    """
    fig, ax = plt.subplots(1, 1, figsize=(14, 8))
    
    # Tu c√≥digo aqu√≠
    # Pista: usa ax.add_patch(patches.Circle(...)) para las neuronas
    # y ax.plot(...) para las conexiones
    pass

graficar_arquitectura([784, 128, 64, 10])
```

### Ejercicio 4: Comparaci√≥n de Inicializaciones en Profundidad (Avanzado)

**Objetivo:** Estudiar emp√≠ricamente el efecto de la inicializaci√≥n en redes muy profundas.

**Tareas:**
1. Crea una red de 20 capas (red muy profunda)
2. Inicializa con 4 estrategias: zeros, tiny (√ó0.001), Xavier, He
3. Para cada inicializaci√≥n, grafica la varianza de activaciones por capa
4. Identifica qu√© estrategias causan **vanishing gradients** y cu√°les causan **exploding gradients**

```python
def estudio_profundidad_inicializacion(n_capas=20, n_neuronas=100):
    """
    Estudia el efecto de la inicializaci√≥n en redes profundas.
    Grafica la varianza de activaciones por capa para cada estrategia.
    """
    import matplotlib.pyplot as plt
    
    estrategias = {
        'Zeros':   lambda n, m: np.zeros((n, m)),
        'Tiny':    lambda n, m: np.random.randn(n, m) * 0.001,
        'Xavier':  lambda n, m: np.random.randn(n, m) * np.sqrt(1.0/n),
        'He':      lambda n, m: np.random.randn(n, m) * np.sqrt(2.0/n),
    }
    
    # Tu c√≥digo aqu√≠
    pass

estudio_profundidad_inicializacion()
```

### Ejercicio 5: Mini Framework de Redes Neuronales (Proyecto)

**Objetivo:** Construir un mini-framework extensible para redes neuronales.

**Tareas:**
1. Crea una clase base `Capa` con m√©todos abstractos `forward()` y `contar_parametros()`
2. Implementa `CapaDensa` y `CapaActivacion` (con funci√≥n identidad por ahora)
3. Implementa `Secuencial` que agrupe capas con `add()`, `forward()`, `resumen()`
4. Agrega soporte para guardar y cargar el modelo completo en un archivo JSON/numpy

```python
class Capa:
    """Clase base abstracta para todas las capas."""
    def forward(self, X):
        raise NotImplementedError
    
    def contar_parametros(self):
        raise NotImplementedError


class Secuencial:
    """
    Contenedor de capas que permite construir redes secuencialmente.
    Uso:
        modelo = Secuencial()
        modelo.add(CapaDensa(784, 128))
        modelo.add(CapaDensa(128, 10))
        salida = modelo.forward(X)
    """
    def __init__(self):
        self.capas = []
    
    def add(self, capa):
        # Tu c√≥digo aqu√≠
        pass
    
    def forward(self, X):
        # Tu c√≥digo aqu√≠
        pass
    
    def resumen(self):
        # Tu c√≥digo aqu√≠
        pass
    
    def guardar(self, filepath):
        # Tu c√≥digo aqu√≠ (usar np.savez)
        pass
    
    def cargar(self, filepath):
        # Tu c√≥digo aqu√≠
        pass
```

---

## üìù Entregables

### 1. C√≥digo Implementado (60%)

**Requisitos m√≠nimos:**
- Clase `CapaDensa` completa con docstrings, validaciones y m√©todo `resumen()`
- Clase `RedNeuronal` con `forward()`, `resumen()`, `contar_parametros()`, y `analizar_activaciones()`
- Al menos 2 ejercicios propuestos implementados y documentados
- Tests que verifiquen shapes correctas y reproducibilidad con seed

**Criterios de calidad:**
- C√≥digo limpio, PEP8, con comentarios explicativos
- Manejo apropiado de errores (`assert`, mensajes descriptivos)
- Funciones con docstrings completos (Args, Returns)

### 2. Notebook de Experimentaci√≥n (25%)

**Debe incluir:**
- Todas las actividades de las partes 1-5 completadas y ejecutadas
- Visualizaciones claras (histogramas de activaciones, comparaci√≥n de arquitecturas)
- An√°lisis comentado de los resultados de cada actividad
- Respuestas escritas a todas las Preguntas de Reflexi√≥n
- Experimentos adicionales creativos (m√≠nimo 2)

### 3. Reporte T√©cnico (15%)

**Secciones requeridas:**
1. Introducci√≥n: objetivo del laboratorio y contexto
2. Marco te√≥rico: conceptos clave (forward propagation, inicializaci√≥n, par√°metros)
3. Metodolog√≠a: qu√© experimentos realizaste y c√≥mo
4. Resultados: tablas y gr√°ficas de experimentos
5. An√°lisis y discusi√≥n: interpretaci√≥n de resultados
6. Conclusiones: aprendizajes clave y limitaciones encontradas

**Extensi√≥n:** 3-5 p√°ginas, formato PDF

### Formato de Entrega

```
Lab02_Entrega_NombreApellido/
‚îú‚îÄ‚îÄ codigo/
‚îÇ   ‚îú‚îÄ‚îÄ red_neuronal.py     # Clases principales
‚îÇ   ‚îú‚îÄ‚îÄ utils.py            # Funciones auxiliares
‚îÇ   ‚îî‚îÄ‚îÄ tests.py            # Tests unitarios
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ experimentos.ipynb
‚îú‚îÄ‚îÄ reporte/
‚îÇ   ‚îî‚îÄ‚îÄ reporte_lab02.pdf
‚îî‚îÄ‚îÄ README.md               # Instrucciones de ejecuci√≥n
```

---

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Concebir (25%)

**Comprensi√≥n conceptual:**
- ‚úÖ Explica por qu√© se necesitan m√∫ltiples capas para problemas complejos
- ‚úÖ Comprende el Teorema de Aproximaci√≥n Universal
- ‚úÖ Identifica cu√°ndo una red sin activaci√≥n es insuficiente
- ‚úÖ Propone arquitecturas adecuadas para problemas dados

**Evidencia:** Respuestas a preguntas de reflexi√≥n, introducci√≥n del reporte

### Dise√±ar (25%)

**Planificaci√≥n de soluciones:**
- ‚úÖ Dise√±a arquitecturas apropiadas para diferentes problemas
- ‚úÖ Justifica elecciones de n√∫mero de capas y neuronas
- ‚úÖ Planifica experimentos significativos con hip√≥tesis claras
- ‚úÖ Considera trade-offs profundidad vs. anchura

**Evidencia:** Ejercicios 1-4, secci√≥n de metodolog√≠a del reporte

### Implementar (30%)

**Construcci√≥n:**
- ‚úÖ Clases `CapaDensa` y `RedNeuronal` funcionales y correctas
- ‚úÖ Forward propagation implementado eficientemente
- ‚úÖ C√≥digo limpio, documentado, con manejo de errores
- ‚úÖ Tests unitarios que verifican comportamiento correcto

**Evidencia:** C√≥digo fuente, notebook ejecutado sin errores

### Operar (20%)

**Validaci√≥n y an√°lisis:**
- ‚úÖ Ejecuta experimentos de benchmarking y escalabilidad
- ‚úÖ Analiza e interpreta distribuciones de activaciones
- ‚úÖ Diagnostica problemas de inicializaci√≥n
- ‚úÖ Extrae conclusiones fundamentadas de los experimentos

**Evidencia:** Notebook de experimentos, secci√≥n de resultados del reporte

### R√∫brica Detallada

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|----------------|----------------------|-------------------|
| **Implementaci√≥n** | C√≥digo impecable, eficiente, bien documentado, con tests | C√≥digo funcional con documentaci√≥n b√°sica | C√≥digo funcional con errores menores | C√≥digo con errores o incompleto |
| **Experimentaci√≥n** | An√°lisis profundo y creativo, hip√≥tesis y conclusiones | Experimentos completos requeridos | Experimentos b√°sicos | Experimentos incompletos |
| **Comprensi√≥n te√≥rica** | Dominio total, conexiones con otros conceptos | Buen entendimiento, aplica correctamente | Comprensi√≥n b√°sica | Comprensi√≥n limitada o incorrecta |
| **Documentaci√≥n** | Excelente: clara, profesional, completa | Buena: completa y entendible | B√°sica: presente pero incompleta | Pobre o ausente |

---

## üìö Referencias Adicionales

### Libros

1. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep Learning*
   - Cap√≠tulo 6: Deep Feedforward Networks (arquitecturas multicapa)
   - Disponible gratuitamente en: http://www.deeplearningbook.org

2. **Nielsen, M.** (2015). *Neural Networks and Deep Learning*
   - Cap√≠tulo 1: Using neural nets to recognize handwritten digits
   - Disponible en: http://neuralnetworksanddeeplearning.com

3. **Chollet, F.** (2021). *Deep Learning with Python* (2nd ed.)
   - Cap√≠tulo 2-3: Fundamentos de redes neuronales
   - Manning Publications

### Art√≠culos Acad√©micos

1. **Cybenko, G.** (1989). "Approximation by superpositions of a sigmoidal function"
   - Prueba original del Teorema de Aproximaci√≥n Universal
   - *Mathematics of Control, Signals and Systems*, 2(4), 303-314

2. **Glorot, X., & Bengio, Y.** (2010). "Understanding the difficulty of training deep feedforward neural networks"
   - Introduce la inicializaci√≥n Xavier/Glorot
   - *Proceedings of AISTATS*, 249-256

3. **He, K., Zhang, X., Ren, S., & Sun, J.** (2015). "Delving deep into rectifiers"
   - Introduce la inicializaci√≥n He para ReLU
   - *Proceedings of ICCV*

### Recursos Online

1. **3Blue1Brown ‚Äî "Neural Networks" series**
   - Visualizaciones excepcionales de forward propagation
   - https://www.youtube.com/watch?v=aircAruvnKk

2. **Stanford CS231n ‚Äî Neural Networks Part 1**
   - Notas completas sobre arquitecturas y forward pass
   - https://cs231n.github.io/neural-networks-1/

3. **Deep Learning Book ‚Äî Chapter 6**
   - Formulaci√≥n matem√°tica rigurosa
   - https://www.deeplearningbook.org/contents/mlp.html

### Tutoriales Interactivos

1. **TensorFlow Playground**
   - Experimenta con arquitecturas en el navegador
   - https://playground.tensorflow.org

2. **Neural Network Visualizer**
   - Visualizaci√≥n interactiva de forward pass
   - https://adamharley.com/nn_vis/

### Documentaci√≥n T√©cnica

- **NumPy**: https://numpy.org/doc/ ‚Äî Referencia completa de operaciones matriciales
- **Matplotlib**: https://matplotlib.org/ ‚Äî Gu√≠a de visualizaciones
- **Python**: https://docs.python.org/3/ ‚Äî Programaci√≥n orientada a objetos

---

## üéì Notas Finales

### Conceptos Clave para Recordar

1. **Red Neuronal = Capas de Neuronas Conectadas en Secuencia**
   - Cada capa transforma la representaci√≥n de los datos
   - Las capas profundas aprenden caracter√≠sticas m√°s abstractas

2. **Forward Propagation: $a^{(l)} = f(a^{(l-1)} \cdot W^{(l)} + b^{(l)})$**
   - C√°lculo secuencial desde entrada hasta salida
   - La salida de cada capa es la entrada de la siguiente

3. **Dimensiones: $(N, m) \cdot (m, k) = (N, k)$**
   - La dimensi√≥n compartida entre entrada y pesos DEBE coincidir
   - Siempre verifica shapes antes de entrenar

4. **Par√°metros: $(n_{in} \times n_{out}) + n_{out}$ por capa**
   - Redes m√°s grandes tienen mayor capacidad representacional
   - Pero tambi√©n requieren m√°s datos para entrenar

5. **Inicializaci√≥n: NUNCA todo ceros**
   - Usar Xavier para activaciones tanh/lineal
   - Usar He para activaciones ReLU
   - Siempre usar `seed` para reproducibilidad

6. **Limitaci√≥n fundamental: Sin activaci√≥n, red profunda = red lineal**
   - La no-linealidad es imprescindible
   - Lab 03 introduce las funciones de activaci√≥n

7. **Dise√±o: Balance profundidad vs. anchura**
   - Profunda: mejor generalizaci√≥n, m√°s dif√≠cil de entrenar
   - Ancha: m√°s par√°metros, puede ser suficiente para algunos problemas

8. **Eficiencia: Batch processing y vectorizaci√≥n NumPy**
   - Procesar en batches es √≥rdenes de magnitud m√°s eficiente
   - Nunca usar loops de Python para operaciones matriciales

### Preparaci√≥n para el Siguiente Lab

**Lab 03: Funciones de Activaci√≥n** introducir√° la no-linealidad que hace que las redes profundas sean verdaderamente poderosas.

Aprender√°s:
- **ReLU**: `max(0, x)` ‚Äî el est√°ndar para capas ocultas
- **Sigmoid**: `1/(1+e^(-x))` ‚Äî para clasificaci√≥n binaria
- **Tanh**: `tanh(x)` ‚Äî activaci√≥n centrada en cero
- **Softmax**: normalizaci√≥n para clasificaci√≥n multiclase
- Derivadas de cada funci√≥n (necesarias para backpropagation)

**Para prepararte:**
1. Repasa c√°lculo diferencial: derivadas de funciones compuestas
2. Practica graficando funciones matem√°ticas con Matplotlib
3. Investiga qu√© es el "problema del gradiente que desaparece"
4. Reflexiona: ¬øqu√© pasar√≠a si todo `max(0,x)` hace al gradiente ser 0 o 1?

### Consejos de Estudio

1. **Implementa desde cero**: No uses TensorFlow/PyTorch en este lab
2. **Verifica siempre shapes**: `print(tensor.shape)` antes de cada operaci√≥n
3. **Visualiza constantemente**: Histogramas y heatmaps revelan mucho
4. **Experimenta**: Cambia arquitecturas, seeds, batch sizes
5. **Documenta hallazgos**: Toma notas de qu√© funcion√≥ y qu√© no
6. **Debug paso a paso**: Verifica intermedios antes de continuar
7. **Compara implementaciones**: Aseg√∫rate que vectorizada y loop dan lo mismo

### Soluci√≥n de Problemas Comunes

**Problema: `ValueError: matmul: Input operand 1 has a mismatch in its core dimension`**
- **Causa**: Shapes incompatibles en multiplicaci√≥n matricial
- **Diagn√≥stico**: `print(X.shape, W.shape)` antes de la operaci√≥n
- **Soluci√≥n**: Verificar que el n√∫mero de columnas de X = filas de W

**Problema: Todas las activaciones son cero**
- **Causa**: Inicializaci√≥n con ceros o valores muy peque√±os
- **Diagn√≥stico**: Verificar std de pesos con `capa.pesos.std()`
- **Soluci√≥n**: Usar inicializaci√≥n aleatoria (Xavier o He)

**Problema: Activaciones crecen exponencialmente por capa**
- **Causa**: Pesos inicializados con valores muy grandes
- **Diagn√≥stico**: `red.analizar_activaciones(X)` ‚Äî ver std por capa
- **Soluci√≥n**: Reducir escala de inicializaci√≥n o usar Xavier/He

**Problema: C√≥digo muy lento al procesar datos grandes**
- **Causa**: Loops de Python en lugar de vectorizaci√≥n NumPy
- **Diagn√≥stico**: Usar `time.perf_counter()` para medir operaciones
- **Soluci√≥n**: Reemplazar loops con operaciones matriciales `@` o `np.dot`

**Problema: Resultados no reproducibles**
- **Causa**: Falta de semilla aleatoria fija
- **Soluci√≥n**: `np.random.seed(42)` al inicio del script o `seed` en constructores

### Comunidad y Soporte

- **Foro del curso**: Para dudas conceptuales y t√©cnicas
- **Horas de oficina**: Para revisi√≥n personalizada de c√≥digo
- **Grupo de estudio**: Trabaja los ejercicios propuestos con compa√±eros
- **Stack Overflow**: Para errores espec√≠ficos de Python/NumPy

### Certificaci√≥n de Completitud

Has completado exitosamente el Lab 02 cuando puedas:

- [ ] Explicar qu√© es forward propagation y c√≥mo fluyen los datos
- [ ] Implementar `CapaDensa` y `RedNeuronal` desde cero sin consultar el material
- [ ] Calcular el n√∫mero de par√°metros de cualquier arquitectura
- [ ] Rastrear correctamente las shapes de tensores en una red
- [ ] Demostrar matem√°ticamente el colapso lineal sin activaciones
- [ ] Dise√±ar arquitecturas apropiadas para clasificaci√≥n y regresi√≥n
- [ ] Comparar estrategias de inicializaci√≥n y justificar cu√°l usar
- [ ] Interpretar histogramas de activaciones para diagnosticar problemas
- [ ] Medir y comparar el rendimiento de diferentes implementaciones

---

**¬°Felicitaciones por completar el Lab 02!** Ahora tienes los fundamentos para construir cualquier arquitectura de red neuronal feedforward.

**Siguiente parada**: Lab 03 ‚Äî Funciones de Activaci√≥n üöÄ

---

*Versi√≥n: 2.0 | Actualizado: 2024 | Licencia: MIT ‚Äî Uso educativo*
