{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Primera Red Neuronal - Práctica\n",
    "\n",
    "En este notebook construiremos nuestra primera red neuronal completa desde cero, conectando múltiples capas de neuronas.\n",
    "\n",
    "## Objetivos\n",
    "1. Construir una red neuronal con múltiples capas\n",
    "2. Entender el flujo de datos (forward propagation)\n",
    "3. Trabajar con diferentes arquitecturas\n",
    "4. Visualizar activaciones de las capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Red de Dos Capas\n",
    "\n",
    "Empezaremos con una red simple: Entrada → Capa Oculta → Salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 1: Red neuronal de 2 capas\n",
    "\n",
    "# Entrada: 3 muestras, cada una con 4 características\n",
    "X = np.array([\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "print(f\"Entrada X shape: {X.shape}\")\n",
    "print(f\"X:\\n{X}\\n\")\n",
    "\n",
    "# Capa 1: 4 entradas → 5 neuronas\n",
    "weights1 = np.random.randn(4, 5) * 0.01\n",
    "biases1 = np.zeros(5)\n",
    "\n",
    "print(f\"Capa 1 - Pesos shape: {weights1.shape}\")\n",
    "print(f\"Capa 1 - Biases shape: {biases1.shape}\\n\")\n",
    "\n",
    "# Forward pass capa 1\n",
    "layer1_output = np.dot(X, weights1) + biases1\n",
    "print(f\"Salida Capa 1 shape: {layer1_output.shape}\")\n",
    "print(f\"Salida Capa 1:\\n{layer1_output}\\n\")\n",
    "\n",
    "# Capa 2: 5 entradas → 2 neuronas (salida)\n",
    "weights2 = np.random.randn(5, 2) * 0.01\n",
    "biases2 = np.zeros(2)\n",
    "\n",
    "print(f\"Capa 2 - Pesos shape: {weights2.shape}\")\n",
    "print(f\"Capa 2 - Biases shape: {biases2.shape}\\n\")\n",
    "\n",
    "# Forward pass capa 2\n",
    "layer2_output = np.dot(layer1_output, weights2) + biases2\n",
    "print(f\"Salida Final shape: {layer2_output.shape}\")\n",
    "print(f\"Salida Final:\\n{layer2_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.1\n",
    "Observa las dimensiones en cada paso. ¿Cómo se transforman los datos a medida que pasan por la red?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Clase RedNeuronal\n",
    "\n",
    "Ahora creemos una clase flexible que pueda manejar cualquier arquitectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    \"\"\"Red neuronal multicapa simple.\"\"\"\n",
    "    \n",
    "    def __init__(self, arquitectura):\n",
    "        \"\"\"\n",
    "        Inicializa la red neuronal.\n",
    "        \n",
    "        Args:\n",
    "            arquitectura: lista con el número de neuronas por capa\n",
    "                         ej: [4, 5, 3, 2] significa:\n",
    "                         - 4 características de entrada\n",
    "                         - capa oculta con 5 neuronas\n",
    "                         - capa oculta con 3 neuronas  \n",
    "                         - capa de salida con 2 neuronas\n",
    "        \"\"\"\n",
    "        self.arquitectura = arquitectura\n",
    "        self.num_capas = len(arquitectura) - 1\n",
    "        self.capas = []\n",
    "        \n",
    "        # Inicializar pesos y biases para cada capa\n",
    "        for i in range(self.num_capas):\n",
    "            n_entradas = arquitectura[i]\n",
    "            n_neuronas = arquitectura[i + 1]\n",
    "            \n",
    "            capa = {\n",
    "                'pesos': np.random.randn(n_entradas, n_neuronas) * 0.01,\n",
    "                'biases': np.zeros(n_neuronas),\n",
    "                'salida': None  # Guardaremos las activaciones\n",
    "            }\n",
    "            self.capas.append(capa)\n",
    "        \n",
    "        print(f\"Red creada: {arquitectura}\")\n",
    "        print(f\"Número de capas: {self.num_capas}\")\n",
    "        self._mostrar_parametros()\n",
    "    \n",
    "    def _mostrar_parametros(self):\n",
    "        \"\"\"Muestra información sobre los parámetros de la red.\"\"\"\n",
    "        total_params = 0\n",
    "        for i, capa in enumerate(self.capas):\n",
    "            params = capa['pesos'].size + capa['biases'].size\n",
    "            total_params += params\n",
    "            print(f\"  Capa {i+1}: {params} parámetros\")\n",
    "        print(f\"  Total: {total_params} parámetros\\n\")\n",
    "    \n",
    "    def forward(self, X, guardar_activaciones=False):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante.\n",
    "        \n",
    "        Args:\n",
    "            X: datos de entrada (batch_size, n_features)\n",
    "            guardar_activaciones: si True, guarda las salidas intermedias\n",
    "        \n",
    "        Returns:\n",
    "            salida final de la red\n",
    "        \"\"\"\n",
    "        activacion = X\n",
    "        \n",
    "        for i, capa in enumerate(self.capas):\n",
    "            # Operación lineal\n",
    "            z = np.dot(activacion, capa['pesos']) + capa['biases']\n",
    "            \n",
    "            # Por ahora, sin función de activación (identidad)\n",
    "            activacion = z\n",
    "            \n",
    "            if guardar_activaciones:\n",
    "                capa['salida'] = activacion\n",
    "        \n",
    "        return activacion\n",
    "    \n",
    "    def obtener_activaciones(self):\n",
    "        \"\"\"Retorna las activaciones guardadas de cada capa.\"\"\"\n",
    "        return [capa['salida'] for capa in self.capas if capa['salida'] is not None]\n",
    "\n",
    "# Probamos nuestra clase\n",
    "red = RedNeuronal([4, 8, 3])\n",
    "\n",
    "# Datos de prueba\n",
    "X_test = np.array([\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0]\n",
    "])\n",
    "\n",
    "salida = red.forward(X_test)\n",
    "print(f\"Entrada shape: {X_test.shape}\")\n",
    "print(f\"Salida shape: {salida.shape}\")\n",
    "print(f\"Salida:\\n{salida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2\n",
    "Crea una red con arquitectura [10, 20, 15, 5] y calcula cuántos parámetros tiene en total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Experimentando con Diferentes Arquitecturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos sintéticos\n",
    "np.random.seed(0)\n",
    "X_data = np.random.randn(100, 10)  # 100 muestras, 10 características\n",
    "\n",
    "print(\"Probando diferentes arquitecturas:\\n\")\n",
    "\n",
    "arquitecturas = [\n",
    "    [10, 5, 1],           # Red pequeña\n",
    "    [10, 20, 10, 1],      # Red mediana\n",
    "    [10, 50, 30, 10, 1],  # Red profunda\n",
    "]\n",
    "\n",
    "for arq in arquitecturas:\n",
    "    print(f\"\\nArquitectura: {arq}\")\n",
    "    red = RedNeuronal(arq)\n",
    "    salida = red.forward(X_data)\n",
    "    print(f\"Salida shape: {salida.shape}\")\n",
    "    print(f\"Ejemplo de salida (primeras 3): {salida[:3].flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Visualización de Activaciones\n",
    "\n",
    "Veamos cómo se transforman los datos a medida que pasan por la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una red y guardamos activaciones\n",
    "red_visual = RedNeuronal([10, 8, 6, 4, 2])\n",
    "\n",
    "# Generamos una muestra\n",
    "X_sample = np.random.randn(1, 10)\n",
    "\n",
    "# Forward pass guardando activaciones\n",
    "salida = red_visual.forward(X_sample, guardar_activaciones=True)\n",
    "activaciones = red_visual.obtener_activaciones()\n",
    "\n",
    "# Visualización\n",
    "fig, axes = plt.subplots(1, len(activaciones) + 1, figsize=(15, 3))\n",
    "\n",
    "# Entrada\n",
    "axes[0].bar(range(len(X_sample[0])), X_sample[0])\n",
    "axes[0].set_title('Entrada')\n",
    "axes[0].set_xlabel('Característica')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cada capa\n",
    "for i, act in enumerate(activaciones):\n",
    "    axes[i+1].bar(range(len(act[0])), act[0])\n",
    "    axes[i+1].set_title(f'Capa {i+1}')\n",
    "    axes[i+1].set_xlabel('Neurona')\n",
    "    axes[i+1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Activaciones a través de la red', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observa cómo la dimensionalidad cambia en cada capa:\")\n",
    "print(f\"Entrada: {X_sample.shape}\")\n",
    "for i, act in enumerate(activaciones):\n",
    "    print(f\"Capa {i+1}: {act.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Red para Clasificación de Dígitos (MNIST)\n",
    "\n",
    "Ahora diseñemos una red apropiada para clasificar dígitos escritos a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura típica para MNIST\n",
    "# Input: 28x28 = 784 píxeles\n",
    "# Output: 10 clases (dígitos 0-9)\n",
    "\n",
    "red_mnist = RedNeuronal([784, 128, 64, 10])\n",
    "\n",
    "# Simulamos imágenes de dígitos (normalmente vendrían del dataset MNIST)\n",
    "batch_size = 32\n",
    "imagenes_simuladas = np.random.rand(batch_size, 784)\n",
    "\n",
    "print(f\"\\nProcesando batch de {batch_size} imágenes...\")\n",
    "predicciones = red_mnist.forward(imagenes_simuladas)\n",
    "\n",
    "print(f\"Predicciones shape: {predicciones.shape}\")\n",
    "print(f\"\\nEjemplo de predicción para la primera imagen:\")\n",
    "print(f\"Scores por clase: {predicciones[0]}\")\n",
    "print(f\"\\nNota: Estos son scores crudos. En Lab 03 aprenderemos a \")\n",
    "print(f\"convertirlos en probabilidades usando Softmax.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Comparación de Redes Profundas vs Anchas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red profunda (muchas capas, pocas neuronas)\n",
    "red_profunda = RedNeuronal([100, 50, 40, 30, 20, 10, 1])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Red ancha (pocas capas, muchas neuronas)\n",
    "red_ancha = RedNeuronal([100, 200, 1])\n",
    "\n",
    "# Comparación\n",
    "X_comp = np.random.randn(10, 100)\n",
    "\n",
    "print(\"\\nComparación de salidas:\")\n",
    "salida_profunda = red_profunda.forward(X_comp)\n",
    "salida_ancha = red_ancha.forward(X_comp)\n",
    "\n",
    "print(f\"Salida red profunda (primeras 3): {salida_profunda[:3].flatten()}\")\n",
    "print(f\"Salida red ancha (primeras 3): {salida_ancha[:3].flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.3 (Desafío)\n",
    "Crea dos redes diferentes con aproximadamente el mismo número de parámetros:\n",
    "- Una red profunda (4+ capas)\n",
    "- Una red ancha (2 capas)\n",
    "\n",
    "Compara sus arquitecturas y el número total de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "# Pista: Para ~10,000 parámetros podrías usar:\n",
    "# Profunda: [10, 30, 25, 20, 10]\n",
    "# Ancha: [10, 100, 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Entendiendo el Problema de la Linealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostramos que sin activación, la red es lineal\n",
    "\n",
    "# Red de 3 capas\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(2, 3) * 0.01\n",
    "b1 = np.zeros(3)\n",
    "W2 = np.random.randn(3, 4) * 0.01\n",
    "b2 = np.zeros(4)\n",
    "W3 = np.random.randn(4, 1) * 0.01\n",
    "b3 = np.zeros(1)\n",
    "\n",
    "X = np.array([[1.0, 2.0]])\n",
    "\n",
    "# Forward pass capa por capa\n",
    "h1 = np.dot(X, W1) + b1\n",
    "h2 = np.dot(h1, W2) + b2\n",
    "y = np.dot(h2, W3) + b3\n",
    "\n",
    "print(\"Red de 3 capas:\")\n",
    "print(f\"Salida: {y}\")\n",
    "\n",
    "# Equivalente a una sola capa\n",
    "W_combined = np.dot(np.dot(W1, W2), W3)\n",
    "b_combined = np.dot(np.dot(b1, W2) + b2, W3) + b3\n",
    "y_combined = np.dot(X, W_combined) + b_combined\n",
    "\n",
    "print(\"\\nEquivalente a 1 capa:\")\n",
    "print(f\"Salida: {y_combined}\")\n",
    "\n",
    "print(f\"\\n¿Son iguales? {np.allclose(y, y_combined)}\")\n",
    "print(\"\\n¡Por eso necesitamos funciones de activación no lineales!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este laboratorio hemos aprendido:\n",
    "\n",
    "1. ✅ Cómo conectar múltiples capas de neuronas\n",
    "2. ✅ Forward propagation en redes profundas\n",
    "3. ✅ Diseñar arquitecturas para diferentes problemas\n",
    "4. ✅ Calcular y entender el número de parámetros\n",
    "5. ✅ Visualizar activaciones a través de la red\n",
    "6. ✅ El problema de la linealidad sin funciones de activación\n",
    "\n",
    "## Próximos Pasos\n",
    "\n",
    "En el Lab 03:\n",
    "- Introduciremos funciones de activación (ReLU, Sigmoid, Tanh, Softmax)\n",
    "- Veremos cómo añaden no-linealidad a la red\n",
    "- Entenderemos cuándo usar cada función de activación\n",
    "\n",
    "## Preguntas de Reflexión\n",
    "\n",
    "1. ¿Por qué necesitamos funciones de activación no lineales?\n",
    "2. ¿Qué ventajas tiene una red profunda sobre una ancha?\n",
    "3. ¿Cómo elegir el número de capas y neuronas por capa?\n",
    "4. ¿Qué pasa si inicializamos todos los pesos a cero?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
