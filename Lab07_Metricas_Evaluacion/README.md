# Lab 07: M√©tricas de Evaluaci√≥n y Matriz de Confusi√≥n

## Objetivos

1. Entender la matriz de confusi√≥n y sus componentes
2. Calcular e interpretar m√©tricas de clasificaci√≥n (Precision, Recall, F1-Score, Accuracy)
3. Utilizar m√©tricas apropiadas seg√∫n el problema
4. Implementar validaci√≥n cruzada (K-Fold)
5. Analizar errores del modelo para mejorarlo
6. Trabajar con datasets balanceados y desbalanceados

## Estructura

```
Lab07_Metricas_Evaluacion/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ teoria.md
‚îú‚îÄ‚îÄ practica.ipynb
‚îî‚îÄ‚îÄ codigo/
    ‚îî‚îÄ‚îÄ metricas.py
```

## Conceptos Clave

### Matriz de Confusi√≥n

```
                Predicci√≥n
             Positivo  Negativo
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
Real   P  ‚îÇ    TP    ‚îÇ    FN    ‚îÇ
       N  ‚îÇ    FP    ‚îÇ    TN    ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

- **TP (True Positives)**: Predicciones positivas correctas
- **TN (True Negatives)**: Predicciones negativas correctas
- **FP (False Positives)**: Falsos positivos (Error Tipo I)
- **FN (False Negatives)**: Falsos negativos (Error Tipo II)

### M√©tricas Principales

**Accuracy (Exactitud)**:
```
Accuracy = (TP + TN) / Total
```
‚Üí Proporci√≥n de predicciones correctas

**Precision (Precisi√≥n)**:
```
Precision = TP / (TP + FP)
```
‚Üí De las predicciones positivas, ¬øcu√°ntas fueron correctas?

**Recall (Sensibilidad)**:
```
Recall = TP / (TP + FN)
```
‚Üí De los casos positivos reales, ¬øcu√°ntos detectamos?

**F1-Score**:
```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```
‚Üí Media arm√≥nica entre Precision y Recall

### Cu√°ndo Usar Cada M√©trica

| Situaci√≥n | M√©trica Principal | Raz√≥n |
|-----------|------------------|-------|
| Dataset balanceado | Accuracy | Clases representadas equitativamente |
| Dataset desbalanceado | F1-Score | Evita sesgo hacia clase mayoritaria |
| FP muy costosos | Precision | Ej: diagn√≥stico que requiere cirug√≠a |
| FN muy costosos | Recall | Ej: detecci√≥n de fraude o enfermedades |
| Balance P y R | F1-Score | Mejor m√©trica √∫nica general |

### Validaci√≥n Cruzada

**K-Fold Cross-Validation**: Divide datos en K partes, entrena K veces

```
Fold 1: [TEST][TRAIN][TRAIN][TRAIN][TRAIN]
Fold 2: [TRAIN][TEST][TRAIN][TRAIN][TRAIN]
...
Fold K: [TRAIN][TRAIN][TRAIN][TRAIN][TEST]

M√©trica final = promedio de K evaluaciones
```

**Ventajas**:
- Estimaci√≥n m√°s robusta
- Mejor uso de datos limitados
- Reduce varianza de la evaluaci√≥n

## Pr√°ctica

### Ejecutar c√≥digo:
```bash
cd codigo/
python metricas.py
```

### Notebook:
```bash
jupyter notebook practica.ipynb
```

## Ejercicios

1. **B√°sico**: Calcular m√©tricas manualmente desde matriz de confusi√≥n
2. **Intermedio**: Implementar K-Fold cross-validation
3. **Avanzado**: Comparar modelos en dataset desbalanceado
4. **Desaf√≠o**: Optimizar umbral de clasificaci√≥n para maximizar F1

## Casos de Uso Reales

### Ejemplo 1: Detecci√≥n de Spam
```
FP: Email importante marcado como spam (muy malo)
FN: Spam que llega a inbox (tolerable)
‚Üí Optimizar: PRECISION (minimizar FP)
```

### Ejemplo 2: Detecci√≥n de Fraude
```
FP: Transacci√≥n leg√≠tima bloqueada (tolerable)
FN: Fraude no detectado (muy malo)
‚Üí Optimizar: RECALL (minimizar FN)
```

### Ejemplo 3: Clasificaci√≥n General
```
Ambos errores igualmente importantes
‚Üí Optimizar: F1-SCORE (balance)
```

## Debugging

**Accuracy alto pero modelo malo**:
- Verificar si dataset est√° desbalanceado
- Revisar otras m√©tricas (Precision, Recall, F1)

**Precision muy alta pero Recall baja**:
- Modelo demasiado conservador
- Reducir umbral de clasificaci√≥n

**Recall muy alto pero Precision baja**:
- Modelo demasiado liberal
- Aumentar umbral de clasificaci√≥n

**Todas las m√©tricas bajas**:
- Problema con los datos o el modelo
- Revisar preprocesamiento
- Considerar modelo m√°s complejo

## Checklist de Aprendizaje

- [ ] Entiendo la matriz de confusi√≥n y sus componentes
- [ ] Puedo calcular Accuracy, Precision, Recall y F1
- [ ] S√© cu√°ndo usar cada m√©trica
- [ ] Puedo implementar validaci√≥n cruzada
- [ ] S√© interpretar resultados en contexto del problema
- [ ] Puedo identificar y solucionar problemas comunes

## Relaci√≥n con Otros Labs

**De Lab 06 (Entrenamiento)**:
- Usamos modelos entrenados
- Aplicamos divisi√≥n train/val/test
- Monitoreamos m√©tricas durante entrenamiento

**Hacia Lab 08 (Frameworks)**:
- PyTorch y TensorFlow tienen m√©tricas built-in
- Mismo principio, implementaci√≥n m√°s f√°cil
- Automatizaci√≥n de validaci√≥n cruzada

## Recursos Adicionales

- Scikit-learn: `sklearn.metrics` (confusion_matrix, classification_report)
- Visualizaci√≥n: seaborn heatmap para matriz de confusi√≥n
- ROC curves y AUC para evaluaci√≥n avanzada

## Verificaci√≥n

Al finalizar este lab deber√≠as poder:
1. ‚úì Generar y interpretar matriz de confusi√≥n
2. ‚úì Calcular m√©tricas de clasificaci√≥n desde cero
3. ‚úì Elegir m√©tricas apropiadas para tu problema
4. ‚úì Implementar K-Fold cross-validation
5. ‚úì Analizar y mejorar modelos bas√°ndote en m√©tricas

## Pr√≥ximo Lab

**Lab 08**: Frameworks de Deep Learning (PyTorch, TensorFlow)
- Implementaci√≥n eficiente de todo lo aprendido
- M√©tricas y validaci√≥n automatizadas

---

**¬°La evaluaci√≥n correcta es tan importante como el entrenamiento! üìä**
