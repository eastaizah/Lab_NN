# Gu√≠a de Laboratorio: M√©tricas de Evaluaci√≥n y Matriz de Confusi√≥n

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Fundamentos de Deep Learning - M√©tricas de Evaluaci√≥n  
**C√≥digo:** Lab 07  
**Duraci√≥n:** 2-3 horas  
**Nivel:** Intermedio  

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender y construir matrices de confusi√≥n para clasificaci√≥n
2. Calcular e interpretar m√©tricas fundamentales (Accuracy, Precision, Recall, F1-Score)
3. Identificar cu√°ndo usar cada m√©trica seg√∫n el problema de negocio
4. Trabajar efectivamente con datasets balanceados y desbalanceados
5. Implementar validaci√≥n cruzada (K-Fold) desde cero
6. Analizar errores del modelo sistem√°ticamente
7. Visualizar resultados de evaluaci√≥n de manera efectiva
8. Tomar decisiones informadas sobre umbrales de clasificaci√≥n
9. Calcular m√©tricas para clasificaci√≥n multiclase
10. Generar reportes de evaluaci√≥n profesionales

## üìö Prerrequisitos

### Conocimientos

- Python intermedio (NumPy, manipulaci√≥n de datos)
- Redes neuronales b√°sicas y entrenamiento (Labs 05-06)
- Conceptos de clasificaci√≥n binaria y multiclase
- Estad√≠stica b√°sica (promedios, distribuciones)

### Software

- Python 3.8+
- NumPy 1.19+
- Matplotlib y Seaborn (visualizaciones)
- Scikit-learn (m√©tricas de referencia)
- Pandas (manipulaci√≥n de datos)

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` - Marco te√≥rico completo sobre m√©tricas
- `README.md` - Estructura del laboratorio
- Labs anteriores (especialmente Lab 06 sobre Entrenamiento)

## üìñ Introducci√≥n

### El Problema de la Evaluaci√≥n

Has entrenado un modelo y obtuviste 95% de accuracy. ¬øExcelente, verdad?

**No necesariamente.** Imagina esto:

```
Dataset de fraude bancario:
- 9,500 transacciones leg√≠timas (95%)
- 500 transacciones fraudulentas (5%)

Modelo "tonto" que siempre predice "NO FRAUDE":
- Accuracy: 95%
- Fraudes detectados: 0
- ¬°Completamente in√∫til!
```

**Este laboratorio te ense√±a a evaluar modelos correctamente.**

### ¬øPor Qu√© Necesitamos M√∫ltiples M√©tricas?

Diferentes problemas requieren diferentes m√©tricas:

**Detecci√≥n de Spam:**
- Falso Positivo (FP): Email importante marcado como spam ‚Üí **MUY MALO**
- Falso Negativo (FN): Spam en inbox ‚Üí Tolerable
- **M√©trica clave: Precision** (minimizar FP)

**Detecci√≥n de C√°ncer:**
- Falso Positivo (FP): Persona sana diagnosticada ‚Üí Tolerable (m√°s pruebas)
- Falso Negativo (FN): C√°ncer no detectado ‚Üí **MUY MALO**
- **M√©trica clave: Recall** (minimizar FN)

**Clasificaci√≥n General:**
- Ambos errores igualmente importantes
- **M√©trica clave: F1-Score** (balance)

### La Matriz de Confusi√≥n: Tu Mejor Amiga

La matriz de confusi√≥n muestra **exactamente** d√≥nde se equivoca tu modelo:

```
                    Predicci√≥n
                 Positivo  Negativo
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
Real       P  ‚îÇ    TP    ‚îÇ    FN    ‚îÇ
           N  ‚îÇ    FP    ‚îÇ    TN    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

- **TP (True Positives)**: ‚úì Correctamente identificados como positivos
- **TN (True Negatives)**: ‚úì Correctamente identificados como negativos
- **FP (False Positives)**: ‚úó Negativos incorrectamente marcados como positivos (Error Tipo I)
- **FN (False Negatives)**: ‚úó Positivos incorrectamente marcados como negativos (Error Tipo II)

### M√©tricas Fundamentales

```
Accuracy   = (TP + TN) / Total          ‚Üí Proporci√≥n de aciertos
Precision  = TP / (TP + FP)             ‚Üí De las predicciones +, ¬øcu√°ntas correctas?
Recall     = TP / (TP + FN)             ‚Üí De los casos + reales, ¬øcu√°ntos detectamos?
F1-Score   = 2 * (P * R) / (P + R)      ‚Üí Media arm√≥nica de Precision y Recall
```

### Aplicaciones en el Mundo Real

**Medicina:**
- Diagn√≥stico de enfermedades (Recall cr√≠tico)
- An√°lisis de im√°genes m√©dicas (Balance P/R)

**Finanzas:**
- Detecci√≥n de fraude (Recall cr√≠tico)
- Aprobaci√≥n de cr√©ditos (Precision importante)

**E-commerce:**
- Sistemas de recomendaci√≥n (Precision para UX)
- Detecci√≥n de rese√±as falsas (Balance)

**Seguridad:**
- Detecci√≥n de intrusiones (Recall cr√≠tico)
- Sistemas de autenticaci√≥n (Balance)

## ü§î Preguntas de Reflexi√≥n Iniciales

1. ¬øPor qu√© accuracy no siempre es una buena m√©trica?
2. ¬øQu√© m√©trica usar√≠as para un detector de bombas en aeropuertos?
3. ¬øC√≥mo afecta el desbalance de clases a la evaluaci√≥n?
4. ¬øQu√© significa "recall de 80%"?
5. ¬øCu√°ndo preferir√≠as precision sobre recall?

## üî¨ Parte 1: Matriz de Confusi√≥n (40 min)

### 1.1 Implementaci√≥n Desde Cero

La **matriz de confusi√≥n** es una tabla cuadrada de dimensi√≥n K√óK (donde K es el n√∫mero de clases) que resume el rendimiento de un clasificador comparando las etiquetas predichas con las etiquetas reales. Cada fila representa la **clase verdadera** de las muestras, mientras que cada columna representa la **clase predicha** por el modelo; esta convenci√≥n es fundamental para interpretar correctamente los valores. La **diagonal principal** contiene los aciertos del modelo ‚Äîes decir, los casos en que la predicci√≥n coincide con la realidad‚Äî, mientras que los elementos **fuera de la diagonal** representan errores, indicando confusiones entre pares de clases espec√≠ficas. Construir esta clase desde cero, en lugar de simplemente llamar a `sklearn.metrics.confusion_matrix`, obliga al estudiante a entender la estructura interna del c√°lculo: el conteo de co-ocurrencias entre cada par (clase_real, clase_predicha), lo cual desarrolla intuici√≥n sobre c√≥mo interpretar cada celda. Al finalizar, se espera obtener una clase reutilizable con m√©todos de visualizaci√≥n que permitan identificar de un vistazo cu√°les son los pares de clases m√°s confundidos por el modelo.

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

class ConfusionMatrix:
    """Matriz de confusi√≥n con visualizaci√≥n"""
    
    def __init__(self, y_true, y_pred, labels=None):
        """
        y_true: etiquetas verdaderas
        y_pred: predicciones del modelo
        labels: nombres de las clases (opcional)
        """
        self.y_true = np.array(y_true)
        self.y_pred = np.array(y_pred)
        self.labels = labels
        
        # Calcular matriz
        self.matrix = self._compute_matrix()
        
        # Para clasificaci√≥n binaria, extraer TP, TN, FP, FN
        if self.matrix.shape == (2, 2):
            self.tn = self.matrix[0, 0]
            self.fp = self.matrix[0, 1]
            self.fn = self.matrix[1, 0]
            self.tp = self.matrix[1, 1]
    
    def _compute_matrix(self):
        """Computa la matriz de confusi√≥n"""
        classes = np.unique(np.concatenate([self.y_true, self.y_pred]))
        n_classes = len(classes)
        
        matrix = np.zeros((n_classes, n_classes), dtype=int)
        
        for i, true_class in enumerate(classes):
            for j, pred_class in enumerate(classes):
                matrix[i, j] = np.sum(
                    (self.y_true == true_class) & (self.y_pred == pred_class)
                )
        
        return matrix
    
    def plot(self, normalize=False, cmap='Blues', figsize=(8, 6)):
        """Visualiza la matriz de confusi√≥n"""
        matrix = self.matrix.astype(float)
        
        if normalize:
            matrix = matrix / matrix.sum(axis=1, keepdims=True)
            fmt = '.2%'
            title = 'Matriz de Confusi√≥n (Normalizada)'
        else:
            fmt = 'd'
            title = 'Matriz de Confusi√≥n'
        
        plt.figure(figsize=figsize)
        sns.heatmap(matrix, annot=True, fmt=fmt, cmap=cmap, 
                   xticklabels=self.labels if self.labels else 'auto',
                   yticklabels=self.labels if self.labels else 'auto',
                   cbar_kws={'label': 'Frecuencia' if not normalize else 'Proporci√≥n'})
        
        plt.xlabel('Predicci√≥n', fontsize=12)
        plt.ylabel('Valor Real', fontsize=12)
        plt.title(title, fontsize=14, pad=20)
        plt.tight_layout()
        plt.show()
    
    def summary(self):
        """Imprime resumen de la matriz"""
        print("=" * 60)
        print("MATRIZ DE CONFUSI√ìN")
        print("=" * 60)
        print(self.matrix)
        
        if self.matrix.shape == (2, 2):
            print(f"\nTrue Negatives  (TN): {self.tn}")
            print(f"False Positives (FP): {self.fp}")
            print(f"False Negatives (FN): {self.fn}")
            print(f"True Positives  (TP): {self.tp}")
            
            total = self.tn + self.fp + self.fn + self.tp
            print(f"\nTotal de muestras: {total}")
            print(f"  Negativos reales: {self.tn + self.fp}")
            print(f"  Positivos reales: {self.fn + self.tp}")
        
        print("=" * 60)

# Ejemplo de uso
y_true = np.array([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1])
y_pred = np.array([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1])

cm = ConfusionMatrix(y_true, y_pred, labels=['Negativo', 'Positivo'])
cm.summary()
cm.plot()
```

### 1.2 Caso de Estudio: Detector de Spam

Un concepto fundamental en evaluaci√≥n de clasificadores es la **asimetr√≠a en el costo de los errores**: no todos los tipos de error tienen la misma gravedad para el negocio o el usuario. En un detector de spam, un **Falso Positivo** (un correo leg√≠timo marcado como spam) puede hacer que el usuario pierda un mensaje importante ‚Äîcomo una confirmaci√≥n de vuelo o una oferta de trabajo‚Äî, lo que constituye un error **muy grave**. Por el contrario, un **Falso Negativo** (un mensaje de spam que pasa al inbox) es simplemente molesto pero no causa da√±o real, resultando en un error **tolerable**. Esta asimetr√≠a deber√≠a influir directamente en el dise√±o del modelo: en lugar del umbral por defecto de 0.5, convendr√≠a usar un umbral m√°s alto para predecir "spam", aceptando m√°s FN a cambio de reducir los FP. Para un clasificador de ~85% de accuracy en un conjunto donde el 30% son spam, se espera que la mayor√≠a de los errores sean FN (spam no detectado), ya que esa estrategia conservadora protege mejor los correos leg√≠timos.

```python
# Simular predicciones de un detector de spam
np.random.seed(42)

# Generar datos
n_samples = 1000
true_spam_rate = 0.3

# Etiquetas verdaderas
y_true = np.random.binomial(1, true_spam_rate, n_samples)

# Simular predicciones (modelo con ~85% accuracy)
y_pred = y_true.copy()
# Introducir algunos errores
error_indices = np.random.choice(n_samples, size=int(0.15 * n_samples), replace=False)
y_pred[error_indices] = 1 - y_pred[error_indices]

# Crear matriz de confusi√≥n
cm_spam = ConfusionMatrix(y_true, y_pred, labels=['Ham', 'Spam'])

print("DETECTOR DE SPAM")
cm_spam.summary()
cm_spam.plot(normalize=True)

# Interpretaci√≥n
print("\nINTERPRETACI√ìN:")
print(f"- {cm_spam.tp} spam detectados correctamente")
print(f"- {cm_spam.fn} spam que pasaron (NO detectados) ‚ö†Ô∏è")
print(f"- {cm_spam.fp} emails leg√≠timos marcados como spam ‚ö†Ô∏è‚ö†Ô∏è")
print(f"- {cm_spam.tn} emails leg√≠timos clasificados correctamente")
```

**Actividad 1.1:** Crea una matriz de confusi√≥n para un problema m√©dico (detecci√≥n de enfermedad). Documenta cu√°ntos FP y FN obtuviste y reflexiona sobre cu√°l es m√°s grave en el contexto m√©dico.

### 1.3 Matriz de Confusi√≥n Multiclase

Cuando el problema tiene K > 2 clases, la matriz de confusi√≥n se extiende a una tabla K√óK donde cada celda (i, j) contiene el n√∫mero de ejemplos de la clase real i que fueron predichos como clase j. El an√°lisis de esta matriz sigue el enfoque **"uno contra el resto"** (One vs. Rest): para cada clase k, se eval√∫a cu√°ntos de sus ejemplos fueron correctamente identificados (celda diagonal) y hacia qu√© otras clases tiende a confundirse (celdas fuera de la diagonal en la fila k). En la inspecci√≥n visual, lo m√°s importante es identificar los pares de clases con mayor confusi√≥n mutua, ya que esto revela si hay similitudes sem√°nticas o de representaci√≥n que el modelo no logra diferenciar. La **matriz de confusi√≥n normalizada** (dividiendo cada fila por el total de muestras de esa clase) es especialmente reveladora en problemas multiclase: permite comparar el rendimiento por clase independientemente de cu√°ntas muestras tiene cada una, exponiendo clases donde el modelo rinde pobremente aunque representen pocos ejemplos. Se espera que las celdas diagonales tengan valores cercanos a 1.0 en un modelo bien entrenado, con errores concentrados entre clases visualmente similares.

```python
# Ejemplo con 3 clases
y_true_multi = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])
y_pred_multi = np.array([0, 1, 2, 0, 2, 2, 0, 1, 1, 0, 1, 2, 2, 1, 2])

cm_multi = ConfusionMatrix(
    y_true_multi, 
    y_pred_multi, 
    labels=['Clase A', 'Clase B', 'Clase C']
)

print("CLASIFICACI√ìN MULTICLASE")
cm_multi.summary()
cm_multi.plot(normalize=True)
```

## üî¨ Parte 2: M√©tricas de Clasificaci√≥n (50 min)

### 2.1 Implementaci√≥n de M√©tricas B√°sicas

Cada m√©trica de clasificaci√≥n captura un aspecto diferente del comportamiento del modelo, y elegir la correcta es tan importante como dise√±ar la arquitectura. A continuaci√≥n se desarrolla la intuici√≥n detr√°s de cada una:

- **Accuracy** `= (TP + TN) / Total`: Mide la fracci√≥n de predicciones correctas sobre el total. Es apropiada cuando las clases est√°n balanceadas y los errores tienen el mismo costo, pero se vuelve **enga√±osa** en datasets desbalanceados ‚Äîun clasificador que siempre predice la clase mayoritaria puede tener 99% de accuracy y ser completamente in√∫til.

- **Precision** `= TP / (TP + FP)`: Responde a la pregunta *"de todas las veces que el modelo dijo 'positivo', ¬øcu√°ntas veces ten√≠a raz√≥n?"*. Alta precisi√≥n significa pocos falsos positivos; es la m√©trica clave cuando el costo de una alarma falsa es alto (p. ej., spam filters, sistemas de aprobaci√≥n de cr√©dito).

- **Recall (Sensibilidad)** `= TP / (TP + FN)`: Responde a *"de todos los casos positivos reales, ¬øcu√°ntos detect√≥ el modelo?"*. Alto recall significa que el modelo "no se pierde" casos positivos; es cr√≠tico cuando el costo de no detectar un positivo es alto (p. ej., diagn√≥stico m√©dico, detecci√≥n de fraude).

- **F1-Score** `= 2¬∑(P¬∑R)/(P+R)`: La **media arm√≥nica** de Precision y Recall. A diferencia de la media aritm√©tica, la arm√≥nica penaliza fuertemente cuando uno de los dos valores es bajo: un modelo con Precision=1.0 y Recall=0.0 obtiene F1=0, no 0.5. Esto lo hace m√°s informativo cuando existe un balance entre ambos objetivos.

- **Specificity** `= TN / (TN + FP)`: Tambi√©n llamada "True Negative Rate", mide qu√© tan bien el modelo identifica los negativos reales. Es la contraparte del Recall para la clase negativa; en medicina se conoce como "especificidad de la prueba".

- **F-beta Score** `= (1+Œ≤¬≤)¬∑(P¬∑R)/(Œ≤¬≤¬∑P+R)`: Generalizaci√≥n del F1 que permite controlar el balance entre Precision y Recall. Con **Œ≤ < 1** se da m√°s peso a Precision (√∫til cuando FP son m√°s costosos); con **Œ≤ > 1** se prioriza Recall (√∫til cuando FN son m√°s costosos). El F2-Score (Œ≤=2) es com√∫n en detecci√≥n m√©dica.

- **MCC (Matthews Correlation Coefficient)**: Considerado por muchos investigadores como la m√©trica individual m√°s informativa para clasificaci√≥n binaria, ya que considera los cuatro valores de la matriz de confusi√≥n (TP, TN, FP, FN) de forma sim√©trica. Tiene rango [-1, 1], donde 1 es predicci√≥n perfecta, 0 equivale a una predicci√≥n aleatoria y -1 indica predicci√≥n completamente inversa. A diferencia del F1, no se ve distorsionado por el desbalance de clases.

La implementaci√≥n desde cero de esta clase consolidar√° la comprensi√≥n de cada f√≥rmula y permitir√° ver c√≥mo interact√∫an entre s√≠ en el reporte final.

```python
class ClassificationMetrics:
    """Calculadora de m√©tricas de clasificaci√≥n"""
    
    def __init__(self, y_true, y_pred):
        self.y_true = np.array(y_true)
        self.y_pred = np.array(y_pred)
        
        # Calcular matriz de confusi√≥n
        self.cm = ConfusionMatrix(y_true, y_pred)
        
        if hasattr(self.cm, 'tp'):  # Clasificaci√≥n binaria
            self.tp = self.cm.tp
            self.tn = self.cm.tn
            self.fp = self.cm.fp
            self.fn = self.cm.fn
    
    def accuracy(self):
        """
        Accuracy = (TP + TN) / Total
        Proporci√≥n de predicciones correctas
        """
        correct = np.sum(self.y_true == self.y_pred)
        total = len(self.y_true)
        return correct / total
    
    def precision(self, zero_division=0):
        """
        Precision = TP / (TP + FP)
        De las predicciones positivas, ¬øcu√°ntas fueron correctas?
        """
        denominator = self.tp + self.fp
        if denominator == 0:
            return zero_division
        return self.tp / denominator
    
    def recall(self, zero_division=0):
        """
        Recall = TP / (TP + FN)
        De los positivos reales, ¬øcu√°ntos detectamos?
        """
        denominator = self.tp + self.fn
        if denominator == 0:
            return zero_division
        return self.tp / denominator
    
    def f1_score(self):
        """
        F1 = 2 * (Precision * Recall) / (Precision + Recall)
        Media arm√≥nica de Precision y Recall
        """
        p = self.precision()
        r = self.recall()
        
        if p + r == 0:
            return 0.0
        
        return 2 * (p * r) / (p + r)
    
    def specificity(self):
        """
        Specificity = TN / (TN + FP)
        De los negativos reales, ¬øcu√°ntos identificamos?
        """
        denominator = self.tn + self.fp
        if denominator == 0:
            return 0.0
        return self.tn / denominator
    
    def f_beta_score(self, beta=1.0):
        """
        F-beta score: permite dar m√°s peso a Precision o Recall
        
        beta < 1: M√°s peso a Precision
        beta > 1: M√°s peso a Recall
        beta = 1: F1-Score
        """
        p = self.precision()
        r = self.recall()
        
        if p + r == 0:
            return 0.0
        
        beta_squared = beta ** 2
        return (1 + beta_squared) * (p * r) / (beta_squared * p + r)
    
    def matthews_correlation_coefficient(self):
        """
        MCC: Correlaci√≥n entre predicciones y realidad
        Rango: [-1, 1]
        1: Perfecto, 0: Aleatorio, -1: Totalmente incorrecto
        """
        numerator = (self.tp * self.tn) - (self.fp * self.fn)
        denominator = np.sqrt(
            (self.tp + self.fp) * (self.tp + self.fn) * 
            (self.tn + self.fp) * (self.tn + self.fn)
        )
        
        if denominator == 0:
            return 0.0
        
        return numerator / denominator
    
    def report(self):
        """Genera reporte completo de m√©tricas"""
        print("=" * 70)
        print("REPORTE DE EVALUACI√ìN")
        print("=" * 70)
        
        print("\nMATRIZ DE CONFUSI√ìN:")
        print(f"  TN: {self.tn:5d}  |  FP: {self.fp:5d}")
        print(f"  FN: {self.fn:5d}  |  TP: {self.tp:5d}")
        
        print("\nM√âTRICAS PRINCIPALES:")
        print(f"  Accuracy:    {self.accuracy():.4f}  ({self.accuracy()*100:.2f}%)")
        print(f"  Precision:   {self.precision():.4f}  ({self.precision()*100:.2f}%)")
        print(f"  Recall:      {self.recall():.4f}  ({self.recall()*100:.2f}%)")
        print(f"  F1-Score:    {self.f1_score():.4f}  ({self.f1_score()*100:.2f}%)")
        
        print("\nM√âTRICAS ADICIONALES:")
        print(f"  Specificity: {self.specificity():.4f}  ({self.specificity()*100:.2f}%)")
        print(f"  F2-Score:    {self.f_beta_score(beta=2):.4f}")
        print(f"  MCC:         {self.matthews_correlation_coefficient():.4f}")
        
        print("\nINTERPRETACI√ìN:")
        self._interpret()
        
        print("=" * 70)
    
    def _interpret(self):
        """Interpreta las m√©tricas"""
        acc = self.accuracy()
        prec = self.precision()
        rec = self.recall()
        f1 = self.f1_score()
        
        # Balance Precision-Recall
        if abs(prec - rec) < 0.1:
            print("  ‚úì Precision y Recall balanceados")
        elif prec > rec + 0.1:
            print("  ‚ö†Ô∏è  Precision > Recall:")
            print("      - Modelo conservador (pocos positivos predichos)")
            print("      - Menos falsos positivos, m√°s falsos negativos")
        else:
            print("  ‚ö†Ô∏è  Recall > Precision:")
            print("      - Modelo liberal (muchos positivos predichos)")
            print("      - Menos falsos negativos, m√°s falsos positivos")
        
        # Accuracy vs F1
        if acc > f1 + 0.1:
            print("  ‚ö†Ô∏è  Accuracy >> F1-Score:")
            print("      - Posible desbalance de clases")
            print("      - Revisar distribuci√≥n del dataset")

# Ejemplo
metrics = ClassificationMetrics(y_true, y_pred)
metrics.report()
```

### 2.2 Comparaci√≥n Visual de M√©tricas

La comparaci√≥n visual de m√©tricas entre modelos es esencial para la selecci√≥n de modelos, ya que los n√∫meros en una tabla pueden resultar dif√≠ciles de interpretar en conjunto. Cuando se optimiza un modelo para una √∫nica m√©trica ‚Äîpor ejemplo, maximizar Accuracy‚Äî se corre el riesgo de degradar silenciosamente otras m√©tricas igualmente importantes: un modelo que maximiza Accuracy en datos desbalanceados puede tener Recall cercano a cero. Los **gr√°ficos de barras** con m√∫ltiples m√©tricas permiten ver de un vistazo el "perfil" del modelo: un modelo bien balanceado mostrar√° barras de altura similar para Precision y Recall, mientras que un modelo sesgado mostrar√° una barra alta en una y baja en la otra. Un **perfil ideal** presenta Accuracy, Precision, Recall y F1 todos por encima de 0.85, sin diferencias mayores a 0.10 entre ellos; cuando Accuracy supera a F1 en m√°s de 0.15 puntos, se debe investigar el balance de clases del dataset. La comparaci√≥n entre un modelo con m√©tricas balanceadas vs. un modelo aleatorio (baseline) tambi√©n es fundamental para validar que el modelo realmente aprendi√≥ algo √∫til.

```python
def plot_metrics_comparison(y_true, y_pred_list, model_names):
    """
    Compara m√©tricas de m√∫ltiples modelos
    
    y_true: etiquetas verdaderas
    y_pred_list: lista de predicciones de diferentes modelos
    model_names: nombres de los modelos
    """
    metrics_dict = {
        'Accuracy': [],
        'Precision': [],
        'Recall': [],
        'F1-Score': []
    }
    
    for y_pred in y_pred_list:
        m = ClassificationMetrics(y_true, y_pred)
        metrics_dict['Accuracy'].append(m.accuracy())
        metrics_dict['Precision'].append(m.precision())
        metrics_dict['Recall'].append(m.recall())
        metrics_dict['F1-Score'].append(m.f1_score())
    
    # Graficar
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()
    
    for idx, (metric_name, values) in enumerate(metrics_dict.items()):
        ax = axes[idx]
        bars = ax.bar(model_names, values, color=plt.cm.viridis(np.linspace(0.3, 0.9, len(values))))
        ax.set_ylabel(metric_name, fontsize=12)
        ax.set_title(f'{metric_name} por Modelo', fontsize=13)
        ax.set_ylim(0, 1.1)
        ax.grid(True, alpha=0.3, axis='y')
        
        # A√±adir valores en las barras
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}',
                   ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.show()

# Ejemplo: Comparar 3 modelos
model1_pred = y_pred
model2_pred = np.random.binomial(1, 0.5, len(y_true))  # Modelo aleatorio
model3_pred = y_true.copy()  # Modelo perfecto
model3_pred[np.random.choice(len(y_true), 10, replace=False)] = 1 - model3_pred[np.random.choice(len(y_true), 10, replace=False)]

plot_metrics_comparison(
    y_true,
    [model1_pred, model2_pred, model3_pred],
    ['Modelo A', 'Modelo Random', 'Modelo B']
)
```

**Actividad 2.1:** Crea 3 modelos con diferentes balances Precision-Recall y comp√°ralos. Observa c√≥mo el perfil de barras cambia y reflexiona sobre cu√°l modelo elegir√≠as para cada contexto de aplicaci√≥n.

### 2.3 Efecto del Umbral de Decisi√≥n

En clasificaci√≥n probabil√≠stica, el modelo no produce directamente una etiqueta binaria sino una **probabilidad** entre 0 y 1. El **umbral de decisi√≥n** (por defecto 0.5) es el valor a partir del cual se decide predecir "positivo": si p(x) ‚â• umbral ‚Üí Positivo. El valor de 0.5 es una elecci√≥n arbitraria que asume que ambos tipos de error tienen el mismo costo y que las clases est√°n balanceadas; en la pr√°ctica, este umbral rara vez es el √≥ptimo. Cuando se **sube el umbral** (p. ej., a 0.7), el modelo se vuelve m√°s conservador: solo predice "positivo" cuando est√° muy seguro, lo que aumenta la Precision pero reduce el Recall (m√°s FN). Cuando se **baja el umbral** (p. ej., a 0.3), el modelo es m√°s agresivo: predice "positivo" con menos certeza, aumentando el Recall pero reduciendo la Precision (m√°s FP). La **curva Precision-Recall** visualiza este tradeoff para todos los umbrales posibles, y su √°rea bajo la curva (AUCPR) resume la calidad del modelo independientemente del umbral elegido. El **punto de operaci√≥n** √≥ptimo se selecciona seg√∫n los requisitos del negocio: si FN son m√°s costosos, se elige un umbral bajo; si FP son m√°s costosos, se elige un umbral alto. El m√°ximo del F1-Score a lo largo de los umbrales indica el punto de mejor balance.

```python
def analyze_threshold_effect(y_true, y_proba, thresholds=np.linspace(0, 1, 21)):
    """
    Analiza c√≥mo diferentes umbrales afectan las m√©tricas
    
    y_true: etiquetas verdaderas
    y_proba: probabilidades predichas (0 a 1)
    thresholds: umbrales a probar
    """
    precisions = []
    recalls = []
    f1_scores = []
    
    for threshold in thresholds:
        y_pred = (y_proba >= threshold).astype(int)
        
        if len(np.unique(y_pred)) == 1:  # Solo una clase predicha
            precisions.append(0)
            recalls.append(0)
            f1_scores.append(0)
            continue
        
        m = ClassificationMetrics(y_true, y_pred)
        precisions.append(m.precision())
        recalls.append(m.recall())
        f1_scores.append(m.f1_score())
    
    # Visualizar
    plt.figure(figsize=(14, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(thresholds, precisions, 'b-o', label='Precision', linewidth=2)
    plt.plot(thresholds, recalls, 'r-s', label='Recall', linewidth=2)
    plt.plot(thresholds, f1_scores, 'g-^', label='F1-Score', linewidth=2)
    plt.xlabel('Umbral de Clasificaci√≥n', fontsize=12)
    plt.ylabel('Valor de M√©trica', fontsize=12)
    plt.title('M√©tricas vs Umbral', fontsize=14)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(recalls, precisions, 'b-o', linewidth=2, markersize=8)
    plt.xlabel('Recall', fontsize=12)
    plt.ylabel('Precision', fontsize=12)
    plt.title('Curva Precision-Recall', fontsize=14)
    plt.grid(True, alpha=0.3)
    
    # Marcar punto √≥ptimo (max F1)
    max_f1_idx = np.argmax(f1_scores)
    plt.plot(recalls[max_f1_idx], precisions[max_f1_idx], 'r*', 
            markersize=20, label=f'Max F1 (threshold={thresholds[max_f1_idx]:.2f})')
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    # Encontrar umbral √≥ptimo
    max_f1_idx = np.argmax(f1_scores)
    print(f"\nUMBRAL √ìPTIMO (maximiza F1):")
    print(f"  Threshold: {thresholds[max_f1_idx]:.2f}")
    print(f"  Precision: {precisions[max_f1_idx]:.4f}")
    print(f"  Recall:    {recalls[max_f1_idx]:.4f}")
    print(f"  F1-Score:  {f1_scores[max_f1_idx]:.4f}")

# Ejemplo
# Generar probabilidades
y_proba = np.random.beta(2, 5, len(y_true))  # Probabilidades sesgadas
y_proba[y_true == 1] += 0.3  # Positivos tienen mayor probabilidad

analyze_threshold_effect(y_true, y_proba)
```

**Actividad 2.2:** Encuentra el umbral √≥ptimo para un problema donde FN son 2x m√°s costosos que FP. Documenta el umbral seleccionado, las m√©tricas resultantes y compara con el umbral que maximiza F1.

## üî¨ Parte 3: Datasets Desbalanceados (40 min)

### 3.1 El Problema del Desbalance

El **desbalance de clases** ocurre cuando una o m√°s clases tienen significativamente m√°s muestras que otras en el conjunto de datos. Este fen√≥meno es extremadamente com√∫n en aplicaciones reales: en detecci√≥n de fraude bancario, apenas el 0.1‚Äì1% de las transacciones son fraudulentas; en diagn√≥stico de enfermedades raras, los casos positivos pueden representar menos del 1%; en clasificaci√≥n de tr√°fico de red, el tr√°fico malicioso es una fracci√≥n m√≠nima del tr√°fico total leg√≠timo. El problema fundamental es que la **Accuracy se vuelve una m√©trica completamente enga√±osa**: si el 95% de las muestras son de clase negativa, un clasificador que **siempre predice negativo** (el "clasificador mayoritario na√Øve") obtiene 95% de Accuracy sin haber aprendido absolutamente nada. Este clasificador na√Øve debe usarse siempre como **baseline** en problemas desbalanceados: cualquier modelo real debe superar este umbral trivial en m√©tricas relevantes (Recall, F1, MCC). El verdadero indicador de utilidad en estos contextos es el Recall de la clase minoritaria ‚Äîsi el modelo no detecta al menos una fracci√≥n razonable de los casos positivos reales, es inutilizable‚Äî junto con el F1-Score que penaliza simult√°neamente los falsos positivos y negativos.

```python
# Crear dataset muy desbalanceado (95% negativos, 5% positivos)
n_samples = 1000
n_positives = 50
n_negatives = 950

y_true_imbalanced = np.array([0] * n_negatives + [1] * n_positives)
np.random.shuffle(y_true_imbalanced)

# Modelo "tonto" que siempre predice negativo
y_pred_dummy = np.zeros_like(y_true_imbalanced)

# Modelo real con 80% accuracy en ambas clases
y_pred_real = y_true_imbalanced.copy()
# Errores en negativos
neg_indices = np.where(y_true_imbalanced == 0)[0]
error_neg = np.random.choice(neg_indices, size=int(0.2 * len(neg_indices)), replace=False)
y_pred_real[error_neg] = 1

# Errores en positivos
pos_indices = np.where(y_true_imbalanced == 1)[0]
error_pos = np.random.choice(pos_indices, size=int(0.2 * len(pos_indices)), replace=False)
y_pred_real[error_pos] = 0

# Comparar
print("DATASET DESBALANCEADO (95% negativos, 5% positivos)")
print("\n1. Modelo Dummy (siempre predice negativo):")
metrics_dummy = ClassificationMetrics(y_true_imbalanced, y_pred_dummy)
metrics_dummy.report()

print("\n2. Modelo Real (80% accuracy en cada clase):")
metrics_real = ClassificationMetrics(y_true_imbalanced, y_pred_real)
metrics_real.report()

print("\n¬°CONCLUSI√ìN!")
print("El modelo dummy tiene 95% accuracy pero es in√∫til.")
print("El modelo real tiene ~90% accuracy y s√≠ detecta positivos.")
print("‚Üí Accuracy NO es suficiente en datasets desbalanceados!")
```

### 3.2 T√©cnicas para Datos Desbalanceados

Existen tres grandes estrategias para lidiar con el desbalance de clases, cada una con sus ventajas y desventajas:

**1. Sobremuestreo (Oversampling) de la clase minoritaria:** Duplica o genera nuevas muestras artificiales de la clase minoritaria hasta igualar el n√∫mero de muestras de la clase mayoritaria. La versi√≥n b√°sica (Random Oversampling) simplemente duplica muestras existentes; versiones avanzadas como SMOTE generan muestras sint√©ticas interpolando entre vecinos. *Pros:* Simple, no pierde informaci√≥n del conjunto original. *Contras:* Riesgo de **overfitting** sobre las muestras duplicadas, ya que el modelo puede memorizar exactamente esas instancias en lugar de generalizar.

**2. Submuestreo (Undersampling) de la clase mayoritaria:** Reduce aleatoriamente la clase mayoritaria hasta igualar el tama√±o de la minoritaria, descartando muestras. *Pros:* Reduce el tiempo de entrenamiento, puede eliminar ruido de la clase mayoritaria. *Contras:* **P√©rdida de informaci√≥n potencialmente valiosa** al descartar muestras leg√≠timas; no recomendable cuando el dataset ya es peque√±o.

**3. Pesos de clase (Class Weights):** Modifica la funci√≥n de p√©rdida para asignar un penalizaci√≥n mayor a los errores en la clase minoritaria, sin alterar el dataset en s√≠. El peso de cada clase es inversamente proporcional a su frecuencia: `w_k = N_total / (K √ó N_k)`. *Pros:* Usa todas las muestras disponibles, es m√°s estable que el resampling. *Contras:* Requiere que el algoritmo soporte class weights; puede ser m√°s dif√≠cil de ajustar el balance correcto.

**¬øCu√°ndo usar cada estrategia?** Si el dataset tiene suficientes muestras de la clase minoritaria (>500), prefiere **class weights** por su simplicidad. Si las muestras son muy pocas (<100), usa **oversampling** para aumentar la diversidad. Si el tiempo de entrenamiento es cr√≠tico y el dataset es muy grande, considera **undersampling** con cuidado.

```python
class ImbalancedDataHandler:
    """Herramientas para manejar datos desbalanceados"""
    
    @staticmethod
    def oversample_minority(X, y):
        """Sobremuestreo de la clase minoritaria"""
        unique, counts = np.unique(y, return_counts=True)
        minority_class = unique[np.argmin(counts)]
        majority_class = unique[np.argmax(counts)]
        
        minority_indices = np.where(y == minority_class)[0]
        majority_count = np.max(counts)
        
        # Duplicar minoritarios hasta igualar mayoritarios
        n_to_add = majority_count - len(minority_indices)
        additional_indices = np.random.choice(minority_indices, size=n_to_add, replace=True)
        
        all_indices = np.concatenate([np.arange(len(y)), additional_indices])
        
        return X[all_indices], y[all_indices]
    
    @staticmethod
    def undersample_majority(X, y):
        """Submuestreo de la clase mayoritaria"""
        unique, counts = np.unique(y, return_counts=True)
        minority_class = unique[np.argmin(counts)]
        majority_class = unique[np.argmax(counts)]
        
        minority_indices = np.where(y == minority_class)[0]
        majority_indices = np.where(y == majority_class)[0]
        
        # Reducir mayoritarios al tama√±o de minoritarios
        sampled_majority = np.random.choice(
            majority_indices, 
            size=len(minority_indices), 
            replace=False
        )
        
        balanced_indices = np.concatenate([minority_indices, sampled_majority])
        np.random.shuffle(balanced_indices)
        
        return X[balanced_indices], y[balanced_indices]
    
    @staticmethod
    def class_weights(y):
        """
        Calcula pesos para balancear clases
        Peso inversamente proporcional a frecuencia
        """
        unique, counts = np.unique(y, return_counts=True)
        total = len(y)
        
        weights = {}
        for cls, count in zip(unique, counts):
            weights[cls] = total / (len(unique) * count)
        
        return weights

# Ejemplo
X_dummy = np.arange(len(y_true_imbalanced)).reshape(-1, 1)

print("T√âCNICAS DE BALANCEO\n")

# Original
print(f"Original: {len(y_true_imbalanced)} muestras")
print(f"  Clase 0: {np.sum(y_true_imbalanced == 0)}")
print(f"  Clase 1: {np.sum(y_true_imbalanced == 1)}")

# Oversample
X_over, y_over = ImbalancedDataHandler.oversample_minority(X_dummy, y_true_imbalanced)
print(f"\nOversample: {len(y_over)} muestras")
print(f"  Clase 0: {np.sum(y_over == 0)}")
print(f"  Clase 1: {np.sum(y_over == 1)}")

# Undersample
X_under, y_under = ImbalancedDataHandler.undersample_majority(X_dummy, y_true_imbalanced)
print(f"\nUndersample: {len(y_under)} muestras")
print(f"  Clase 0: {np.sum(y_under == 0)}")
print(f"  Clase 1: {np.sum(y_under == 1)}")

# Pesos
weights = ImbalancedDataHandler.class_weights(y_true_imbalanced)
print(f"\nPesos de clase:")
for cls, weight in weights.items():
    print(f"  Clase {cls}: {weight:.4f}")
```

**Actividad 3.1:** Compara el rendimiento de un modelo entrenado en datos originales vs balanceados. Documenta espec√≠ficamente el Recall de la clase minoritaria en cada caso y explica por qu√© las diferencias observadas tienen sentido.

## üî¨ Parte 4: Validaci√≥n Cruzada (45 min)

### 4.1 K-Fold Cross-Validation

Evaluar un modelo con una √∫nica divisi√≥n train/test tiene un problema fundamental: la **alta varianza en la estimaci√≥n del rendimiento**. Si el conjunto de test, por azar, contiene muestras "f√°ciles", la m√©trica ser√° optimista; si contiene muestras "dif√≠ciles", ser√° pesimista. Este problema se conoce en estad√≠stica como **varianza del estimador**. La **K-Fold Cross-Validation** resuelve esto dividiendo el dataset en K subconjuntos ("folds") de tama√±o similar: en cada iteraci√≥n, uno de los K folds se usa como conjunto de validaci√≥n y los K-1 restantes como entrenamiento. Al rotar sistem√°ticamente cu√°l fold act√∫a como validaci√≥n, **todas las muestras son usadas para validaci√≥n exactamente una vez**, lo que produce K estimaciones de la m√©trica. El promedio de estas K estimaciones es un estimador m√°s robusto del rendimiento real del modelo. La elecci√≥n de K implica un tradeoff bias-varianza: **K=5** es el m√°s utilizado en la pr√°ctica porque ofrece un buen balance entre costo computacional y varianza del estimador; **K=10** produce estimaciones m√°s estables pero requiere m√°s tiempo. Con K=N (Leave-One-Out), la varianza del estimador es m√≠nima pero el costo computacional es prohibitivo. La variante **Stratified K-Fold** es especialmente importante en datasets desbalanceados: garantiza que la proporci√≥n de clases en cada fold sea representativa del dataset completo, evitando folds donde la clase minoritaria est√© ausente o sobrerrepresentada.

```python
class KFoldCrossValidator:
    """Implementaci√≥n de K-Fold Cross-Validation"""
    
    def __init__(self, n_splits=5, shuffle=True, random_state=None):
        self.n_splits = n_splits
        self.shuffle = shuffle
        self.random_state = random_state
    
    def split(self, X, y=None):
        """
        Genera √≠ndices de train/test para cada fold
        
        Retorna: generador de tuplas (train_idx, test_idx)
        """
        n_samples = len(X)
        indices = np.arange(n_samples)
        
        if self.shuffle:
            if self.random_state is not None:
                np.random.seed(self.random_state)
            np.random.shuffle(indices)
        
        fold_sizes = np.full(self.n_splits, n_samples // self.n_splits, dtype=int)
        fold_sizes[:n_samples % self.n_splits] += 1
        
        current = 0
        for fold_size in fold_sizes:
            start, stop = current, current + fold_size
            test_idx = indices[start:stop]
            train_idx = np.concatenate([indices[:start], indices[stop:]])
            yield train_idx, test_idx
            current = stop
    
    def cross_validate(self, model, X, y, metric_fn=None):
        """
        Ejecuta cross-validation completa
        
        model: modelo con m√©todos fit() y predict()
        X, y: datos
        metric_fn: funci√≥n de m√©trica (default: accuracy)
        """
        if metric_fn is None:
            metric_fn = lambda y_true, y_pred: np.mean(y_true == y_pred)
        
        scores = []
        fold_metrics = []
        
        print(f"Ejecutando {self.n_splits}-Fold Cross-Validation...")
        print("=" * 70)
        
        for fold, (train_idx, test_idx) in enumerate(self.split(X, y), 1):
            # Dividir datos
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            
            # Entrenar
            model.fit(X_train, y_train)
            
            # Predecir
            y_pred = model.predict(X_test)
            
            # Evaluar
            score = metric_fn(y_test, y_pred)
            scores.append(score)
            
            # M√©tricas detalladas
            metrics = ClassificationMetrics(y_test, y_pred)
            fold_metrics.append({
                'accuracy': metrics.accuracy(),
                'precision': metrics.precision(),
                'recall': metrics.recall(),
                'f1': metrics.f1_score()
            })
            
            print(f"Fold {fold}/{self.n_splits}: Score = {score:.4f}")
        
        print("=" * 70)
        
        # Resultados
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        
        print(f"\nRESULTADOS:")
        print(f"  Mean Score: {mean_score:.4f} (+/- {std_score:.4f})")
        print(f"  Min Score:  {np.min(scores):.4f}")
        print(f"  Max Score:  {np.max(scores):.4f}")
        
        # Promediar m√©tricas
        mean_metrics = {}
        for key in fold_metrics[0].keys():
            values = [m[key] for m in fold_metrics]
            mean_metrics[key] = (np.mean(values), np.std(values))
        
        print(f"\nM√âTRICAS PROMEDIO:")
        for key, (mean, std) in mean_metrics.items():
            print(f"  {key.capitalize():12s}: {mean:.4f} (+/- {std:.4f})")
        
        return {
            'scores': scores,
            'mean': mean_score,
            'std': std_score,
            'fold_metrics': fold_metrics
        }

# Ejemplo con modelo dummy
class DummyClassifier:
    """Clasificador simple para demostraci√≥n"""
    
    def fit(self, X, y):
        # "Entrenar": guardar la clase m√°s frecuente
        unique, counts = np.unique(y, return_counts=True)
        self.most_common_class = unique[np.argmax(counts)]
        return self
    
    def predict(self, X):
        # Predecir la clase m√°s frecuente
        return np.full(len(X), self.most_common_class)

# Usar
cv = KFoldCrossValidator(n_splits=5, shuffle=True, random_state=42)
model = DummyClassifier()

results = cv.cross_validate(model, X_dummy, y_true_imbalanced)
```

### 4.2 Visualizaci√≥n de Resultados de CV

La varianza en los scores entre folds es una se√±al diagn√≥stica crucial sobre la **estabilidad del modelo**. Si los scores var√≠an poco entre folds (desviaci√≥n est√°ndar < 0.03), el modelo es **robusto**: su rendimiento es predecible independientemente del subconjunto de datos usado, lo que genera confianza para su despliegue en producci√≥n. Si la varianza es alta (std > 0.05), el modelo es **sensible a la partici√≥n de datos**: puede estar sobreajustando al conjunto de entrenamiento o puede haber subconjuntos del dataset con caracter√≠sticas muy diferentes (heterogeneidad). Las **barras de error** en los gr√°ficos de m√©tricas promedio representan ¬±1 desviaci√≥n est√°ndar entre folds: barras cortas indican consistencia, barras largas indican inestabilidad. Para la **selecci√≥n de modelos** con cross-validation, no solo se debe preferir el modelo con mayor media, sino tambi√©n considerar el que tenga menor varianza: un modelo con media=0.87 y std=0.01 es preferible a uno con media=0.89 y std=0.08, especialmente en aplicaciones cr√≠ticas. Los **intervalos de confianza** al 95% se pueden calcular como `media ¬± 1.96 √ó std / ‚àöK`, y son la forma correcta de reportar m√©tricas en papers y reportes profesionales.

```python
def plot_cv_results(cv_results):
    """Visualiza resultados de cross-validation"""
    scores = cv_results['scores']
    fold_metrics = cv_results['fold_metrics']
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Gr√°fica 1: Scores por fold
    ax1 = axes[0]
    folds = np.arange(1, len(scores) + 1)
    ax1.plot(folds, scores, 'bo-', linewidth=2, markersize=10)
    ax1.axhline(y=cv_results['mean'], color='r', linestyle='--', 
               label=f"Mean: {cv_results['mean']:.4f}")
    ax1.fill_between(folds, 
                     cv_results['mean'] - cv_results['std'],
                     cv_results['mean'] + cv_results['std'],
                     alpha=0.2, color='red')
    ax1.set_xlabel('Fold', fontsize=12)
    ax1.set_ylabel('Score', fontsize=12)
    ax1.set_title('Score por Fold', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Gr√°fica 2: M√©tricas promedio
    ax2 = axes[1]
    metric_names = list(fold_metrics[0].keys())
    metric_means = [np.mean([m[name] for m in fold_metrics]) for name in metric_names]
    metric_stds = [np.std([m[name] for m in fold_metrics]) for name in metric_names]
    
    bars = ax2.bar(metric_names, metric_means, yerr=metric_stds, 
                   capsize=5, color=plt.cm.viridis(np.linspace(0.3, 0.9, len(metric_names))))
    ax2.set_ylabel('Valor', fontsize=12)
    ax2.set_title('M√©tricas Promedio (con std)', fontsize=14)
    ax2.set_ylim(0, 1.1)
    ax2.grid(True, alpha=0.3, axis='y')
    
    # A√±adir valores
    for bar, mean, std in zip(bars, metric_means, metric_stds):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + std + 0.02,
                f'{mean:.3f}',
                ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.show()

plot_cv_results(results)
```

**Actividad 4.1:** Implementa Stratified K-Fold que mantiene la proporci√≥n de clases en cada fold. Compara los resultados con el K-Fold est√°ndar en un dataset desbalanceado y documenta las diferencias en la varianza entre folds.

## üìä An√°lisis Final de Rendimiento

### Dashboard Completo de Evaluaci√≥n

Un **reporte de evaluaci√≥n profesional** debe integrar todas las perspectivas del rendimiento del modelo en un √∫nico documento coherente, facilitando tanto la toma de decisiones t√©cnicas como la comunicaci√≥n con stakeholders no t√©cnicos. El flujo de trabajo est√°ndar es: **entrenar** el modelo con los datos de entrenamiento ‚Üí **evaluar** con el conjunto de test usando m√∫ltiples m√©tricas ‚Üí **interpretar** los resultados en el contexto del problema ‚Üí **decidir** si el modelo es apto para producci√≥n o requiere ajustes. Para una **audiencia t√©cnica**, el reporte debe incluir la matriz de confusi√≥n completa, todas las m√©tricas con intervalos de confianza, la curva Precision-Recall y los resultados de cross-validation. Para una **audiencia no t√©cnica** (gerencia, clientes), conviene traducir las m√©tricas a t√©rminos de negocio: "el modelo detecta el 87% de los fraudes reales" en lugar de "Recall = 0.87". Los **intervalos de confianza** para las m√©tricas son especialmente importantes cuando el conjunto de test es peque√±o: con 100 muestras, una diferencia de 2% en Accuracy entre dos modelos puede no ser estad√≠sticamente significativa. El dashboard que se implementa a continuaci√≥n integra matriz de confusi√≥n normalizada, barras de m√©tricas, distribuci√≥n de predicciones, curva Precision-Recall y resumen textual en una √∫nica figura de referencia profesional.

```python
class EvaluationDashboard:
    """Dashboard completo para evaluaci√≥n de modelos"""
    
    def __init__(self, y_true, y_pred, y_proba=None, model_name="Modelo"):
        self.y_true = y_true
        self.y_pred = y_pred
        self.y_proba = y_proba
        self.model_name = model_name
        
        self.cm = ConfusionMatrix(y_true, y_pred)
        self.metrics = ClassificationMetrics(y_true, y_pred)
    
    def generate_report(self):
        """Genera reporte completo con visualizaciones"""
        fig = plt.figure(figsize=(16, 10))
        
        # 1. Matriz de confusi√≥n
        ax1 = plt.subplot(2, 3, 1)
        matrix = self.cm.matrix.astype(float)
        matrix_norm = matrix / matrix.sum(axis=1, keepdims=True)
        sns.heatmap(matrix_norm, annot=True, fmt='.2%', cmap='Blues', ax=ax1,
                   xticklabels=['Neg', 'Pos'], yticklabels=['Neg', 'Pos'])
        ax1.set_title('Matriz de Confusi√≥n', fontsize=13)
        ax1.set_xlabel('Predicci√≥n')
        ax1.set_ylabel('Real')
        
        # 2. M√©tricas principales
        ax2 = plt.subplot(2, 3, 2)
        metrics_data = {
            'Accuracy': self.metrics.accuracy(),
            'Precision': self.metrics.precision(),
            'Recall': self.metrics.recall(),
            'F1-Score': self.metrics.f1_score()
        }
        bars = ax2.barh(list(metrics_data.keys()), list(metrics_data.values()),
                       color=plt.cm.viridis([0.3, 0.5, 0.7, 0.9]))
        ax2.set_xlim(0, 1)
        ax2.set_title('M√©tricas Principales', fontsize=13)
        ax2.grid(True, alpha=0.3, axis='x')
        
        for i, (bar, value) in enumerate(zip(bars, metrics_data.values())):
            ax2.text(value + 0.02, i, f'{value:.3f}', va='center')
        
        # 3. Distribuci√≥n de predicciones
        ax3 = plt.subplot(2, 3, 3)
        pred_dist = np.bincount(self.y_pred, minlength=2)
        true_dist = np.bincount(self.y_true, minlength=2)
        x = np.arange(2)
        width = 0.35
        ax3.bar(x - width/2, true_dist, width, label='Real', alpha=0.8)
        ax3.bar(x + width/2, pred_dist, width, label='Predicho', alpha=0.8)
        ax3.set_xticks(x)
        ax3.set_xticklabels(['Negativo', 'Positivo'])
        ax3.set_ylabel('Cantidad')
        ax3.set_title('Distribuci√≥n de Clases', fontsize=13)
        ax3.legend()
        ax3.grid(True, alpha=0.3, axis='y')
        
        # 4. Precision-Recall si tenemos probabilidades
        if self.y_proba is not None:
            ax4 = plt.subplot(2, 3, 4)
            thresholds = np.linspace(0, 1, 50)
            precisions = []
            recalls = []
            
            for t in thresholds:
                y_pred_t = (self.y_proba >= t).astype(int)
                if len(np.unique(y_pred_t)) == 2:
                    m = ClassificationMetrics(self.y_true, y_pred_t)
                    precisions.append(m.precision())
                    recalls.append(m.recall())
                else:
                    precisions.append(0)
                    recalls.append(0)
            
            ax4.plot(recalls, precisions, 'b-', linewidth=2)
            ax4.set_xlabel('Recall')
            ax4.set_ylabel('Precision')
            ax4.set_title('Curva Precision-Recall', fontsize=13)
            ax4.grid(True, alpha=0.3)
        
        # 5. Errores
        ax5 = plt.subplot(2, 3, 5)
        error_data = {
            'True Neg': self.cm.tn,
            'False Pos': self.cm.fp,
            'False Neg': self.cm.fn,
            'True Pos': self.cm.tp
        }
        colors = ['green', 'red', 'orange', 'green']
        ax5.pie(error_data.values(), labels=error_data.keys(), autopct='%1.1f%%',
               colors=colors, startangle=90)
        ax5.set_title('Distribuci√≥n de Resultados', fontsize=13)
        
        # 6. Resumen de texto
        ax6 = plt.subplot(2, 3, 6)
        ax6.axis('off')
        
        summary = f"""
        RESUMEN - {self.model_name}
        
        Total de muestras: {len(self.y_true)}
        
        Matriz de Confusi√≥n:
          TN: {self.cm.tn}  |  FP: {self.cm.fp}
          FN: {self.cm.fn}  |  TP: {self.cm.tp}
        
        M√©tricas:
          Accuracy:  {self.metrics.accuracy():.4f}
          Precision: {self.metrics.precision():.4f}
          Recall:    {self.metrics.recall():.4f}
          F1-Score:  {self.metrics.f1_score():.4f}
          
        Interpretaci√≥n:
          {"Buen balance P/R" if abs(self.metrics.precision() - self.metrics.recall()) < 0.1 else "Desbalance P/R"}
          {"Accuracy confiable" if abs(self.metrics.accuracy() - self.metrics.f1_score()) < 0.1 else "Revisar balance de clases"}
        """
        
        ax6.text(0.1, 0.5, summary, fontsize=10, family='monospace',
                verticalalignment='center')
        
        plt.suptitle(f'Dashboard de Evaluaci√≥n - {self.model_name}', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()

# Usar dashboard
dashboard = EvaluationDashboard(y_true, y_pred, y_proba, model_name="Detector de Fraude")
dashboard.generate_report()
```

## üéØ EJERCICIOS PROPUESTOS

### Nivel B√°sico

**Ejercicio 1:** Matriz de Confusi√≥n Manual
```
Dadas predicciones y etiquetas verdaderas:
a) Construye la matriz de confusi√≥n manualmente
b) Calcula TP, TN, FP, FN
c) Computa Accuracy, Precision, Recall, F1
```

**Ejercicio 2:** Interpretaci√≥n de M√©tricas
```
Dado un problema m√©dico con estas m√©tricas:
- Accuracy: 0.92
- Precision: 0.45
- Recall: 0.95
- F1: 0.61

a) ¬øQu√© nos dicen estas m√©tricas?
b) ¬øEl modelo es √∫til?
c) ¬øQu√© tipo de errores comete m√°s?
```

**Ejercicio 3:** Selecci√≥n de M√©trica
```
Para cada escenario, indica la m√©trica m√°s importante:
a) Detector de spam en email
b) Diagn√≥stico de enfermedad mortal
c) Sistema de aprobaci√≥n de cr√©ditos
d) Clasificaci√≥n de im√°genes balanceadas
```

### Nivel Intermedio

**Ejercicio 4:** Optimizaci√≥n de Umbral
```
Implementa un sistema que:
- Pruebe diferentes umbrales (0.1 a 0.9)
- Grafique Precision y Recall vs Umbral
- Encuentre el umbral que maximiza F1
- Compare con umbral que minimiza FN
```

**Ejercicio 5:** Manejo de Desbalance
```
Dado un dataset 90/10:
- Implementa 3 t√©cnicas de balanceo
- Entrena un modelo en cada versi√≥n
- Compara m√©tricas
- Recomienda la mejor aproximaci√≥n
```

**Ejercicio 6:** K-Fold Completo
```
Implementa K-Fold CV que:
- Use K=5 folds
- Calcule todas las m√©tricas
- Genere intervalo de confianza (95%)
- Visualice resultados por fold
```

### Nivel Avanzado

**Ejercicio 7:** Sistema de Evaluaci√≥n Completo
```
Crea un sistema que:
- Genere matriz de confusi√≥n
- Calcule todas las m√©tricas
- Ejecute K-Fold CV
- Genere dashboard visual
- Produzca reporte PDF
```

**Ejercicio 8:** ROC y AUC
```
Implementa desde cero:
- Curva ROC (TPR vs FPR)
- C√°lculo de AUC
- Comparaci√≥n de m√∫ltiples modelos
- Punto √≥ptimo en la curva
```

**Ejercicio 9:** An√°lisis de Errores
```
Desarrolla un sistema que:
- Identifique patrones en errores (FP y FN)
- Clasifique errores por tipo
- Sugiera mejoras al modelo
- Visualice casos dif√≠ciles
```

## üìù Entregables

### 1. C√≥digo Fuente
- `metrics.py`: Implementaci√≥n de todas las m√©tricas
- `confusion_matrix.py`: Clase de matriz de confusi√≥n
- `cross_validation.py`: K-Fold CV
- `evaluation_dashboard.py`: Dashboard visual
- `experiments.ipynb`: Notebook con experimentos

### 2. Experimentos
- Comparaci√≥n de m√©tricas en diferentes problemas
- An√°lisis de datasets desbalanceados
- Resultados de cross-validation
- Optimizaci√≥n de umbrales

### 3. Visualizaciones
- Matrices de confusi√≥n
- Curvas Precision-Recall
- Comparaciones de modelos
- Dashboards completos

### 4. Reporte (3-4 p√°ginas)
- An√°lisis de diferentes m√©tricas
- Casos de uso apropiados
- Manejo de desbalance
- Conclusiones y recomendaciones

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Conceive (Concebir) - 25%
- [ ] Comprensi√≥n de cada m√©trica y su significado
- [ ] Identificaci√≥n de m√©tricas apropiadas por problema
- [ ] Dise√±o de estrategias de evaluaci√≥n
- [ ] An√°lisis cr√≠tico de resultados

### Design (Dise√±ar) - 25%
- [ ] Implementaci√≥n correcta de m√©tricas
- [ ] C√≥digo limpio y modular
- [ ] Visualizaciones efectivas
- [ ] Sistema de evaluaci√≥n robusto

### Implement (Implementar) - 30%
- [ ] Todas las m√©tricas calculadas correctamente
- [ ] K-Fold CV funcional
- [ ] Manejo de casos edge (divisi√≥n por cero, etc.)
- [ ] Resultados reproducibles

### Operate (Operar) - 20%
- [ ] Experimentaci√≥n exhaustiva
- [ ] Interpretaci√≥n correcta de resultados
- [ ] Recomendaciones fundamentadas
- [ ] Documentaci√≥n clara

## üìã R√∫brica de Evaluaci√≥n

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|--------------|---------------------|------------------|
| **Implementaci√≥n** | Todas las m√©tricas perfectas | M√©tricas correctas | Algunas m√©tricas correctas | Errores en c√°lculos |
| **Comprensi√≥n** | Interpretaci√≥n profunda | Buena interpretaci√≥n | Interpretaci√≥n b√°sica | Interpretaci√≥n pobre |
| **Experimentaci√≥n** | An√°lisis exhaustivo | Buenos experimentos | Experimentos b√°sicos | Experimentos insuficientes |
| **Visualizaci√≥n** | Dashboards profesionales | Buenas visualizaciones | Visualizaciones b√°sicas | Visualizaciones pobres |
| **Aplicaci√≥n** | Selecci√≥n perfecta de m√©tricas | Buena selecci√≥n | Selecci√≥n razonable | Selecci√≥n inadecuada |

## üìö Referencias Adicionales

### Papers y Libros
1. Powers, D. M. (2011). "Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation"
2. Sokolova, M., & Lapalme, G. (2009). "A systematic analysis of performance measures for classification tasks"
3. "Pattern Recognition and Machine Learning" (Bishop) - Cap√≠tulo sobre evaluaci√≥n

### Recursos Online
- Scikit-learn: Documentaci√≥n de m√©tricas
- Towards Data Science: Tutoriales sobre m√©tricas
- Google ML Crash Course: Classification metrics
- Confusion Matrix Calculator (online tools)

### Herramientas
- `sklearn.metrics`: Implementaci√≥n de referencia
- `seaborn`: Visualizaci√≥n de matrices
- `yellowbrick`: Visualizaciones ML avanzadas
- `mlxtend`: Plotting utilities

## üéì Notas Finales

### Gu√≠a R√°pida de M√©tricas

**¬øQu√© m√©trica usar?**

```
Dataset balanceado ‚Üí Accuracy
Dataset desbalanceado ‚Üí F1-Score
FP muy costosos ‚Üí Precision
FN muy costosos ‚Üí Recall
Balance general ‚Üí F1-Score
Multiclase ‚Üí Macro/Weighted F1
```

### Checklist de Evaluaci√≥n

Antes de confiar en un modelo:
- [ ] Calcul√© m√∫ltiples m√©tricas (no solo accuracy)
- [ ] Analic√© la matriz de confusi√≥n
- [ ] Consider√© el balance de clases
- [ ] Us√© cross-validation
- [ ] Interpret√© resultados en contexto del problema
- [ ] Visualic√© resultados
- [ ] Document√© hallazgos

### Errores Comunes

‚ùå **Confiar solo en accuracy en datos desbalanceados**
‚ùå **No considerar el costo de diferentes errores**
‚ùå **Olvidar normalizar matrices de confusi√≥n**
‚ùå **No usar cross-validation para estimaci√≥n robusta**
‚ùå **Ignorar la distribuci√≥n de clases**

### Reflexi√≥n Final

**La evaluaci√≥n correcta es tan importante como el modelo mismo.**

Un modelo con 99% accuracy puede ser in√∫til.
Un modelo con 80% accuracy puede ser invaluable.

**Todo depende del contexto y las m√©tricas correctas.**

### Pr√≥ximos Pasos

En el siguiente laboratorio (Lab 08), aprender√°s:
- Frameworks modernos (PyTorch, TensorFlow)
- Implementaci√≥n eficiente de todo lo aprendido
- M√©tricas automatizadas
- Productizaci√≥n de modelos

¬°La evaluaci√≥n correcta es tan importante como el entrenamiento! üìä

---

**"Torture the data, and it will confess to anything." - Ronald Coase**

**¬°La evaluaci√≥n correcta es tan importante como el entrenamiento! üìä**
