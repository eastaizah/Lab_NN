# Lab 11: Transformers

## ğŸ“‹ DescripciÃ³n

Este laboratorio cubre la arquitectura **Transformer** y el mecanismo de **Self-Attention**, que revolucionaron el Deep Learning y son la base de los modelos mÃ¡s avanzados de IA actual (GPT-4, BERT, DALL-E, ChatGPT, etc.).

## ğŸ¯ Objetivos de Aprendizaje

Al completar este laboratorio, serÃ¡s capaz de:

1. **Entender Self-Attention**: Comprender el mecanismo fundamental de atenciÃ³n con Q, K, V
2. **Implementar Multi-Head Attention**: MÃºltiples cabezas de atenciÃ³n en paralelo
3. **Aplicar Positional Encoding**: Codificar informaciÃ³n de posiciÃ³n en secuencias
4. **Construir Transformer Blocks**: Ensamblar bloques encoder y decoder completos
5. **Fine-tunear BERT**: Adaptar BERT pre-entrenado para anÃ¡lisis de sentimiento
6. **Generar texto con GPT**: Usar GPT-2 para generaciÃ³n autoregresiva
7. **Visualizar AtenciÃ³n**: Interpretar y visualizar patrones de atenciÃ³n
8. **Comparar con RNNs**: Entender ventajas de Transformers sobre arquitecturas recurrentes

## ğŸ“š Contenido

### 1. TeorÃ­a (`teoria.md`)

Documento teÃ³rico completo que cubre:

- **MotivaciÃ³n y limitaciones de RNNs/LSTMs**
- **Self-Attention Mechanism**: Q, K, V y Scaled Dot-Product
- **Multi-Head Attention**: MÃºltiples perspectivas en paralelo
- **Positional Encoding**: Sinusoidal y aprendido
- **Arquitectura Transformer Completa**: Encoder-Decoder
- **Variantes**: BERT (Encoder-only) vs GPT (Decoder-only)
- **Vision Transformers (ViT)**: Transformers para imÃ¡genes
- **Transfer Learning y Fine-tuning**: Pre-entrenamiento y adaptaciÃ³n
- **VisualizaciÃ³n e Interpretabilidad**: Attention maps
- **Aplicaciones Modernas**: ChatGPT, DALL-E, AlphaFold, etc.

### 2. CÃ³digo (`codigo/transformers.py`)

Implementaciones completas en Python:

#### Parte 1: Self-Attention (NumPy)
```python
class SelfAttentionNumPy:
    """Self-attention desde cero con NumPy"""
    
class MultiHeadAttentionNumPy:
    """Multi-head attention con mÃºltiples cabezas"""
```

#### Parte 2: Positional Encoding
```python
class PositionalEncodingSinusoidal:
    """Positional encoding con funciones seno/coseno"""
```

#### Parte 3: Transformer Blocks (PyTorch)
```python
class TransformerEncoderBlock(nn.Module):
    """Bloque encoder: Self-Attention + FFN"""
    
class TransformerDecoderBlock(nn.Module):
    """Bloque decoder: Masked Attention + Cross-Attention + FFN"""
    
class TransformerModel(nn.Module):
    """Transformer completo (Encoder-Decoder)"""
```

#### Parte 4: Hugging Face
```python
class BERTSentimentClassifier:
    """Fine-tuning de BERT para sentimiento"""
    
class GPT2TextGenerator:
    """GeneraciÃ³n de texto con GPT-2"""
```

### 3. PrÃ¡ctica (`practica.ipynb`)

Notebook Jupyter interactivo con:

- **Parte 1**: Self-Attention paso a paso
- **Parte 2**: Multi-Head Attention
- **Parte 3**: Positional Encoding y visualizaciÃ³n
- **Parte 4**: Transformer Blocks completos
- **Parte 5**: BERT Fine-tuning para anÃ¡lisis de sentimiento
- **Parte 6**: GPT-2 Text Generation
- **Parte 7**: ComparaciÃ³n con RNNs/LSTMs
- **Parte 8**: Ejercicios y proyectos avanzados
- Clase `TransformerBlock`: Bloque completo
- Ejemplos con Hugging Face Transformers
- Fine-tuning para tareas especÃ­ficas

## CÃ³mo Usar Este Laboratorio

### OpciÃ³n 1: Jupyter Notebook (Recomendado)

```bash
# Desde el directorio del repositorio
cd Lab11_Transformers
jupyter notebook practica.ipynb
```

### OpciÃ³n 2: Script Python

```bash
# Ejecutar el cÃ³digo de ejemplo
python codigo/transformers.py
```

### OpciÃ³n 3: Lectura y ExperimentaciÃ³n

1. Lee `teoria.md` para entender los conceptos
2. Abre `practica.ipynb` en Jupyter
3. Ejecuta cada celda y experimenta con los parÃ¡metros
4. Completa los ejercicios propuestos
5. Revisa `codigo/transformers.py` como referencia

## Requisitos

```bash
pip install numpy matplotlib jupyter torch transformers datasets
```

## Conceptos Clave

- **Attention**: Mecanismo para enfocarse en partes relevantes del input
- **Self-Attention**: AtenciÃ³n sobre la misma secuencia
- **Query, Key, Value**: Tres proyecciones lineales para calcular atenciÃ³n
- **Multi-Head Attention**: MÃºltiples atenciones en paralelo
- **Positional Encoding**: InformaciÃ³n de posiciÃ³n sin recurrencia
- **Feed-Forward Network**: Red densa despuÃ©s de atenciÃ³n
- **Layer Normalization**: NormalizaciÃ³n para estabilidad
- **Encoder**: Procesa input (ej: BERT)
- **Decoder**: Genera output (ej: GPT)

## Ejercicios

### Ejercicio 11.1: Self-Attention Manual
Calcula attention scores manualmente para una secuencia pequeÃ±a.

### Ejercicio 11.2: Positional Encoding
Implementa y visualiza positional encoding sinusoidal.

### Ejercicio 11.3: Multi-Head Attention
Completa la implementaciÃ³n desde cero.

### Ejercicio 11.4: Fine-tuning BERT
Entrena BERT para clasificaciÃ³n de sentimientos.

### Ejercicio 11.5: GeneraciÃ³n con GPT (DesafÃ­o)
Usa GPT-2 para generar texto coherente.

## Ventajas de Transformers sobre RNNs

1. **ParalelizaciÃ³n**: Procesa toda la secuencia en paralelo (vs secuencial en RNN)
2. **Dependencias Largas**: Captura relaciones a cualquier distancia
3. **Menos Bias Inductivo**: Aprende estructura desde datos
4. **Escalabilidad**: Funciona mejor con mÃ¡s datos y parÃ¡metros
5. **Interpretabilidad**: Attention weights son visualizables

## Arquitecturas Transformer Famosas

### BERT (2018) - Google

**CaracterÃ­sticas:**
- Encoder-only (bidireccional)
- Pre-training: Masked Language Modeling (MLM)
- 340M parÃ¡metros (BERT-large)
- Estado del arte en comprensiÃ³n de lenguaje

**Aplicaciones:**
- ClasificaciÃ³n de texto
- Question Answering
- Named Entity Recognition
- Sentence similarity

### GPT (2018-2023) - OpenAI

**GPT-1/2/3/4:**
- Decoder-only (autoregresivo)
- Pre-training: Next token prediction
- GPT-3: 175B parÃ¡metros
- GPT-4: multimodal

**Aplicaciones:**
- GeneraciÃ³n de texto
- TraducciÃ³n
- Resumen
- ConversaciÃ³n (ChatGPT)

### T5 (2019) - Google

**CaracterÃ­sticas:**
- Encoder-Decoder completo
- Text-to-Text framework
- Todas las tareas como text generation

### Vision Transformer (ViT) - 2020

**InnovaciÃ³n:**
- Aplica Transformers a imÃ¡genes
- Divide imagen en patches
- Trata patches como "tokens"
- Supera CNNs en grandes datasets

### Otras Variantes

- **RoBERTa**: BERT mejorado
- **ALBERT**: BERT mÃ¡s eficiente
- **ELECTRA**: Pre-training mÃ¡s eficiente
- **DeBERTa**: Disentangled attention
- **Llama**: Open-source de Meta
- **Claude**: Anthropic
- **Gemini**: Google multimodal

## Aplicaciones

### Procesamiento de Lenguaje Natural

1. **ComprensiÃ³n**:
   - ClasificaciÃ³n de texto
   - AnÃ¡lisis de sentimientos
   - Named Entity Recognition
   - Question Answering

2. **GeneraciÃ³n**:
   - TraducciÃ³n automÃ¡tica
   - Resumen de texto
   - GeneraciÃ³n creativa
   - DiÃ¡logo (chatbots)

3. **RepresentaciÃ³n**:
   - Embeddings contextuales
   - Similarity search
   - Clustering semÃ¡ntico

### VisiÃ³n Computacional

- **ViT**: ClasificaciÃ³n de imÃ¡genes
- **DETR**: DetecciÃ³n de objetos
- **Segmenter**: SegmentaciÃ³n semÃ¡ntica
- **CLIP**: VisiÃ³n-lenguaje

### Multimodal

- **CLIP**: Imagen + Texto
- **DALL-E**: Texto â†’ Imagen
- **Flamingo**: VisiÃ³n + Lenguaje
- **GPT-4**: Multimodal completo

### Audio

- **Whisper**: Speech recognition
- **AudioLM**: GeneraciÃ³n de audio
- **MusicGen**: GeneraciÃ³n de mÃºsica

### Ciencia

- **AlphaFold**: PredicciÃ³n de proteÃ­nas
- **ESM**: Modelado de secuencias de proteÃ­nas
- **Molecule generation**: DiseÃ±o de fÃ¡rmacos

## Mecanismo de AtenciÃ³n

### Scaled Dot-Product Attention

```python
Attention(Q, K, V) = softmax(Q @ K^T / âˆšd_k) @ V

Donde:
- Q (queries): quÃ© buscar
- K (keys): quÃ© se ofrece
- V (values): quÃ© contenido devolver
- d_k: dimensiÃ³n de keys (para escalar)
```

**Ejemplo intuitivo:**
```
"El gato bebiÃ³ la leche porque estaba hambriento"

Al procesar "estaba", attention se enfoca en "gato" (no "leche")
porque el modelo aprendiÃ³ que "hambriento" se refiere al sujeto.
```

### Multi-Head Attention

```python
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) @ W_O

head_i = Attention(Q @ W_Q^i, K @ W_K^i, V @ W_V^i)
```

**Ventaja:**
- Cada "head" puede enfocarse en diferentes aspectos
- Head 1: sintaxis
- Head 2: semÃ¡ntica
- Head 3: contexto largo
- etc.

## Arquitectura Transformer Completa

### Encoder (ej: BERT)

```
Input Tokens
    â†“
Token Embeddings + Positional Encoding
    â†“
[Multi-Head Self-Attention
    â†“
Add & Norm
    â†“
Feed-Forward Network
    â†“
Add & Norm] Ã— N layers
    â†“
Output Representations
```

### Decoder (ej: GPT)

```
Output Tokens (shifted)
    â†“
Token Embeddings + Positional Encoding
    â†“
[Masked Multi-Head Self-Attention
    â†“
Add & Norm
    â†“
Feed-Forward Network
    â†“
Add & Norm] Ã— N layers
    â†“
Linear â†’ Softmax
    â†“
Next Token Probabilities
```

### Encoder-Decoder (ej: T5)

```
Encoder              Decoder
Input â†’ [Enc Blocks] â†’ [Dec Blocks] â†’ Output
                â†“
        Cross-Attention
```

## Positional Encoding

**Problema:** Self-attention es permutation invariant (sin orden).

**SoluciÃ³n:** Agregar informaciÃ³n de posiciÃ³n.

**Sinusoidal (original):**
```python
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**Aprendido (alternativa):**
- Embeddings de posiciÃ³n entrenables
- Usado en BERT, GPT

## Transfer Learning con Transformers

### 1. Pre-training (costoso, una vez)

**BERT:**
- Masked Language Modeling (MLM)
- Next Sentence Prediction (NSP)

**GPT:**
- Next token prediction

### 2. Fine-tuning (rÃ¡pido, por tarea)

```python
# Cargar modelo pre-entrenado
model = BertForSequenceClassification.from_pretrained('bert-base')

# Fine-tune en datos especÃ­ficos
train(model, task_data)
```

### 3. Prompt Engineering (sin fine-tuning)

**Few-shot learning:**
```
Clasifica sentimiento:
"La pelÃ­cula fue increÃ­ble" â†’ Positivo
"No me gustÃ³ nada" â†’ Negativo
"Estuvo bien" â†’ ?
```

## Limitaciones de Transformers

1. **Costo Computacional**: O(nÂ²) en longitud de secuencia
2. **Memoria**: Attention matrix crece cuadrÃ¡ticamente
3. **Datos**: Requieren grandes cantidades para pre-training
4. **Interpretabilidad**: Modelos muy grandes son cajas negras
5. **Sesgo**: Heredan sesgos de datos de entrenamiento

## Mejoras Recientes

### Efficient Transformers

- **Linformer**: O(n) complexity
- **Performer**: Kernel approximation
- **Longformer**: Sparse attention
- **Big Bird**: Sparse + global attention

### Scaling Laws

- MÃ¡s parÃ¡metros â†’ mejor rendimiento (hasta ~)
- Compute-optimal: balance datos/parÃ¡metros
- Chinchilla scaling laws

## Notas Importantes

âš ï¸ **GPU Requerida**: Transformers grandes requieren GPUs potentes.

ğŸ’¡ **Hugging Face**: Biblioteca estÃ¡ndar para usar modelos pre-entrenados.

ğŸš€ **Fine-tuning > Training from Scratch**: Casi siempre mejor usar modelo pre-entrenado.

âš¡ **Prompting**: Para modelos muy grandes (GPT-4), prompting puede ser suficiente sin fine-tuning.

## PrÃ³ximo Paso

Transformers son la base de modelos generativos modernos:

ğŸ‘‰ **Vuelve a [Lab 08: IA Generativa](../Lab08_IA_Generativa/)** con nuevo contexto sobre Transformers para entender mejor modelos como DALL-E, GPT, etc.

## Recursos Adicionales

### Papers Fundamentales
- **Attention Is All You Need** (2017) - Paper original de Transformers
- **BERT**: Pre-training of Deep Bidirectional Transformers (2018)
- **Language Models are Few-Shot Learners** (GPT-3, 2020)
- **An Image is Worth 16x16 Words** (ViT, 2020)

### Tutoriales
- [The Illustrated Transformer - Jay Alammar](http://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Hugging Face Course](https://huggingface.co/course)
- [Stanford CS224N](http://web.stanford.edu/class/cs224n/)

### Herramientas
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [Hugging Face Datasets](https://huggingface.co/datasets)
- [Weights & Biases](https://wandb.ai/) - Para tracking experiments

## Preguntas Frecuentes

**P: Â¿Por quÃ© Transformers son mejores que RNNs?**  
R: Procesan en paralelo (mÃ¡s rÃ¡pido), capturan dependencias largas mejor, y escalan mejor con mÃ¡s datos/parÃ¡metros.

**P: Â¿CuÃ¡ndo usar BERT vs GPT?**  
R: BERT para comprensiÃ³n (clasificaciÃ³n, Q&A). GPT para generaciÃ³n (texto, traducciÃ³n, diÃ¡logo).

**P: Â¿Puedo entrenar Transformers desde cero?**  
R: Posible pero costoso. Para proyectos, usar modelos pre-entrenados y hacer fine-tuning.

**P: Â¿QuÃ© es mejor: mÃ¡s heads o mÃ¡s layers?**  
R: MÃ¡s layers tÃ­picamente ayuda mÃ¡s. Heads: 8-16 es estÃ¡ndar, mÃ¡s no siempre ayuda.

**P: Â¿Transformers solo para NLP?**  
R: No. ViT para imÃ¡genes, Transformers para audio, video, proteÃ­nas, y mÃ¡s. Es una arquitectura general.

## VerificaciÃ³n de Conocimientos

- [ ] Entiendo el mecanismo de Self-Attention (Q, K, V)
- [ ] Puedo explicar Multi-Head Attention
- [ ] Entiendo por quÃ© se necesita Positional Encoding
- [ ] Conozco la diferencia entre Encoder (BERT) y Decoder (GPT)
- [ ] Puedo implementar Self-Attention desde cero
- [ ] SÃ© usar Hugging Face para fine-tuning
- [ ] Entiendo las ventajas de Transformers sobre RNNs
- [ ] Conozco aplicaciones mÃ¡s allÃ¡ de NLP

## ConclusiÃ³n

**Transformers han revolucionado el deep learning:**

- âœ… Estado del arte en NLP
- âœ… Emergiendo en visiÃ³n computacional
- âœ… Modelos multimodales potentes
- âœ… Base para IA generativa moderna
- âœ… Arquitectura general para muchos dominios

**"Attention is All You Need"** - y tenÃ­an razÃ³n! ğŸš€

---

**Â¡Has completado los laboratorios de Neural Networks!** ğŸ“

Ahora tienes las bases para entender y aplicar:
- Redes neuronales desde cero
- CNNs para visiÃ³n
- RNNs/LSTMs para secuencias
- Transformers para todo
- IA Generativa moderna
