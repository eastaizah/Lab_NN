{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11: Transformers - Pr√°ctica\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "En este laboratorio aprender√°s:\n",
    "\n",
    "1. **Self-Attention**: Implementaci√≥n paso a paso del mecanismo de atenci√≥n\n",
    "2. **Multi-Head Attention**: M√∫ltiples cabezas de atenci√≥n en paralelo\n",
    "3. **Positional Encoding**: C√≥mo a√±adir informaci√≥n de posici√≥n\n",
    "4. **Transformer Blocks**: Construcci√≥n de bloques encoder y decoder\n",
    "5. **BERT Fine-tuning**: Adaptar BERT para an√°lisis de sentimiento\n",
    "6. **GPT-2 Generation**: Generaci√≥n de texto con GPT-2\n",
    "7. **Attention Visualization**: Visualizar y entender patrones de atenci√≥n\n",
    "8. **Comparaci√≥n con RNNs**: Ventajas de Transformers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup e Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Importar implementaciones\n",
    "import sys\n",
    "sys.path.append('./codigo')\n",
    "from transformers import (\n",
    "    SelfAttentionNumPy, MultiHeadAttentionNumPy,\n",
    "    PositionalEncodingSinusoidal, create_causal_mask,\n",
    "    visualize_attention_weights, visualize_multi_head_attention,\n",
    "    visualize_positional_encoding\n",
    ")\n",
    "\n",
    "print(\"‚úì Imports completados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 1: Self-Attention Paso a Paso\n",
    "\n",
    "### 1.1 Concepto de Self-Attention\n",
    "\n",
    "Self-attention permite que cada palabra \"atienda\" a todas las dem√°s para determinar su representaci√≥n contextual.\n",
    "\n",
    "**F√≥rmula:**\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q¬∑K^T / ‚àöd_k) ¬∑ V\n",
    "```\n",
    "\n",
    "**Componentes:**\n",
    "- **Q (Query)**: \"¬øQu√© estoy buscando?\"\n",
    "- **K (Key)**: \"¬øQu√© informaci√≥n tengo?\"\n",
    "- **V (Value)**: \"La informaci√≥n que proporciono\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo simple: Calcular atenci√≥n manualmente\n",
    "\n",
    "# Embeddings de ejemplo para 3 palabras\n",
    "# \"El gato duerme\"\n",
    "np.random.seed(42)\n",
    "d_model = 4\n",
    "seq_len = 3\n",
    "\n",
    "X = np.array([\n",
    "    [1.0, 0.5, 0.2, 0.1],  # \"El\"\n",
    "    [0.2, 1.0, 0.3, 0.5],  # \"gato\"\n",
    "    [0.3, 0.4, 1.0, 0.2]   # \"duerme\"\n",
    "])\n",
    "\n",
    "print(\"Entrada X (embeddings):\")\n",
    "print(X)\n",
    "print(f\"Shape: {X.shape} (seq_len={seq_len}, d_model={d_model})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Crear matrices W_Q, W_K, W_V (simplificadas como identidad)\n",
    "d_k = d_model\n",
    "W_Q = np.eye(d_model)\n",
    "W_K = np.eye(d_model)\n",
    "W_V = np.eye(d_model)\n",
    "\n",
    "# Paso 2: Proyectar a Q, K, V\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(\"Query Q:\")\n",
    "print(Q)\n",
    "print(\"\\nKey K:\")\n",
    "print(K)\n",
    "print(\"\\nValue V:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Calcular scores de atenci√≥n (Q¬∑K^T)\n",
    "scores = Q @ K.T\n",
    "\n",
    "print(\"Scores (Q¬∑K^T):\")\n",
    "print(scores)\n",
    "print(f\"\\nInterpretaci√≥n: scores[i,j] = compatibilidad entre palabra i y palabra j\")\n",
    "print(f\"scores[0,1] = {scores[0,1]:.3f} ‚Üí compatibilidad 'El' con 'gato'\")\n",
    "print(f\"scores[1,2] = {scores[1,2]:.3f} ‚Üí compatibilidad 'gato' con 'duerme'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Escalar por ‚àöd_k\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "\n",
    "print(f\"Scores escalados (dividir por ‚àö{d_k} = {np.sqrt(d_k):.2f}):\")\n",
    "print(scaled_scores)\n",
    "print(f\"\\n¬øPor qu√© escalar? Para mantener varianza estable y gradientes saludables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Aplicar softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)\n",
    "\n",
    "print(\"Pesos de atenci√≥n (despu√©s de softmax):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nVerificaci√≥n: Cada fila suma 1.0\")\n",
    "print(f\"Suma fila 0: {attention_weights[0].sum():.4f}\")\n",
    "print(f\"Suma fila 1: {attention_weights[1].sum():.4f}\")\n",
    "print(f\"Suma fila 2: {attention_weights[2].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Aplicar atenci√≥n a valores\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(\"Output final:\")\n",
    "print(output)\n",
    "print(f\"\\nShape: {output.shape}\")\n",
    "print(f\"\\nInterpretaci√≥n:\")\n",
    "print(f\"output[0] = representaci√≥n contextual de 'El'\")\n",
    "print(f\"         = {attention_weights[0,0]:.3f} * V[0] + {attention_weights[0,1]:.3f} * V[1] + {attention_weights[0,2]:.3f} * V[2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matriz de atenci√≥n\n",
    "tokens = ['El', 'gato', 'duerme']\n",
    "fig = visualize_attention_weights(attention_weights, tokens, \"Self-Attention: Paso a Paso\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretaci√≥n de la matriz:\")\n",
    "print(\"- Filas: Palabras que atienden (Queries)\")\n",
    "print(\"- Columnas: Palabras atendidas (Keys)\")\n",
    "print(\"- Valores: Cu√°nta atenci√≥n se da\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Self-Attention con Nuestra Implementaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar nuestra clase SelfAttentionNumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Secuencia m√°s larga\n",
    "seq_len, d_model = 7, 16\n",
    "X = np.random.randn(seq_len, d_model) * 0.5\n",
    "\n",
    "# Crear self-attention\n",
    "attention = SelfAttentionNumPy(d_model, d_k=16, seed=42)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = attention(X, return_attention=True)\n",
    "\n",
    "print(f\"Entrada: {X.shape}\")\n",
    "print(f\"Salida: {output.shape}\")\n",
    "print(f\"Pesos de atenci√≥n: {attn_weights.shape}\")\n",
    "\n",
    "# Visualizar\n",
    "tokens = ['El', 'perro', 'negro', 'corri√≥', 'por', 'el', 'parque']\n",
    "fig = visualize_attention_weights(attn_weights, tokens, \"Self-Attention: Ejemplo Completo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 1: Self-Attention con M√°scara Causal\n",
    "\n",
    "Implementa una m√°scara causal para prevenir que las palabras atiendan al futuro (necesario en GPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1: M√°scara Causal\n",
    "\n",
    "# TODO: Crear m√°scara causal\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"M√°scara causal:\")\n",
    "print(causal_mask.astype(int))\n",
    "print(\"\\nTrue = posici√≥n enmascarada (no puede atender)\")\n",
    "\n",
    "# TODO: Aplicar self-attention con m√°scara\n",
    "output_masked, attn_masked = attention(X, mask=causal_mask, return_attention=True)\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sin m√°scara\n",
    "im1 = axes[0].imshow(attn_weights, cmap='viridis', aspect='auto')\n",
    "axes[0].set_title('Sin m√°scara (bidireccional)')\n",
    "axes[0].set_xlabel('Key')\n",
    "axes[0].set_ylabel('Query')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Con m√°scara\n",
    "im2 = axes[1].imshow(attn_masked, cmap='viridis', aspect='auto')\n",
    "axes[1].set_title('Con m√°scara causal (GPT-style)')\n",
    "axes[1].set_xlabel('Key')\n",
    "axes[1].set_ylabel('Query')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìå Observa: Con m√°scara, cada posici√≥n solo atiende a posiciones anteriores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2: Multi-Head Attention\n",
    "\n",
    "### 2.1 ¬øPor qu√© Multi-Head?\n",
    "\n",
    "- **M√∫ltiples perspectivas**: Diferentes cabezas aprenden diferentes tipos de relaciones\n",
    "- **Mayor capacidad**: Cada cabeza se especializa\n",
    "- **Paralelizaci√≥n**: Todas las cabezas se computan simult√°neamente\n",
    "\n",
    "**Ejemplo:**\n",
    "- Head 1: Relaciones sint√°cticas (sujeto-verbo)\n",
    "- Head 2: Relaciones sem√°nticas (palabras relacionadas)\n",
    "- Head 3: Posiciones relativas\n",
    "- Head 4: Correferencia (pronombres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Multi-Head Attention\n",
    "np.random.seed(42)\n",
    "\n",
    "seq_len, d_model, num_heads = 6, 64, 8\n",
    "X = np.random.randn(seq_len, d_model) * 0.3\n",
    "\n",
    "# Multi-head attention\n",
    "mha = MultiHeadAttentionNumPy(d_model, num_heads, seed=42)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_list = mha(X, return_attention=True)\n",
    "\n",
    "print(f\"Entrada: {X.shape}\")\n",
    "print(f\"N√∫mero de cabezas: {num_heads}\")\n",
    "print(f\"Dimensi√≥n por cabeza: {d_model // num_heads}\")\n",
    "print(f\"Salida: {output.shape}\")\n",
    "print(f\"\\nAtenci√≥n por cabeza:\")\n",
    "for i, attn in enumerate(attn_list):\n",
    "    print(f\"  Head {i+1}: {attn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar diferentes cabezas\n",
    "tokens = ['La', 'ni√±a', 'lee', 'un', 'libro', 'interesante']\n",
    "fig = visualize_multi_head_attention(attn_list, tokens, num_heads_to_show=4)\n",
    "plt.suptitle('Multi-Head Attention: Diferentes Patrones', fontsize=14, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Observaci√≥n:\")\n",
    "print(\"Cada cabeza aprende diferentes patrones de atenci√≥n\")\n",
    "print(\"Algunas se enfocan en palabras cercanas, otras en relaciones espec√≠ficas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 2: Comparar N√∫mero de Cabezas\n",
    "\n",
    "Experimenta con diferente n√∫mero de cabezas y observa los patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2: Comparar diferentes n√∫meros de cabezas\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(6, 64) * 0.3\n",
    "tokens = ['El', 'gato', 'persigue', 'al', 'rat√≥n', 'r√°pido']\n",
    "\n",
    "head_configs = [1, 4, 8, 16]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, num_heads in enumerate(head_configs):\n",
    "    # TODO: Crear multi-head attention con num_heads\n",
    "    mha = MultiHeadAttentionNumPy(64, num_heads, seed=42)\n",
    "    \n",
    "    # TODO: Calcular atenci√≥n\n",
    "    output, attn_list = mha(X, return_attention=True)\n",
    "    \n",
    "    # Promediar todas las cabezas\n",
    "    avg_attn = np.mean(np.array(attn_list), axis=0)\n",
    "    \n",
    "    # Visualizar\n",
    "    im = axes[idx].imshow(avg_attn, cmap='viridis', aspect='auto')\n",
    "    axes[idx].set_title(f'{num_heads} cabezas')\n",
    "    axes[idx].set_xticks(range(len(tokens)))\n",
    "    axes[idx].set_yticks(range(len(tokens)))\n",
    "    axes[idx].set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    axes[idx].set_yticklabels(tokens)\n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Pregunta de reflexi√≥n:\")\n",
    "print(\"¬øC√≥mo cambian los patrones con m√°s cabezas?\")\n",
    "print(\"¬øHay un punto de rendimiento decreciente?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 3: Positional Encoding\n",
    "\n",
    "### 3.1 El Problema de la Posici√≥n\n",
    "\n",
    "Self-attention es **permutation-invariant**: no distingue orden.\n",
    "\n",
    "```\n",
    "\"El gato persigue al rat√≥n\" = \"rat√≥n al persigue gato El\" ‚ùå\n",
    "```\n",
    "\n",
    "**Soluci√≥n:** A√±adir informaci√≥n de posici√≥n mediante Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear positional encoding\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "\n",
    "pe = PositionalEncodingSinusoidal(d_model, max_len)\n",
    "\n",
    "print(f\"Positional Encoding creado:\")\n",
    "print(f\"  Dimensi√≥n del modelo: {d_model}\")\n",
    "print(f\"  Longitud m√°xima: {max_len}\")\n",
    "print(f\"  Shape: {pe.encoding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar positional encoding\n",
    "fig = visualize_positional_encoding(d_model=128, max_len=100)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® Interpretaci√≥n:\")\n",
    "print(\"- Diferentes frecuencias en diferentes dimensiones\")\n",
    "print(\"- Dimensiones bajas: frecuencias altas (cambio r√°pido)\")\n",
    "print(\"- Dimensiones altas: frecuencias bajas (cambio lento)\")\n",
    "print(\"- Patr√≥n √∫nico para cada posici√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examinar encoding de posiciones espec√≠ficas\n",
    "positions_to_check = [0, 10, 20, 50]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for pos in positions_to_check:\n",
    "    encoding_pos = pe.get_encoding(pos+1)[pos]\n",
    "    plt.plot(encoding_pos, label=f'pos={pos}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Dimensi√≥n')\n",
    "plt.ylabel('Valor de encoding')\n",
    "plt.title('Positional Encoding para diferentes posiciones')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Cada posici√≥n tiene un patr√≥n √∫nico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 3: Similaridad entre Posiciones\n",
    "\n",
    "Calcula la similitud coseno entre encodings de diferentes posiciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 3: Similaridad entre posiciones\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calcula similitud coseno entre dos vectores.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# TODO: Obtener encoding para 50 posiciones\n",
    "seq_len = 50\n",
    "encodings = pe.get_encoding(seq_len)\n",
    "\n",
    "# TODO: Calcular matriz de similaridad\n",
    "similarity_matrix = np.zeros((seq_len, seq_len))\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        similarity_matrix[i, j] = cosine_similarity(encodings[i], encodings[j])\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Similitud coseno')\n",
    "plt.xlabel('Posici√≥n')\n",
    "plt.ylabel('Posici√≥n')\n",
    "plt.title('Similaridad entre Positional Encodings')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä An√°lisis:\")\n",
    "print(f\"Similaridad pos[0] vs pos[1]: {similarity_matrix[0, 1]:.3f}\")\n",
    "print(f\"Similaridad pos[0] vs pos[10]: {similarity_matrix[0, 10]:.3f}\")\n",
    "print(f\"Similaridad pos[0] vs pos[49]: {similarity_matrix[0, 49]:.3f}\")\n",
    "print(\"\\nüí° Posiciones cercanas tienen mayor similaridad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 4: Transformer Block Completo (PyTorch)\n",
    "\n",
    "Ahora construiremos un bloque transformer completo usando PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar PyTorch y nuestras implementaciones\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import (\n",
    "        TransformerEncoderBlock, TransformerDecoderBlock,\n",
    "        PositionwiseFeedForward, TransformerModel\n",
    "    )\n",
    "    PYTORCH_AVAILABLE = True\n",
    "    print(\"‚úì PyTorch importado correctamente\")\n",
    "except ImportError:\n",
    "    PYTORCH_AVAILABLE = False\n",
    "    print(\"‚ö† PyTorch no disponible. Esta secci√≥n se omitir√°.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    # Crear un Transformer Encoder Block\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    dropout = 0.1\n",
    "    \n",
    "    encoder_block = TransformerEncoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "    \n",
    "    print(\"Transformer Encoder Block:\")\n",
    "    print(encoder_block)\n",
    "    \n",
    "    # Contar par√°metros\n",
    "    num_params = sum(p.numel() for p in encoder_block.parameters())\n",
    "    print(f\"\\nN√∫mero de par√°metros: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    # Test forward pass\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    \n",
    "    # Input aleatorio\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = encoder_block(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"\\n‚úì La dimensi√≥n se preserva: (batch, seq_len, d_model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Transformer Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    # Crear Transformer completo\n",
    "    vocab_size = 10000\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    d_ff = 2048\n",
    "    \n",
    "    transformer = TransformerModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        d_ff=d_ff,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"Transformer Model (Encoder-Decoder):\")\n",
    "    print(f\"  Vocabulario: {vocab_size}\")\n",
    "    print(f\"  Dimensi√≥n del modelo: {d_model}\")\n",
    "    print(f\"  Capas: {num_layers}\")\n",
    "    print(f\"  Cabezas de atenci√≥n: {num_heads}\")\n",
    "    \n",
    "    # Contar par√°metros\n",
    "    total_params = sum(p.numel() for p in transformer.parameters())\n",
    "    print(f\"\\nTotal de par√°metros: {total_params:,}\")\n",
    "    print(f\"Tama√±o aproximado: {total_params * 4 / 1024**2:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    # Test forward pass del transformer completo\n",
    "    batch_size = 2\n",
    "    src_len = 15\n",
    "    tgt_len = 12\n",
    "    \n",
    "    # Secuencias de entrada (√≠ndices de tokens)\n",
    "    src = torch.randint(0, vocab_size, (batch_size, src_len))\n",
    "    tgt = torch.randint(0, vocab_size, (batch_size, tgt_len))\n",
    "    \n",
    "    # Crear m√°scara causal para decoder\n",
    "    tgt_mask = transformer.generate_square_subsequent_mask(tgt_len)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "    \n",
    "    print(f\"Source shape: {src.shape}\")\n",
    "    print(f\"Target shape: {tgt.shape}\")\n",
    "    print(f\"Output logits shape: {logits.shape}\")\n",
    "    print(f\"\\nOutput: (batch={batch_size}, tgt_len={tgt_len}, vocab_size={vocab_size})\")\n",
    "    print(\"\\n‚úì Para cada posici√≥n en target, predice distribuci√≥n sobre vocabulario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 5: BERT Fine-tuning para An√°lisis de Sentimiento\n",
    "\n",
    "Usaremos Hugging Face Transformers para fine-tuning de BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar Hugging Face Transformers\n",
    "try:\n",
    "    from transformers import BERTSentimentClassifier\n",
    "    HF_AVAILABLE = True\n",
    "    print(\"‚úì Hugging Face Transformers disponible\")\n",
    "except:\n",
    "    HF_AVAILABLE = False\n",
    "    print(\"‚ö† Hugging Face Transformers no disponible\")\n",
    "    print(\"Instalar con: pip install transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    # Crear clasificador de sentimientos con BERT\n",
    "    print(\"Cargando BERT pre-entrenado...\")\n",
    "    classifier = BERTSentimentClassifier(\n",
    "        model_name='bert-base-uncased',\n",
    "        num_labels=2  # Positivo/Negativo\n",
    "    )\n",
    "    print(\"‚úì BERT cargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    # Datos de ejemplo para fine-tuning\n",
    "    train_texts = [\n",
    "        \"This movie is absolutely fantastic! I loved it.\",\n",
    "        \"Terrible film. Complete waste of time.\",\n",
    "        \"Great performances and amazing cinematography.\",\n",
    "        \"Boring and predictable. Not recommended.\",\n",
    "        \"One of the best movies I've ever seen!\",\n",
    "        \"Awful. I couldn't finish watching it.\",\n",
    "        \"Brilliant storytelling and excellent acting.\",\n",
    "        \"Very disappointing. Expected much more.\"\n",
    "    ]\n",
    "    \n",
    "    train_labels = [1, 0, 1, 0, 1, 0, 1, 0]  # 1=Positivo, 0=Negativo\n",
    "    \n",
    "    print(f\"Dataset de entrenamiento: {len(train_texts)} ejemplos\")\n",
    "    print(f\"\\nEjemplos:\")\n",
    "    for text, label in zip(train_texts[:3], train_labels[:3]):\n",
    "        sentiment = \"Positivo\" if label == 1 else \"Negativo\"\n",
    "        print(f\"  [{sentiment}] {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE and PYTORCH_AVAILABLE:\n",
    "    # Fine-tuning simple\n",
    "    optimizer = torch.optim.AdamW(classifier.model.parameters(), lr=2e-5)\n",
    "    \n",
    "    print(\"Entrenando...\\n\")\n",
    "    num_epochs = 3\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Un paso de entrenamiento\n",
    "        loss = classifier.train_step(train_texts, train_labels, optimizer)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úì Fine-tuning completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    # Evaluar con nuevos textos\n",
    "    test_texts = [\n",
    "        \"I really enjoyed this film. Highly recommend!\",\n",
    "        \"Not good at all. Very disappointing.\",\n",
    "        \"Amazing movie with great actors.\"\n",
    "    ]\n",
    "    \n",
    "    predictions, probabilities = classifier.predict(test_texts)\n",
    "    \n",
    "    print(\"Predicciones:\\n\")\n",
    "    for text, pred, probs in zip(test_texts, predictions, probabilities):\n",
    "        sentiment = \"Positivo\" if pred == 1 else \"Negativo\"\n",
    "        confidence = probs[pred] * 100\n",
    "        print(f\"Texto: {text}\")\n",
    "        print(f\"Predicci√≥n: {sentiment} (confianza: {confidence:.1f}%)\")\n",
    "        print(f\"Probabilidades: Neg={probs[0]:.3f}, Pos={probs[1]:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualizar Atenci√≥n de BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    # Extraer pesos de atenci√≥n\n",
    "    text = \"This movie is absolutely fantastic and amazing!\"\n",
    "    \n",
    "    # Obtener atenci√≥n de √∫ltima capa\n",
    "    attention = classifier.get_attention_weights(text, layer=-1)\n",
    "    \n",
    "    print(f\"Atenci√≥n extra√≠da:\")\n",
    "    print(f\"  Shape: {attention.shape}\")\n",
    "    print(f\"  (num_heads={attention.shape[0]}, seq_len={attention.shape[1]})\")\n",
    "    \n",
    "    # Tokenizar para obtener tokens\n",
    "    tokens = classifier.tokenizer.tokenize(text)\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    \n",
    "    # Visualizar primera cabeza\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention[0], cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Peso de atenci√≥n')\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.xlabel('Key (tokens atendidos)')\n",
    "    plt.ylabel('Query (tokens que atienden)')\n",
    "    plt.title('BERT Attention - Head 1, √öltima Capa')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüîç Observaci√≥n:\")\n",
    "    print(\"BERT aprende a atender a palabras relevantes para el sentimiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 6: GPT-2 Text Generation\n",
    "\n",
    "Generaci√≥n de texto usando GPT-2 pre-entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    from transformers import GPT2TextGenerator\n",
    "    \n",
    "    print(\"Cargando GPT-2...\")\n",
    "    generator = GPT2TextGenerator(model_name='gpt2')\n",
    "    print(\"‚úì GPT-2 cargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    # Generar texto\n",
    "    prompt = \"Once upon a time in a magical forest,\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"Generando...\\n\")\n",
    "    \n",
    "    generated_texts = generator.generate(\n",
    "        prompt=prompt,\n",
    "        max_length=100,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=3\n",
    "    )\n",
    "    \n",
    "    for i, text in enumerate(generated_texts, 1):\n",
    "        print(f\"--- Generaci√≥n {i} ---\")\n",
    "        print(text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_AVAILABLE:\n",
    "    # Analizar probabilidades del siguiente token\n",
    "    text = \"The capital of France is\"\n",
    "    \n",
    "    token_probs = generator.get_next_token_probabilities(text, top_k=10)\n",
    "    \n",
    "    print(f\"Texto: '{text}'\")\n",
    "    print(f\"\\nTop-10 tokens m√°s probables:\\n\")\n",
    "    \n",
    "    for token, prob in token_probs:\n",
    "        print(f\"  '{token}': {prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 4: Experimentar con Par√°metros de Generaci√≥n\n",
    "\n",
    "Prueba diferentes valores de temperatura, top_k y top_p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 4: Experimentar con generaci√≥n\n",
    "\n",
    "if HF_AVAILABLE:\n",
    "    prompt = \"Artificial intelligence is\"\n",
    "    \n",
    "    configs = [\n",
    "        {'temperature': 0.5, 'top_k': 50, 'name': 'Conservadora (T=0.5)'},\n",
    "        {'temperature': 1.0, 'top_k': 50, 'name': 'Balanceada (T=1.0)'},\n",
    "        {'temperature': 1.5, 'top_k': 50, 'name': 'Creativa (T=1.5)'},\n",
    "    ]\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{config['name']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # TODO: Generar texto con configuraci√≥n espec√≠fica\n",
    "        texts = generator.generate(\n",
    "            prompt=prompt,\n",
    "            max_length=80,\n",
    "            temperature=config['temperature'],\n",
    "            top_k=config['top_k'],\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        \n",
    "        print(texts[0])\n",
    "    \n",
    "    print(\"\\nüí° Observaci√≥n:\")\n",
    "    print(\"- Temperatura baja ‚Üí Texto m√°s predecible\")\n",
    "    print(\"- Temperatura alta ‚Üí Texto m√°s creativo/aleatorio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 7: Comparaci√≥n con RNNs/LSTMs\n",
    "\n",
    "### 7.1 Ventajas de Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n de complejidad computacional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Aspecto': 'Complejidad por capa',\n",
    "        'RNN/LSTM': 'O(n¬∑d¬≤)',\n",
    "        'Transformer': 'O(n¬≤¬∑d)',\n",
    "        'Ganador': 'Depende de n vs d'\n",
    "    },\n",
    "    {\n",
    "        'Aspecto': 'Operaciones secuenciales',\n",
    "        'RNN/LSTM': 'O(n)',\n",
    "        'Transformer': 'O(1)',\n",
    "        'Ganador': 'Transformer'\n",
    "    },\n",
    "    {\n",
    "        'Aspecto': 'Camino m√°ximo',\n",
    "        'RNN/LSTM': 'O(n)',\n",
    "        'Transformer': 'O(1)',\n",
    "        'Ganador': 'Transformer'\n",
    "    },\n",
    "    {\n",
    "        'Aspecto': 'Paralelizaci√≥n',\n",
    "        'RNN/LSTM': 'No',\n",
    "        'Transformer': 'S√≠',\n",
    "        'Ganador': 'Transformer'\n",
    "    },\n",
    "    {\n",
    "        'Aspecto': 'Dependencias largas',\n",
    "        'RNN/LSTM': 'Dif√≠cil',\n",
    "        'Transformer': 'F√°cil',\n",
    "        'Ganador': 'Transformer'\n",
    "    },\n",
    "    {\n",
    "        'Aspecto': 'Memoria para inferencia',\n",
    "        'RNN/LSTM': 'O(d)',\n",
    "        'Transformer': 'O(n¬∑d)',\n",
    "        'Ganador': 'RNN/LSTM'\n",
    "    },\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARACI√ìN: RNN/LSTM vs Transformer\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular tiempo de procesamiento\n",
    "\n",
    "def simulate_processing_time(architecture, seq_len, d_model=512):\n",
    "    \"\"\"\n",
    "    Simula tiempo relativo de procesamiento.\n",
    "    No es tiempo real, solo comparaci√≥n relativa.\n",
    "    \"\"\"\n",
    "    if architecture == 'RNN':\n",
    "        # Procesamiento secuencial: O(n)\n",
    "        return seq_len\n",
    "    elif architecture == 'Transformer':\n",
    "        # Procesamiento paralelo pero O(n¬≤)\n",
    "        return seq_len ** 1.5  # Simplificado\n",
    "\n",
    "seq_lengths = [10, 20, 50, 100, 200, 500]\n",
    "rnn_times = [simulate_processing_time('RNN', n) for n in seq_lengths]\n",
    "transformer_times = [simulate_processing_time('Transformer', n) for n in seq_lengths]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lengths, rnn_times, marker='o', label='RNN (secuencial)', linewidth=2)\n",
    "plt.plot(seq_lengths, transformer_times, marker='s', label='Transformer (paralelo)', linewidth=2)\n",
    "plt.xlabel('Longitud de secuencia')\n",
    "plt.ylabel('Tiempo relativo')\n",
    "plt.title('Tiempo de Procesamiento: RNN vs Transformer')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä An√°lisis:\")\n",
    "print(\"- RNN: Crece linealmente pero es secuencial (lento en GPUs)\")\n",
    "print(\"- Transformer: Crece m√°s r√°pido pero se puede paralelizar\")\n",
    "print(\"- En la pr√°ctica con GPUs, Transformers son mucho m√°s r√°pidos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 8: Ejercicios Finales\n",
    "\n",
    "### üéØ Ejercicio 5: An√°lisis de Atenci√≥n en Diferentes Capas\n",
    "\n",
    "Analiza c√≥mo cambian los patrones de atenci√≥n a trav√©s de las capas de BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 5: Atenci√≥n en diferentes capas\n",
    "\n",
    "if HF_AVAILABLE:\n",
    "    from transformers import compare_attention_patterns\n",
    "    \n",
    "    text = \"The quick brown fox jumps over the lazy dog\"\n",
    "    \n",
    "    # TODO: Visualizar atenci√≥n en capas 0, 6 y 11\n",
    "    fig = compare_attention_patterns(text, layers_to_show=[0, 6, 11])\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Preguntas de reflexi√≥n:\")\n",
    "    print(\"1. ¬øC√≥mo cambian los patrones de atenci√≥n entre capas?\")\n",
    "    print(\"2. ¬øLas capas tempranas son m√°s sint√°cticas o sem√°nticas?\")\n",
    "    print(\"3. ¬øQu√© patrones observas en la capa final?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 6: Fine-tuning Personalizado\n",
    "\n",
    "Crea tu propio dataset y fine-tunea BERT para una tarea espec√≠fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 6: Tu propio fine-tuning\n",
    "\n",
    "if HF_AVAILABLE and PYTORCH_AVAILABLE:\n",
    "    # TODO: Define tu dataset (por ejemplo, clasificaci√≥n de spam)\n",
    "    custom_texts = [\n",
    "        # A√±ade tus ejemplos aqu√≠\n",
    "        \"Get rich quick! Click now!\",  # Spam\n",
    "        \"Meeting tomorrow at 3pm\",      # No spam\n",
    "        # ... m√°s ejemplos\n",
    "    ]\n",
    "    \n",
    "    custom_labels = [\n",
    "        1,  # 1 = Spam\n",
    "        0,  # 0 = No spam\n",
    "        # ...\n",
    "    ]\n",
    "    \n",
    "    # TODO: Crea un clasificador\n",
    "    # spam_classifier = BERTSentimentClassifier(...)\n",
    "    \n",
    "    # TODO: Entrena\n",
    "    # ...\n",
    "    \n",
    "    # TODO: Eval√∫a\n",
    "    # ...\n",
    "    \n",
    "    print(\"\\nüéì Completa este ejercicio con tu propia tarea de clasificaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resumen y Conclusiones\n",
    "\n",
    "### ‚úÖ Lo que aprendimos:\n",
    "\n",
    "1. **Self-Attention**: Mecanismo fundamental que permite capturar relaciones entre elementos\n",
    "2. **Multi-Head Attention**: M√∫ltiples perspectivas en paralelo para mayor capacidad\n",
    "3. **Positional Encoding**: Soluci√≥n elegante para incorporar informaci√≥n de posici√≥n\n",
    "4. **Transformer Architecture**: Bloques encoder-decoder con residual connections y layer norm\n",
    "5. **BERT vs GPT**: Encoder-only (comprensi√≥n) vs Decoder-only (generaci√≥n)\n",
    "6. **Transfer Learning**: Pre-entrenamiento + fine-tuning para tareas espec√≠ficas\n",
    "7. **Visualizaci√≥n**: Interpretaci√≥n de patrones de atenci√≥n\n",
    "\n",
    "### üöÄ Ventajas clave de Transformers:\n",
    "\n",
    "- ‚úÖ Paralelizaci√≥n completa\n",
    "- ‚úÖ Captura dependencias de largo alcance\n",
    "- ‚úÖ Escalabilidad a modelos gigantes\n",
    "- ‚úÖ Versatilidad (NLP, visi√≥n, audio, etc.)\n",
    "- ‚úÖ Transfer learning efectivo\n",
    "\n",
    "### üìö Pr√≥ximos pasos:\n",
    "\n",
    "1. Experimenta con modelos m√°s grandes (BERT-Large, GPT-3)\n",
    "2. Prueba Vision Transformers (ViT) para im√°genes\n",
    "3. Explora modelos multimodales (CLIP, Flamingo)\n",
    "4. Estudia optimizaciones (Flash Attention, sparse attention)\n",
    "5. Implementa tu propia aplicaci√≥n con Transformers\n",
    "\n",
    "### üéØ Desaf√≠os avanzados:\n",
    "\n",
    "- Implementa un Transformer desde cero para traducci√≥n\n",
    "- Fine-tunea GPT-2 en tu propio corpus de texto\n",
    "- Crea un sistema de Q&A usando BERT\n",
    "- Experimenta con prompt engineering en GPT\n",
    "- Visualiza e interpreta attention maps en profundidad\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ¬°Felicitaciones!\n",
    "\n",
    "Has completado el laboratorio de Transformers. Ahora entiendes la arquitectura que ha revolucionado el Deep Learning y que alimenta los modelos m√°s avanzados de IA actuales (ChatGPT, GPT-4, DALL-E, etc.).\n",
    "\n",
    "**\"Attention is All You Need\"** - Vaswani et al., 2017\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
