{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Funciones de Activaci√≥n - Pr√°ctica Interactiva\n",
    "\n",
    "En este notebook implementaremos y experimentaremos con diferentes funciones de activaci√≥n.\n",
    "\n",
    "## Objetivos\n",
    "1. Implementar funciones de activaci√≥n desde cero\n",
    "2. Visualizar su comportamiento\n",
    "3. Entender sus derivadas\n",
    "4. Comparar su desempe√±o en redes neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('codigo/')\n",
    "\n",
    "# Configuraci√≥n de matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Implementaci√≥n de Funciones de Activaci√≥n\n",
    "\n",
    "### Ejercicio 1.1: Implementar Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Implementa la funci√≥n sigmoid.\n",
    "    \n",
    "    F√≥rmula: œÉ(x) = 1 / (1 + e^(-x))\n",
    "    \"\"\"\n",
    "    # TU C√ìDIGO AQU√ç\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Implementa la derivada de sigmoid.\n",
    "    \n",
    "    F√≥rmula: œÉ'(x) = œÉ(x) * (1 - œÉ(x))\n",
    "    \"\"\"\n",
    "    # TU C√ìDIGO AQU√ç\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Probar implementaci√≥n\n",
    "x_test = np.array([0, 1, -1, 2, -2])\n",
    "print(\"Sigmoid(x):\", sigmoid(x_test))\n",
    "print(\"Sigmoid'(x):\", sigmoid_derivative(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1.2: Implementar ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Implementa ReLU.\n",
    "    \n",
    "    F√≥rmula: ReLU(x) = max(0, x)\n",
    "    \"\"\"\n",
    "    # TU C√ìDIGO AQU√ç\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Implementa la derivada de ReLU.\n",
    "    \"\"\"\n",
    "    # TU C√ìDIGO AQU√ç\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Probar implementaci√≥n\n",
    "print(\"ReLU(x):\", relu(x_test))\n",
    "print(\"ReLU'(x):\", relu_derivative(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1.3: Implementar Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    \"\"\"Implementa tanh usando NumPy.\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"\n",
    "    Implementa la derivada de tanh.\n",
    "    \n",
    "    F√≥rmula: tanh'(x) = 1 - tanh¬≤(x)\n",
    "    \"\"\"\n",
    "    # TU C√ìDIGO AQU√ç\n",
    "    t = tanh(x)\n",
    "    return 1 - t**2\n",
    "\n",
    "# Probar implementaci√≥n\n",
    "print(\"Tanh(x):\", tanh(x_test))\n",
    "print(\"Tanh'(x):\", tanh_derivative(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Visualizaci√≥n de Funciones de Activaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear rango de valores\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Calcular valores\n",
    "y_sigmoid = sigmoid(x)\n",
    "y_tanh = tanh(x)\n",
    "y_relu = relu(x)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, y_sigmoid, 'b-', linewidth=2, label='Sigmoid')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('œÉ(x)')\n",
    "plt.title('Funci√≥n Sigmoid')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axhline(y=1, color='r', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, y_tanh, 'g-', linewidth=2, label='Tanh')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('tanh(x)')\n",
    "plt.title('Funci√≥n Tanh')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axhline(y=1, color='r', linestyle='--', alpha=0.3)\n",
    "plt.axhline(y=-1, color='r', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, y_relu, 'r-', linewidth=2, label='ReLU')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "plt.title('Funci√≥n ReLU')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: An√°lisis de Derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular derivadas\n",
    "dy_sigmoid = sigmoid_derivative(x)\n",
    "dy_tanh = tanh_derivative(x)\n",
    "dy_relu = relu_derivative(x)\n",
    "\n",
    "# Visualizar derivadas\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, dy_sigmoid, 'b-', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"œÉ'(x)\")\n",
    "plt.title('Derivada de Sigmoid')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, dy_tanh, 'g-', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"tanh'(x)\")\n",
    "plt.title('Derivada de Tanh')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, dy_relu, 'r-', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"ReLU'(x)\")\n",
    "plt.title('Derivada de ReLU')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observaciones:\")\n",
    "print(\"1. Sigmoid: Derivada m√°xima en x=0 (~0.25), se acerca a 0 en extremos\")\n",
    "print(\"2. Tanh: Derivada m√°xima en x=0 (1.0), se acerca a 0 en extremos\")\n",
    "print(\"3. ReLU: Derivada constante (1) para x>0, 0 para x‚â§0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Problema del Gradiente que Desaparece\n",
    "\n",
    "Simulemos c√≥mo los gradientes se propagan en una red profunda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simular_backprop(activacion_func, derivative_func, num_capas=10):\n",
    "    \"\"\"\n",
    "    Simula backpropagation a trav√©s de m√∫ltiples capas.\n",
    "    \"\"\"\n",
    "    # Valor de entrada aleatorio\n",
    "    x = np.random.randn()\n",
    "    \n",
    "    # Forward pass\n",
    "    activaciones = [x]\n",
    "    for _ in range(num_capas):\n",
    "        x = activacion_func(x)\n",
    "        activaciones.append(x)\n",
    "    \n",
    "    # Backward pass (gradientes)\n",
    "    gradientes = [1.0]  # Gradiente inicial\n",
    "    for i in range(num_capas, 0, -1):\n",
    "        grad = gradientes[-1] * derivative_func(activaciones[i-1])\n",
    "        gradientes.append(grad)\n",
    "    \n",
    "    return gradientes[::-1]\n",
    "\n",
    "# Simular para diferentes funciones\n",
    "num_capas = 20\n",
    "grad_sigmoid = simular_backprop(sigmoid, sigmoid_derivative, num_capas)\n",
    "grad_tanh = simular_backprop(tanh, tanh_derivative, num_capas)\n",
    "grad_relu = simular_backprop(relu, relu_derivative, num_capas)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 5))\n",
    "capas = range(num_capas + 1)\n",
    "\n",
    "plt.semilogy(capas, np.abs(grad_sigmoid), 'b-o', label='Sigmoid', markersize=4)\n",
    "plt.semilogy(capas, np.abs(grad_tanh), 'g-s', label='Tanh', markersize=4)\n",
    "plt.semilogy(capas, np.abs(grad_relu), 'r-^', label='ReLU', markersize=4)\n",
    "\n",
    "plt.xlabel('Profundidad de la Capa', fontsize=12)\n",
    "plt.ylabel('Magnitud del Gradiente (escala log)', fontsize=12)\n",
    "plt.title('Gradiente que Desaparece en Redes Profundas', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Observaci√≥n Clave:\")\n",
    "print(\"- Sigmoid y Tanh: Los gradientes DESAPARECEN r√°pidamente\")\n",
    "print(\"- ReLU: Mantiene gradientes constantes (m√°s estable)\")\n",
    "print(\"\\nEsto explica por qu√© ReLU es preferida en redes profundas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Softmax para Clasificaci√≥n Multiclase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Implementa softmax de manera num√©ricamente estable.\n",
    "    \n",
    "    Args:\n",
    "        x: array de forma (n_samples, n_classes)\n",
    "    \"\"\"\n",
    "    # Restar el m√°ximo para estabilidad num√©rica\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Ejemplo: 3 muestras, 4 clases\n",
    "scores = np.array([\n",
    "    [3.2, 1.5, 0.8, 2.1],  # Muestra 1\n",
    "    [0.5, 2.8, 1.2, 0.9],  # Muestra 2\n",
    "    [1.1, 0.7, 3.5, 1.3]   # Muestra 3\n",
    "])\n",
    "\n",
    "probabilidades = softmax(scores)\n",
    "\n",
    "print(\"Scores originales:\")\n",
    "print(scores)\n",
    "print(\"\\nProbabilidades (despu√©s de Softmax):\")\n",
    "print(np.round(probabilidades, 3))\n",
    "print(\"\\nSuma por fila (debe ser 1.0):\")\n",
    "print(np.sum(probabilidades, axis=1))\n",
    "print(\"\\nClase predicha para cada muestra:\")\n",
    "print(np.argmax(probabilidades, axis=1))\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Scores\n",
    "im1 = axes[0].imshow(scores, cmap='viridis', aspect='auto')\n",
    "axes[0].set_xlabel('Clases')\n",
    "axes[0].set_ylabel('Muestras')\n",
    "axes[0].set_title('Scores Originales')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Probabilidades\n",
    "im2 = axes[1].imshow(probabilidades, cmap='viridis', aspect='auto')\n",
    "axes[1].set_xlabel('Clases')\n",
    "axes[1].set_ylabel('Muestras')\n",
    "axes[1].set_title('Probabilidades (Softmax)')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Ejercicio Pr√°ctico - Red con Diferentes Activaciones\n",
    "\n",
    "Implementa una red neuronal simple y compara el desempe√±o con diferentes funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos sint√©ticos (XOR problem)\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Crear datos XOR\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = (X[:, 0] * X[:, 1] > 0).astype(int)\n",
    "\n",
    "# Visualizar datos\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Clase 0', alpha=0.6)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Clase 1', alpha=0.6)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Problema XOR (No Linealmente Separable)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Este es un problema NO LINEALMENTE SEPARABLE.\")\n",
    "print(\"Necesitamos funciones de activaci√≥n no lineales para resolverlo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronalSimple:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activacion='relu'):\n",
    "        # Inicializaci√≥n de pesos\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.activacion = activacion\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Capa 1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        \n",
    "        # Activaci√≥n\n",
    "        if self.activacion == 'relu':\n",
    "            self.a1 = relu(self.z1)\n",
    "        elif self.activacion == 'sigmoid':\n",
    "            self.a1 = sigmoid(self.z1)\n",
    "        elif self.activacion == 'tanh':\n",
    "            self.a1 = tanh(self.z1)\n",
    "        \n",
    "        # Capa 2 (salida)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)  # Sigmoid para clasificaci√≥n binaria\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def calcular_perdida(self, y_pred, y_true):\n",
    "        # Binary cross-entropy\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "        return loss\n",
    "\n",
    "# Entrenar con diferentes activaciones\n",
    "activaciones = ['relu', 'sigmoid', 'tanh']\n",
    "historias = {}\n",
    "\n",
    "for act in activaciones:\n",
    "    print(f\"\\nEntrenando con {act.upper()}...\")\n",
    "    red = RedNeuronalSimple(2, 10, 1, activacion=act)\n",
    "    \n",
    "    perdidas = []\n",
    "    epochs = 1000\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward\n",
    "        y_pred = red.forward(X)\n",
    "        perdida = red.calcular_perdida(y_pred, y.reshape(-1, 1))\n",
    "        perdidas.append(perdida)\n",
    "        \n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"  Epoch {epoch}: P√©rdida = {perdida:.4f}\")\n",
    "    \n",
    "    historias[act] = perdidas\n",
    "\n",
    "# Comparar curvas de aprendizaje\n",
    "plt.figure(figsize=(10, 6))\n",
    "for act, perdidas in historias.items():\n",
    "    plt.plot(perdidas, label=act.upper(), linewidth=2)\n",
    "\n",
    "plt.xlabel('√âpoca', fontsize=12)\n",
    "plt.ylabel('P√©rdida', fontsize=12)\n",
    "plt.title('Comparaci√≥n de Funciones de Activaci√≥n', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Observa c√≥mo diferentes activaciones afectan la velocidad de convergencia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Desaf√≠o - Implementa Leaky ReLU\n",
    "\n",
    "Implementa Leaky ReLU y comp√°rala con ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Implementa Leaky ReLU.\n",
    "    \n",
    "    TU C√ìDIGO AQU√ç\n",
    "    \"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Implementa la derivada de Leaky ReLU.\n",
    "    \n",
    "    TU C√ìDIGO AQU√ç\n",
    "    \"\"\"\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x <= 0] = alpha\n",
    "    return dx\n",
    "\n",
    "# Comparar ReLU vs Leaky ReLU\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "y_relu = relu(x)\n",
    "y_leaky = leaky_relu(x)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_relu, 'r-', linewidth=2, label='ReLU')\n",
    "plt.plot(x, y_leaky, 'b-', linewidth=2, label='Leaky ReLU', alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('ReLU vs Leaky ReLU')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, relu_derivative(x), 'r-', linewidth=2, label=\"ReLU'\")\n",
    "plt.plot(x, leaky_relu_derivative(x), 'b-', linewidth=2, label=\"Leaky ReLU'\", alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.title('Derivadas')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVentaja de Leaky ReLU: Permite gradientes peque√±os para valores negativos.\")\n",
    "print(\"Esto evita el problema de 'neuronas muertas' de ReLU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen y Conclusiones\n",
    "\n",
    "### Lo que aprendimos:\n",
    "\n",
    "1. **Funciones de Activaci√≥n son Esenciales**: Sin ellas, la red es lineal\n",
    "\n",
    "2. **ReLU es el Est√°ndar**: Simple, eficiente, efectiva para redes profundas\n",
    "\n",
    "3. **Gradientes Importan**: Sigmoid/Tanh saturan, ReLU mantiene gradientes\n",
    "\n",
    "4. **Elecci√≥n por Capa**:\n",
    "   - Capas ocultas ‚Üí ReLU/Leaky ReLU\n",
    "   - Clasificaci√≥n binaria ‚Üí Sigmoid\n",
    "   - Clasificaci√≥n multiclase ‚Üí Softmax\n",
    "\n",
    "5. **Experimentaci√≥n es Clave**: Prueba diferentes opciones\n",
    "\n",
    "### Pr√≥ximos pasos:\n",
    "\n",
    "En el siguiente laboratorio exploraremos **Funciones de P√©rdida** que cuantifican qu√© tan bien (o mal) est√° aprendiendo nuestra red.\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Excelente trabajo! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
