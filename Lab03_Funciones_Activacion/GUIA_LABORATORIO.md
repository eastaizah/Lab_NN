# Gu√≠a de Laboratorio: Funciones de Activaci√≥n

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Funciones de Activaci√≥n  
**C√≥digo:** Lab 03  
**Duraci√≥n:** 2-3 horas  
**Nivel:** B√°sico-Intermedio  

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender el rol de funciones de activaci√≥n
2. Implementar ReLU, Sigmoid, Tanh, Softmax desde cero
3. Calcular derivadas para backpropagation
4. Visualizar y comparar diferentes activaciones
5. Elegir activaci√≥n apropiada para cada problema
6. Reconocer problema del gradiente que desaparece
7. Evitar neuronas muertas en ReLU
8. Implementar activaciones eficientemente
9. Entender no-linealidad en redes profundas

## üìö Prerrequisitos

### Conocimientos

- Completar Lab 01-02
- Python intermedio (clases, funciones, NumPy)
- √Ålgebra lineal b√°sica
- Comprensi√≥n de conceptos de labs anteriores

### Software

- Python 3.8+
- NumPy 1.19+
- Matplotlib 3.0+
- Jupyter Notebook (recomendado)

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` - Marco te√≥rico completo
- `README.md` - Visi√≥n general del laboratorio

## üìñ Introducci√≥n

Las **funciones de activaci√≥n** introducen **no-linealidad** en redes neuronales. Sin ellas, cualquier red profunda es equivalente a regresi√≥n lineal.

### Contexto del Problema

En Lab 02 vimos que sin activaci√≥n, una red profunda colapsa a una sola capa lineal. Para aprender patrones complejos necesitamos no-linealidad.

### Funciones de Activaci√≥n

Transforman la salida de cada neurona agregando capacidad de modelar relaciones complejas:

```
Entrada ‚Üí Suma Ponderada ‚Üí ACTIVACI√ìN ‚Üí Salida
```

### Conceptos Fundamentales

**1. No-linealidad:** Permite aprender XOR, c√≠rculos, patrones complejos

**2. Principales funciones:**
- **ReLU:** max(0, x) - Est√°ndar para capas ocultas
- **Sigmoid:** 1/(1+e^-x) - Clasificaci√≥n binaria
- **Tanh:** tanh(x) - Centrada en cero
- **Softmax:** Clasificaci√≥n multiclase

**3. Derivadas:** Necesarias para backpropagation

### Aplicaciones

Cada activaci√≥n tiene su uso ideal:
- Capas ocultas ‚Üí ReLU
- Salida binaria ‚Üí Sigmoid
- Salida multiclase ‚Üí Softmax

## üî¨ Parte 1: Implementaci√≥n de Activaciones (45 min)

### 1.1 ReLU (Rectified Linear Unit)

```python
def relu(x):
    return np.maximum(0, x)

def relu_derivada(x):
    return (x > 0).astype(float)

# Prueba
x = np.array([-2, -1, 0, 1, 2])
print(f"ReLU: {relu(x)}")  # [0, 0, 0, 1, 2]
print(f"Derivada: {relu_derivada(x)}")  # [0, 0, 0, 1, 1]
```

### 1.2 Sigmoid

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivada(x):
    s = sigmoid(x)
    return s * (1 - s)

# Prueba
x = np.array([-2, 0, 2])
print(f"Sigmoid: {sigmoid(x)}")  # [0.12, 0.5, 0.88]
```

### 1.3 Tanh

```python
def tanh(x):
    return np.tanh(x)

def tanh_derivada(x):
    return 1 - np.tanh(x)**2

# Prueba
x = np.array([-1, 0, 1])
print(f"Tanh: {tanh(x)}")  # [-0.76, 0, 0.76]
```

### 1.4 Softmax

```python
def softmax(x):
    # Estabilizaci√≥n num√©rica
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# Prueba
x = np.array([[1, 2, 3]])
probs = softmax(x)
print(f"Softmax: {probs}")  # [[0.09, 0.24, 0.67]]
print(f"Suma: {probs.sum()}")  # 1.0
```

### Actividades

1. Implementar Leaky ReLU
2. Graficar todas las funciones
3. Verificar derivadas num√©ricamente

## üî¨ Parte 2: Integraci√≥n con Redes (45 min)

### 2.1 Clase Activaci√≥n

```python
class Activacion:
    def __init__(self, funcion, derivada):
        self.funcion = funcion
        self.derivada = derivada
    
    def forward(self, entradas):
        self.entradas = entradas
        self.salida = self.funcion(entradas)
        return self.salida
    
    def backward(self, grad_salida):
        return grad_salida * self.derivada(self.entradas)
```

### 2.2 Red con Activaciones

```python
class RedConActivaciones:
    def __init__(self, arquitectura, activaciones):
        self.capas = []
        for i in range(len(arquitectura)-1):
            self.capas.append(CapaDensa(arquitectura[i], arquitectura[i+1]))
            if i < len(activaciones):
                self.capas.append(Activacion(activaciones[i], None))
    
    def forward(self, X):
        activacion = X
        for capa in self.capas:
            activacion = capa.forward(activacion)
        return activacion
```

### 2.3 Ejemplo de Uso

```python
# Red para clasificaci√≥n binaria
red = RedConActivaciones(
    arquitectura=[10, 20, 15, 1],
    activaciones=[relu, relu, sigmoid]
)

X = np.random.randn(32, 10)
output = red.forward(X)
print(f"Output shape: {output.shape}")  # (32, 1)
print(f"Output range: [{output.min():.3f}, {output.max():.3f}]")  # [0, 1]
```

### Actividades

1. Crear red para MNIST con ReLU en ocultas y Softmax en salida
2. Comparar salidas con/sin activaci√≥n
3. Medir impacto en tiempo de ejecuci√≥n

## üî¨ Parte 3: Visualizaci√≥n y An√°lisis (40 min)

### 3.1 Graficar Funciones

```python
import matplotlib.pyplot as plt

x = np.linspace(-5, 5, 100)

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.plot(x, relu(x), label='ReLU')
plt.plot(x, sigmoid(x), label='Sigmoid')
plt.plot(x, tanh(x), label='Tanh')
plt.legend()
plt.title('Funciones de Activaci√≥n')
plt.grid(True)

plt.subplot(1, 3, 2)
plt.plot(x, relu_derivada(x), label='ReLU'')
plt.plot(x, sigmoid_derivada(x), label='Sigmoid'')
plt.plot(x, tanh_derivada(x), label='Tanh'')
plt.legend()
plt.title('Derivadas')
plt.grid(True)

plt.subplot(1, 3, 3)
# Comparar saturaci√≥n
plt.plot(x, sigmoid_derivada(x), label='Sigmoid' (satura)')
plt.plot(x, relu_derivada(x), label='ReLU' (no satura)')
plt.legend()
plt.title('Problema de Saturaci√≥n')
plt.grid(True)

plt.tight_layout()
plt.savefig('activaciones.png')
```

### 3.2 Problema del Gradiente que Desaparece

```python
def demostrar_gradiente_desaparece():
    x = np.array([10.0])  # Valor grande
    
    # Sigmoid satura
    for i in range(10):
        grad = sigmoid_derivada(x)
        print(f"Capa {i}: gradiente = {grad[0]:.10f}")
        x = sigmoid(x)  # Propagar
    
    # ReLU no satura
    x = np.array([10.0])
    for i in range(10):
        grad = relu_derivada(x)
        print(f"Capa {i}: gradiente = {grad[0]}")
        x = relu(x)
```

### 3.3 Neuronas Muertas en ReLU

```python
def detectar_neuronas_muertas(red, X):
    _ = red.forward(X)
    for i, capa in enumerate(red.capas):
        if hasattr(capa, 'salida'):
            muertas = (capa.salida <= 0).all(axis=0).sum()
            total = capa.salida.shape[1]
            print(f"Capa {i}: {muertas}/{total} neuronas muertas")
```

### Actividades

1. Visualizar todas las funciones y derivadas
2. Demostrar saturaci√≥n de gradiente
3. Detectar neuronas muertas en red con ReLU

## üî¨ Parte 4: Casos de Uso (40 min)

### 4.1 Clasificaci√≥n Binaria

```python
# Spam detection
red_spam = RedConActivaciones(
    [100, 64, 32, 1],
    [relu, relu, sigmoid]
)
```

### 4.2 Clasificaci√≥n Multiclase

```python
# MNIST
red_mnist = RedConActivaciones(
    [784, 256, 128, 10],
    [relu, relu, softmax]
)
```

### 4.3 Regresi√≥n

```python
# Predicci√≥n de precios (sin activaci√≥n en salida)
red_regresion = RedConActivaciones(
    [20, 64, 32, 1],
    [relu, relu, lambda x: x]  # Identidad en salida
)
```

### 4.4 Comparaci√≥n Experimental

```python
def comparar_activaciones():
    X = np.random.randn(100, 10)
    
    configs = [
        ('Solo Sigmoid', [sigmoid] * 3),
        ('Solo ReLU', [relu] * 3),
        ('Mixto', [relu, relu, sigmoid])
    ]
    
    for nombre, acts in configs:
        red = RedConActivaciones([10, 20, 15, 5], acts)
        salida = red.forward(X)
        print(f"{nombre}: mean={salida.mean():.3f}, std={salida.std():.3f}")
```

### Actividades

1. Implementar red para cada tipo de problema
2. Comparar diferentes combinaciones de activaciones
3. Medir impacto en distribuci√≥n de salidas

## üìä An√°lisis Final de Rendimiento

### Comparaci√≥n de Implementaciones

En esta secci√≥n comparar√°s diferentes enfoques de implementaci√≥n para entender las ventajas de cada uno.

**Criterios de comparaci√≥n:**
- Velocidad de ejecuci√≥n
- Uso de memoria
- Claridad del c√≥digo
- Mantenibilidad

### M√©tricas de Desempe√±o

Mide y compara:
- Tiempo de forward pass
- Escalabilidad con tama√±o de datos
- Eficiencia computacional

## ÔøΩÔøΩ EJERCICIOS PROPUESTOS

### Ejercicio 1: Implementar ELU (B√°sico)

```python
ELU(x) = x si x > 0
       = Œ±(e^x - 1) si x <= 0
```

Implementa forward y backward.

### Ejercicio 2: An√°lisis de Saturaci√≥n (Intermedio)

Grafica derivadas de Sigmoid y Tanh para x en [-10, 10].
¬øEn qu√© rangos se saturan?

### Ejercicio 3: Red con Diferentes Activaciones (Intermedio)

Entrena red simple con:
- Solo Sigmoid
- Solo ReLU  
- Mezcla

Compara velocidad de convergencia.

### Ejercicio 4: Softmax con Temperatura (Avanzado)

```python
Softmax(x/T)  donde T = temperatura
```

Observa c√≥mo T afecta distribuci√≥n de probabilidades.

### Ejercicio 5: Detecci√≥n de Problemas (Avanzado)

Implementa:
- Detector de gradientes que desaparecen
- Detector de neuronas muertas
- Recomendador de activaci√≥n

## üìù Entregables

### 1. C√≥digo Implementado (60%)

**Requisitos m√≠nimos:**
- Implementaciones completas y funcionales
- C√≥digo limpio y bien documentado
- Pruebas y validaci√≥n
- Manejo apropiado de errores

### 2. Notebook de Experimentaci√≥n (25%)

**Debe incluir:**
- Experimentos con diferentes configuraciones
- Visualizaciones claras
- An√°lisis de resultados
- Comparaciones y conclusiones

### 3. Reporte T√©cnico (15%)

**Secciones:**
1. Introducci√≥n y objetivos
2. Metodolog√≠a
3. Resultados experimentales
4. An√°lisis y discusi√≥n
5. Conclusiones

**Extensi√≥n:** 3-5 p√°ginas

### Formato de Entrega

```
Lab03_Entrega/
‚îú‚îÄ‚îÄ codigo/
‚îÇ   ‚îî‚îÄ‚îÄ [archivos .py]
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ experimentos.ipynb
‚îú‚îÄ‚îÄ reporte/
‚îÇ   ‚îî‚îÄ‚îÄ reporte_lab03.pdf
‚îî‚îÄ‚îÄ README.md
```

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Concebir (25%)

**Comprender el problema:**
- Identificar requisitos y restricciones
- Analizar alternativas de soluci√≥n
- Reconocer implicaciones de decisiones de dise√±o

### Dise√±ar (25%)

**Planificar soluciones:**
- Dise√±ar arquitecturas apropiadas
- Estructurar c√≥digo eficientemente
- Considerar escalabilidad y mantenibilidad

### Implementar (30%)

**Construcci√≥n:**
- C√≥digo funcional y correcto
- Implementaci√≥n eficiente
- Documentaci√≥n adecuada
- Pruebas comprehensivas

### Operar (20%)

**Validaci√≥n y an√°lisis:**
- Experimentaci√≥n sistem√°tica
- An√°lisis cr√≠tico de resultados
- Visualizaciones informativas
- Conclusiones fundamentadas

### R√∫brica Detallada

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|---------------|---------------------|-------------------|
| **Implementaci√≥n** | Impecable, eficiente, documentado | Funcional con docs | B√°sico funcional | Con errores |
| **Experimentaci√≥n** | An√°lisis profundo | Completo | B√°sico | Incompleto |
| **Documentaci√≥n** | Excelente | Buena | B√°sica | Pobre |
| **Comprensi√≥n** | Dominio total | Buen entendimiento | Comprensi√≥n b√°sica | Comprensi√≥n limitada |

## üìö Referencias Adicionales

### Libros

1. **"Deep Learning" - Goodfellow, Bengio, Courville**
   - Cap√≠tulos relevantes para este lab
   - www.deeplearningbook.org

2. **"Neural Networks and Deep Learning" - Michael Nielsen**
   - neuralnetworksanddeeplearning.com

### Recursos Online

1. **CS231n: Stanford**
   - http://cs231n.stanford.edu/

2. **3Blue1Brown: Neural Networks**
   - Videos educativos excelentes

3. **TensorFlow Playground**
   - https://playground.tensorflow.org/

### Documentaci√≥n

- NumPy: https://numpy.org/doc/
- Matplotlib: https://matplotlib.org/
- Python: https://docs.python.org/3/

## üéì Notas Finales

### Conceptos Clave para Recordar

1. **No-linealidad es esencial:** Sin activaci√≥n, red = regresi√≥n lineal
2. **ReLU es el est√°ndar:** Simple, eficiente, efectiva
3. **Softmax para multiclase:** Convierte scores a probabilidades
4. **Sigmoid para binaria:** Salida entre 0 y 1
5. **Gradientes importan:** Evitar saturaci√≥n
6. **Neuronas muertas:** Problema de ReLU con inicializaci√≥n mala
7. **Derivadas simples:** ReLU' = 1 si x>0, 0 si no
8. **Combinaciones ideales:** ReLU+ReLU+Softmax para clasificaci√≥n

### Preparaci√≥n para el Siguiente Lab

**Lab 04: Funciones de P√©rdida**

Aprender√°s:
- MSE, MAE para regresi√≥n
- Cross-Entropy para clasificaci√≥n
- Gradient descent
- Learning rate
- Overfitting

Prep√°rate para entender optimizaci√≥n.

### Consejos de Estudio

1. **Implementa desde cero** - No uses frameworks todav√≠a
2. **Visualiza** - Dibuja y grafica para entender
3. **Experimenta** - Prueba diferentes configuraciones
4. **Debug sistem√°ticamente** - Verifica paso a paso
5. **Documenta** - Anota hallazgos y experimentos

### Soluci√≥n de Problemas Comunes

**Errores de dimensiones:**
- Verifica shape de todas las matrices
- Usa print(variable.shape) liberalmente

**Resultados inesperados:**
- Verifica inicializaci√≥n
- Asegura reproducibilidad con seed
- Revisa cada paso del c√°lculo

**C√≥digo lento:**
- Usa vectorizaci√≥n de NumPy
- Evita loops innecesarios
- Procesa en batches

### Certificaci√≥n de Completitud

Has completado exitosamente Lab 03 cuando puedas:

- [ ] Comprender el rol de funciones de activaci√≥n
- [ ] Implementar ReLU, Sigmoid, Tanh, Softmax desde cero
- [ ] Calcular derivadas para backpropagation
- [ ] Visualizar y comparar diferentes activaciones
- [ ] Elegir activaci√≥n apropiada para cada problema
- [ ] Reconocer problema del gradiente que desaparece
- [ ] Evitar neuronas muertas en ReLU
- [ ] Implementar activaciones eficientemente
- [ ] Entender no-linealidad en redes profundas

**¬°Felicitaciones!** Contin√∫a con el siguiente laboratorio.

---

**¬øPreguntas?** Revisa teor√≠a, experimenta, y consulta referencias.

**¬°√âxito en tu aprendizaje! üöÄ**
