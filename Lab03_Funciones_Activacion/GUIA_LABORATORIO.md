# Gu√≠a de Laboratorio: Funciones de Activaci√≥n

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Funciones de Activaci√≥n en Redes Neuronales  
**C√≥digo:** Lab 03  
**Duraci√≥n:** 2-3 horas  
**Nivel:** B√°sico-Intermedio  

---

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender el rol fundamental de las funciones de activaci√≥n en redes neuronales
2. Implementar ReLU, Sigmoid, Tanh, Leaky ReLU y Softmax desde cero usando NumPy
3. Calcular las derivadas de cada funci√≥n de activaci√≥n (necesarias para backpropagation)
4. Visualizar y comparar el comportamiento de diferentes activaciones
5. Elegir la funci√≥n de activaci√≥n apropiada para cada tipo de problema y capa
6. Reconocer y demostrar el problema del gradiente que desaparece (vanishing gradient)
7. Identificar y diagnosticar el problema de neuronas muertas en ReLU
8. Integrar funciones de activaci√≥n en la arquitectura de red del Lab 02
9. Entender por qu√© la no-linealidad es esencial para el aprendizaje profundo

---

## üìö Prerrequisitos

### Conocimientos

- **Labs 01 y 02 completados**: neuronas, forward propagation, clases `CapaDensa` y `RedNeuronal`
- Python intermedio: clases, lambdas, funciones de orden superior
- √Ålgebra lineal b√°sica: vectores, matrices, broadcasting
- C√°lculo diferencial b√°sico: derivadas de funciones elementales (chain rule)

### Software

- Python 3.8+
- NumPy 1.19+
- Matplotlib 3.0+
- Jupyter Notebook (recomendado)

### Material de Lectura

Antes de comenzar este laboratorio:
- `teoria.md` ‚Äî Marco te√≥rico completo sobre funciones de activaci√≥n
- `README.md` ‚Äî Visi√≥n general del laboratorio
- Repasa la Parte 4 del Lab 02 (limitaciones sin activaci√≥n no lineal)

---

## üìñ Introducci√≥n

En el Lab 02 demostramos matem√°ticamente que una red neuronal sin funciones de activaci√≥n no lineal es equivalente a una sola transformaci√≥n lineal, sin importar cu√°ntas capas tenga. Esta es la limitaci√≥n fundamental que las **funciones de activaci√≥n** vienen a resolver.

### Contexto del Problema

¬øPor qu√© necesitamos no-linealidad? Considera estos problemas del mundo real:

**Problema del XOR** (no linealmente separable):
```
 (0,0)‚Üí0    (1,1)‚Üí0
 (0,1)‚Üí1    (1,0)‚Üí1
```
No existe ninguna l√≠nea recta que separe estas clases. Una red lineal falla; una con activaciones no lineales lo resuelve.

**Reconocimiento de d√≠gitos escritos a mano:**
La relaci√≥n entre 784 p√≠xeles y el d√≠gito 0-9 es altamente no lineal. Ninguna funci√≥n lineal puede capturar estos patrones complejos.

### Enfoque con Funciones de Activaci√≥n

Las funciones de activaci√≥n se aplican elemento a elemento despu√©s de la transformaci√≥n lineal de cada capa:

```
                 Capa 1                          Capa 2
X  ‚Üí  [z = X¬∑W‚ÇÅ + b‚ÇÅ]  ‚Üí  [a = f(z)]  ‚Üí  [z = a¬∑W‚ÇÇ + b‚ÇÇ]  ‚Üí  [a = g(z)]  ‚Üí  Y
         transformaci√≥n         ACTIVACI√ìN      transformaci√≥n         ACTIVACI√ìN
         lineal                 NO LINEAL       lineal                 NO LINEAL
```

Las funciones de activaci√≥n m√°s importantes:

| Funci√≥n | Ecuaci√≥n | Rango | Uso t√≠pico |
|---------|----------|-------|------------|
| **ReLU** | $\max(0, x)$ | $[0, +\infty)$ | Capas ocultas (est√°ndar) |
| **Sigmoid** | $\frac{1}{1+e^{-x}}$ | $(0, 1)$ | Salida binaria |
| **Tanh** | $\tanh(x)$ | $(-1, 1)$ | RNNs, capas ocultas |
| **Softmax** | $\frac{e^{x_i}}{\sum e^{x_j}}$ | $(0,1)$, suma=1 | Salida multiclase |
| **Leaky ReLU** | $\max(\alpha x, x)$ | $(-\infty, +\infty)$ | Alternativa a ReLU |

### Conceptos Fundamentales

**1. No-linealidad como capacidad representacional:**

Con activaciones no lineales, la red puede aproximar cualquier funci√≥n continua (Teorema de Aproximaci√≥n Universal). Sin ellas, solo puede aprender funciones lineales.

**2. Derivadas ‚Äî por qu√© importan:**

Durante el entrenamiento, el algoritmo de backpropagation necesita calcular la derivada de la funci√≥n de p√©rdida respecto a cada par√°metro. Esto requiere las derivadas de las activaciones:

$$\frac{\partial \mathcal{L}}{\partial W^{(l)}} = \frac{\partial \mathcal{L}}{\partial a^{(l)}} \cdot \frac{\partial a^{(l)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial W^{(l)}}$$

donde $\frac{\partial a^{(l)}}{\partial z^{(l)}} = f'(z^{(l)})$ es la derivada de la activaci√≥n.

**3. El problema del gradiente que desaparece:**

Sigmoid y Tanh "saturan" para valores extremos de $x$: su derivada se acerca a 0. Al multiplicar muchos gradientes peque√±os durante backpropagation (una por cada capa), el gradiente se hace exponencialmente peque√±o. Las capas iniciales no aprenden.

### Aplicaciones Pr√°cticas

La elecci√≥n correcta de activaci√≥n determina el √©xito del entrenamiento:
- Clasificar correos como spam/no-spam ‚Üí Sigmoid en la salida
- Clasificar im√°genes en 1000 categor√≠as ‚Üí Softmax en la salida
- Red profunda para reconocimiento de voz ‚Üí ReLU en capas ocultas
- Red recurrente LSTM para texto ‚Üí Tanh en las puertas internas

### Motivaci√≥n Hist√≥rica

Las primeras redes (a√±os 80-90) usaban Sigmoid y Tanh. El problema del gradiente desvaneciente fue identificado por Hochreiter (1991). D√©cadas despu√©s, Nair y Hinton (2010) propusieron ReLU como alternativa eficiente, lo que desbloque√≥ el entrenamiento de redes muy profundas y desencaden√≥ el "renacimiento" del deep learning moderno.

---

## üî¨ Parte 1: Implementaci√≥n de Funciones de Activaci√≥n (40 min)

### 1.1 Introducci√≥n Conceptual: Forward y Backward

**¬øQu√© hacemos?** Implementar cada funci√≥n de activaci√≥n junto con su derivada.

**¬øPor qu√© lo hacemos?** Para el forward pass necesitamos la funci√≥n $f(x)$. Para el backward pass (backpropagation) necesitamos su derivada $f'(x)$. En este lab implementamos ambas para estar preparados para Lab 05 (backpropagation).

**¬øC√≥mo lo hacemos?** Cada funci√≥n opera elemento a elemento sobre arrays NumPy, aprovechando broadcasting.

**¬øQu√© resultados esperar?** Funciones que toman un array de cualquier shape y devuelven otro array del mismo shape con los valores transformados.

### 1.2 ReLU (Rectified Linear Unit)

ReLU es la funci√≥n de activaci√≥n m√°s usada en la actualidad. Su simplicidad la hace computacionalmente eficiente y su derivada no-cero en la regi√≥n positiva evita el problema del gradiente desvaneciente.

**Intuici√≥n:** ReLU "apaga" las neuronas que reciben se√±al negativa y deja pasar sin cambios las que reciben se√±al positiva. Esto crea sparsity: en promedio, la mitad de las neuronas est√°n activas en cada forward pass.

$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{si } x > 0 \\ 0 & \text{si } x \leq 0 \end{cases}$$

$$\text{ReLU}'(x) = \begin{cases} 1 & \text{si } x > 0 \\ 0 & \text{si } x \leq 0 \end{cases}$$

```python
import numpy as np
import matplotlib.pyplot as plt

def relu(x):
    """
    Rectified Linear Unit (ReLU).
    
    Aplica max(0, x) elemento a elemento.
    Ventajas: simple, eficiente, evita gradiente desvaneciente.
    Desventajas: neuronas muertas para x <= 0.
    
    Args:
        x: Array NumPy de cualquier shape
    Returns:
        Array del mismo shape con max(0, x)
    """
    return np.maximum(0, x)


def relu_derivada(x):
    """
    Derivada de ReLU.
    
    1 donde x > 0, 0 donde x <= 0.
    Nota: t√©cnicamente no diferenciable en x=0, pero en pr√°ctica
    se usa 0 o 1 en ese punto sin consecuencias.
    
    Args:
        x: Array NumPy (valores ANTES de aplicar ReLU)
    Returns:
        Array del mismo shape con la derivada
    """
    return (x > 0).astype(float)


# Prueba y verificaci√≥n
print("=" * 50)
print("PRUEBA DE ReLU")
print("=" * 50)

x_prueba = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])

print(f"\nEntrada:         {x_prueba}")
print(f"ReLU(x):         {relu(x_prueba)}")
print(f"ReLU'(x):        {relu_derivada(x_prueba)}")

# Verificaci√≥n matem√°tica
print(f"\nVerificaciones:")
print(f"  ReLU(-5) = 0:  {relu(np.array([-5.0]))[0] == 0}")
print(f"  ReLU(5)  = 5:  {relu(np.array([5.0]))[0] == 5.0}")
print(f"  ReLU'(-5) = 0: {relu_derivada(np.array([-5.0]))[0] == 0}")
print(f"  ReLU'(5) = 1:  {relu_derivada(np.array([5.0]))[0] == 1.0}")

# ReLU con arrays multidimensionales
X = np.random.randn(4, 3)
print(f"\nArray (4,3):\n{X}")
print(f"\nReLU(array):\n{relu(X)}")
print(f"  Porcentaje activado: {(relu(X) > 0).mean():.1%}")
```

**Actividad 1.1**: Verifica que `relu(-100)` = 0 y `relu(100)` = 100. ¬øPor qu√© ReLU es tan eficiente computacionalmente comparada con Sigmoid?

**Actividad 1.2**: Calcula el porcentaje de neuronas activas (salida > 0) cuando la entrada sigue una distribuci√≥n normal. ¬øQu√© porcentaje esperas te√≥ricamente?

### 1.3 Sigmoid (Sigmoide)

Sigmoid fue hist√≥ricamente la primera funci√≥n de activaci√≥n ampliamente usada. Su salida en el rango (0, 1) la hace ideal para modelar probabilidades.

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

$$\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$$

**Intuici√≥n:** Sigmoid "comprime" cualquier valor real al rango (0, 1). Para $x$ muy negativo ‚Üí 0, para $x$ muy positivo ‚Üí 1, para $x=0$ ‚Üí 0.5. El problema: para valores extremos, la derivada $\sigma'(x) \approx 0$ (saturaci√≥n).

```python
def sigmoid(x):
    """
    Funci√≥n Sigmoide.
    
    Comprime valores al rango (0, 1).
    Ideal para probabilidades en clasificaci√≥n binaria.
    
    NOTA: Para valores muy negativos puede generar overflow.
    Se usa la versi√≥n num√©ricamente estable.
    
    Args:
        x: Array NumPy de cualquier shape
    Returns:
        Array del mismo shape con valores en (0, 1)
    """
    # Versi√≥n num√©ricamente estable para evitar overflow
    return np.where(x >= 0,
                    1 / (1 + np.exp(-x)),
                    np.exp(x) / (1 + np.exp(x)))


def sigmoid_derivada(x):
    """
    Derivada de Sigmoid.
    
    œÉ'(x) = œÉ(x) * (1 - œÉ(x))
    
    ‚ö†Ô∏è SATURACI√ìN: Para |x| > 5, la derivada es ‚âà 0
    Esto causa el problema del gradiente desvaneciente.
    
    Args:
        x: Array NumPy (valores ANTES de aplicar sigmoid)
    Returns:
        Array del mismo shape con la derivada
    """
    s = sigmoid(x)
    return s * (1 - s)


# Prueba y verificaci√≥n
print("=" * 50)
print("PRUEBA DE SIGMOID")
print("=" * 50)

x_prueba = np.array([-10.0, -2.0, 0.0, 2.0, 10.0])

print(f"\nEntrada:           {x_prueba}")
print(f"Sigmoid(x):        {sigmoid(x_prueba).round(4)}")
print(f"Sigmoid'(x):       {sigmoid_derivada(x_prueba).round(6)}")

print(f"\nPropiedades:")
print(f"  œÉ(0) = 0.5:    {abs(sigmoid(np.array([0.0]))[0] - 0.5) < 1e-10}")
print(f"  Rango en (-‚àû):  {sigmoid(np.array([-100.0]))[0]:.10f}")
print(f"  Rango en (+‚àû):  {sigmoid(np.array([100.0]))[0]:.10f}")

print(f"\n‚ö†Ô∏è Saturaci√≥n del gradiente:")
for val in [-10, -5, 0, 5, 10]:
    grad = sigmoid_derivada(np.array([float(val)]))[0]
    print(f"  œÉ'({val:3d}) = {grad:.8f}")
```

**Actividad 1.3**: Verifica num√©ricamente que la derivada `sigmoid_derivada(x)` es correcta compar√°ndola con una aproximaci√≥n num√©rica:
```
f'(x) ‚âà (f(x+h) - f(x-h)) / (2h)  con h = 1e-5
```

**Actividad 1.4**: Grafica `sigmoid(x)` y `sigmoid_derivada(x)` para $x \in [-6, 6]$. ¬øEn qu√© rango la derivada es significativa?

### 1.4 Tanh (Tangente Hiperb√≥lica)

Tanh es similar a Sigmoid pero con salidas en $(-1, 1)$ y centrada en cero. Esto facilita el aprendizaje porque las activaciones positivas y negativas se balancean.

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1$$

$$\tanh'(x) = 1 - \tanh^2(x)$$

```python
def tanh(x):
    """
    Tangente Hiperb√≥lica (Tanh).
    
    Similar a Sigmoid pero con rango (-1, 1) y centrada en 0.
    Convergencia m√°s r√°pida que Sigmoid en la pr√°ctica.
    A√∫n sufre de saturaci√≥n para |x| grande.
    
    Args:
        x: Array NumPy de cualquier shape
    Returns:
        Array del mismo shape con valores en (-1, 1)
    """
    return np.tanh(x)  # NumPy tiene tanh optimizado


def tanh_derivada(x):
    """
    Derivada de Tanh.
    
    tanh'(x) = 1 - tanh¬≤(x)
    
    Rango de la derivada: (0, 1]
    M√°ximo en x=0: tanh'(0) = 1
    
    Args:
        x: Array NumPy (valores ANTES de aplicar tanh)
    Returns:
        Array del mismo shape con la derivada
    """
    return 1 - np.tanh(x) ** 2


# Prueba
print("=" * 50)
print("PRUEBA DE TANH")
print("=" * 50)

x_prueba = np.array([-5.0, -1.0, 0.0, 1.0, 5.0])
print(f"\nEntrada:     {x_prueba}")
print(f"Tanh(x):     {tanh(x_prueba).round(4)}")
print(f"Tanh'(x):    {tanh_derivada(x_prueba).round(6)}")

print(f"\nPropiedades:")
print(f"  Antisim√©trica: tanh(-x) = -tanh(x)")
t1 = tanh(np.array([2.0]))[0]
t2 = tanh(np.array([-2.0]))[0]
print(f"  tanh(2) = {t1:.4f}, tanh(-2) = {t2:.4f}, suma = {t1+t2:.10f}")

print(f"\nComparaci√≥n Sigmoid vs Tanh (derivadas):")
print(f"{'x':>6} | {'œÉ(x)':<10} | {'œÉ\'(x)':<12} | {'tanh(x)':<10} | {'tanh\'(x)'}")
print("-" * 60)
for v in [-3, -1, 0, 1, 3]:
    sv = sigmoid(np.array([float(v)]))[0]
    sdv = sigmoid_derivada(np.array([float(v)]))[0]
    tv = tanh(np.array([float(v)]))[0]
    tdv = tanh_derivada(np.array([float(v)]))[0]
    print(f"{v:>6} | {sv:<10.4f} | {sdv:<12.6f} | {tv:<10.4f} | {tdv:.6f}")
```

**Actividad 1.5**: ¬øPor qu√© Tanh converge m√°s r√°pido que Sigmoid en la pr√°ctica? Pista: relaciona con las salidas centradas en cero.

### 1.5 Softmax

Softmax es especial: opera sobre un vector completo (no elemento a elemento) y produce una distribuci√≥n de probabilidad v√°lida (todos los valores en $(0,1)$ y suma = 1).

$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

**Truco de estabilidad num√©rica:** Restar el m√°ximo antes de exponenciar evita overflow sin cambiar el resultado:

$$\text{softmax}(z_i) = \frac{e^{z_i - \max(z)}}{\sum_j e^{z_j - \max(z)}}$$

```python
def softmax(x):
    """
    Funci√≥n Softmax.
    
    Convierte un vector de scores en una distribuci√≥n de probabilidad.
    La suma de todas las salidas es exactamente 1.
    
    Usa estabilizaci√≥n num√©rica restando el m√°ximo para evitar overflow.
    
    Args:
        x: Array (batch_size, n_clases) o (n_clases,)
    Returns:
        Array del mismo shape con probabilidades que suman 1
    """
    # Estabilizaci√≥n num√©rica: restar el m√°ximo no cambia el resultado
    # pero previene overflow con valores grandes
    x_stable = x - np.max(x, axis=-1, keepdims=True)
    exp_x = np.exp(x_stable)
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)


# Prueba
print("=" * 55)
print("PRUEBA DE SOFTMAX")
print("=" * 55)

# Caso 1: Un vector simple
z = np.array([[1.0, 2.0, 3.0, 4.0]])  # Un batch, 4 clases
probs = softmax(z)
print(f"\nScores:             {z[0]}")
print(f"Probabilidades:     {probs[0].round(4)}")
print(f"Suma:               {probs.sum():.10f}")
print(f"Clase predicha:     {np.argmax(probs)}")

# Caso 2: Batch de 3 muestras con 5 clases
batch_z = np.random.randn(3, 5)
batch_probs = softmax(batch_z)
print(f"\nBatch scores:\n{batch_z.round(3)}")
print(f"\nBatch probabilidades:\n{batch_probs.round(4)}")
print(f"Sumas por muestra: {batch_probs.sum(axis=1)}")

# Caso 3: Efecto de temperatura
print("\nüå°Ô∏è  Efecto de temperatura en Softmax:")
z_temp = np.array([[1.0, 2.0, 5.0]])
for T in [0.1, 0.5, 1.0, 2.0, 10.0]:
    p = softmax(z_temp / T)
    print(f"  T={T:4.1f}: {p[0].round(3)}")
print("  ‚Üí T peque√±o: m√°s 'confiado'; T grande: m√°s uniforme")
```

**Actividad 1.6**: Demuestra que restar el m√°ximo antes de exponenciar no cambia el resultado del softmax. Verifica con un ejemplo num√©rico concreto con y sin la estabilizaci√≥n.

**Actividad 1.7**: Implementa `softmax` sin estabilizaci√≥n num√©rica y demuestra cu√°ndo falla (overflow) con valores grandes como `np.array([[1000., 2000., 3000.]])`.

### 1.6 Leaky ReLU

Leaky ReLU soluciona el problema de las "neuronas muertas" de ReLU: en lugar de hacer 0 en la regi√≥n negativa, permite un gradiente peque√±o $\alpha$.

$$\text{LeakyReLU}(x) = \max(\alpha x, x) = \begin{cases} x & \text{si } x > 0 \\ \alpha x & \text{si } x \leq 0 \end{cases}$$

```python
def leaky_relu(x, alpha=0.01):
    """
    Leaky ReLU.
    
    Soluciona el problema de neuronas muertas de ReLU
    permitiendo un peque√±o gradiente en la regi√≥n negativa.
    
    Args:
        x: Array NumPy de cualquier shape
        alpha: Pendiente en la regi√≥n negativa (default=0.01)
    Returns:
        Array del mismo shape
    """
    return np.where(x > 0, x, alpha * x)


def leaky_relu_derivada(x, alpha=0.01):
    """Derivada de Leaky ReLU."""
    return np.where(x > 0, 1.0, alpha)


# Prueba
x_prueba = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])
print("Leaky ReLU (alpha=0.01):")
print(f"  Entrada: {x_prueba}")
print(f"  Salida:  {leaky_relu(x_prueba).round(4)}")
print(f"  Derivada:{leaky_relu_derivada(x_prueba)}")
```

**Actividad 1.8**: Implementa ELU (Exponential Linear Unit):
$$\text{ELU}(x) = \begin{cases} x & \text{si } x > 0 \\ \alpha(e^x - 1) & \text{si } x \leq 0 \end{cases}$$

### Actividades de Verificaci√≥n

**Actividad 1.9**: Crea una funci√≥n `verificar_derivada(func, func_deriv, x, h=1e-5)` que compare la derivada anal√≠tica con la aproximaci√≥n num√©rica usando diferencias finitas. Verifica todas las funciones implementadas.

**Actividad 1.10**: Implementa todas las funciones de activaci√≥n y sus derivadas en un diccionario para acceso f√°cil:
```python
ACTIVACIONES = {
    'relu': (relu, relu_derivada),
    'sigmoid': (sigmoid, sigmoid_derivada),
    'tanh': (tanh, tanh_derivada),
    'leaky_relu': (leaky_relu, leaky_relu_derivada),
}
```

### Preguntas de Reflexi√≥n

**Pregunta 1.1 (Concebir):** ¬øPor qu√© no usamos funciones de activaci√≥n polinomiales (e.g., $f(x) = x^2$) a pesar de ser no lineales?

**Pregunta 1.2 (Dise√±ar):** ¬øQu√© funci√≥n de activaci√≥n usar√≠as para la capa oculta de una red que debe predecir una probabilidad de lluvia? ¬øY para la capa de salida?

**Pregunta 1.3 (Implementar):** ¬øPor qu√© el "truco de estabilidad num√©rica" en Softmax (restar el m√°ximo) produce exactamente el mismo resultado matem√°tico?

**Pregunta 1.4 (Operar):** Si observas que en producci√≥n el modelo predice siempre la misma clase (probabilidad muy alta para una clase, muy baja para otras), ¬øqu√© podr√≠a estar pasando con la temperatura del Softmax?

---

## üî¨ Parte 2: Integraci√≥n con la Arquitectura de Red (40 min)

### 2.1 Introducci√≥n Conceptual: Capas de Activaci√≥n

**¬øQu√© hacemos?** Integrar las funciones de activaci√≥n en la arquitectura modular del Lab 02.

**¬øPor qu√© lo hacemos?** Las funciones de activaci√≥n son operaciones separadas en el grafo computacional. Separarlas en clases propias facilita:
- Agregar cualquier activaci√≥n a cualquier capa sin modificar `CapaDensa`
- Implementar backpropagation de forma modular
- Experimentar con diferentes combinaciones de capas y activaciones

**Analog√≠a:** En electr√≥nica, los componentes (resistores, capacitores) son separados y se conectan seg√∫n el dise√±o del circuito. Del mismo modo, `CapaDensa` y `CapaActivacion` son componentes que se combinan libremente.

**¬øQu√© resultados esperar?** Una clase `CapaActivacion` que aplica cualquier funci√≥n de activaci√≥n y puede calcular gradientes para backpropagation.

### 2.2 Clase CapaActivacion

```python
class CapaActivacion:
    """
    Capa de activaci√≥n independiente.
    
    Aplica una funci√≥n de activaci√≥n elemento a elemento.
    Guarda la entrada para calcular gradientes en backpropagation.
    
    Args:
        funcion: Funci√≥n de activaci√≥n f(x)
        derivada: Derivada de la funci√≥n f'(x)
        nombre: Nombre descriptivo (para display)
    """
    
    def __init__(self, funcion, derivada, nombre="activacion"):
        self.funcion = funcion
        self.derivada = derivada
        self.nombre = nombre
        self.entradas = None
        self.salida = None
    
    def forward(self, entradas):
        """
        Aplica la funci√≥n de activaci√≥n.
        
        Guarda la entrada para usar en backward pass.
        
        Args:
            entradas: Array (batch_size, n_neuronas) ‚Äî salida de CapaDensa
        Returns:
            salida: Array del mismo shape con activaci√≥n aplicada
        """
        self.entradas = entradas.copy()
        self.salida = self.funcion(entradas)
        return self.salida
    
    def backward(self, grad_salida):
        """
        Backpropagation a trav√©s de la activaci√≥n.
        
        Multiplica el gradiente entrante por la derivada local.
        (Regla de la cadena)
        
        Args:
            grad_salida: Gradiente de la p√©rdida respecto a la salida
        Returns:
            grad_entrada: Gradiente respecto a la entrada de esta capa
        """
        return grad_salida * self.derivada(self.entradas)
    
    def contar_parametros(self):
        """Las capas de activaci√≥n no tienen par√°metros aprendibles."""
        return 0
    
    def __repr__(self):
        return f"CapaActivacion({self.nombre})"


# Instancias predefinidas
ActivacionReLU = lambda: CapaActivacion(relu, relu_derivada, "ReLU")
ActivacionSigmoid = lambda: CapaActivacion(sigmoid, sigmoid_derivada, "Sigmoid")
ActivacionTanh = lambda: CapaActivacion(tanh, tanh_derivada, "Tanh")
ActivacionLeakyReLU = lambda: CapaActivacion(leaky_relu, leaky_relu_derivada, "LeakyReLU")


# Ejemplo de uso
print("=" * 50)
print("PRUEBA DE CapaActivacion")
print("=" * 50)

capa_relu = ActivacionReLU()
X = np.array([[-2.0, -1.0, 0.0, 1.0, 2.0]])

salida = capa_relu.forward(X)
print(f"\nEntrada:  {X[0]}")
print(f"ReLU:     {salida[0]}")

# Simular un gradiente del backward pass
grad = np.ones_like(salida)  # Gradiente = 1 para todos
grad_entrada = capa_relu.backward(grad)
print(f"Gradiente entrada (backprop): {grad_entrada[0]}")
```

### 2.3 Red Neuronal con Activaciones

```python
class RedNeuronalConActivaciones:
    """
    Red neuronal que intercala capas densas y capas de activaci√≥n.
    
    Permite construir redes con cualquier combinaci√≥n de
    capas y funciones de activaci√≥n.
    
    Ejemplo:
        red = RedNeuronalConActivaciones(
            arquitectura=[10, 20, 15, 1],
            activaciones=['relu', 'relu', 'sigmoid']
        )
    """
    
    ACTIVACIONES_DISPONIBLES = {
        'relu':       (relu, relu_derivada),
        'sigmoid':    (sigmoid, sigmoid_derivada),
        'tanh':       (tanh, tanh_derivada),
        'leaky_relu': (leaky_relu, leaky_relu_derivada),
        'lineal':     (lambda x: x, lambda x: np.ones_like(x)),
    }
    
    def __init__(self, arquitectura, activaciones, seed=None):
        """
        Args:
            arquitectura: Lista de neuronas [n_in, n1, n2, ..., n_out]
            activaciones: Lista de nombres de activaciones, una por capa
                         len(activaciones) == len(arquitectura) - 1
        """
        assert len(activaciones) == len(arquitectura) - 1, \
            "Necesitas una activaci√≥n por capa densa"
        
        self.capas = []
        
        for i in range(len(arquitectura) - 1):
            n_in = arquitectura[i]
            n_out = arquitectura[i + 1]
            nombre_act = activaciones[i]
            
            # Capa densa
            self.capas.append(CapaDensa(n_in, n_out, seed=seed))
            
            # Capa de activaci√≥n
            func, deriv = self.ACTIVACIONES_DISPONIBLES[nombre_act]
            self.capas.append(CapaActivacion(func, deriv, nombre_act))
        
        self.arquitectura = arquitectura
        self.activaciones = activaciones
    
    def forward(self, X):
        """Forward pass a trav√©s de todas las capas."""
        activacion = X
        for capa in self.capas:
            activacion = capa.forward(activacion)
        return activacion
    
    def resumen(self):
        """Imprime la arquitectura con activaciones."""
        print("\n" + "=" * 65)
        print("ARQUITECTURA DE LA RED")
        print("=" * 65)
        total_params = 0
        for i, capa in enumerate(self.capas):
            params = capa.contar_parametros()
            total_params += params
            print(f"  Capa {i+1}: {capa!r:<40} | {params:>10,} par√°metros")
        print("-" * 65)
        print(f"  TOTAL:                                       {total_params:>10,} par√°metros")
        print("=" * 65)
    
    def contar_parametros(self):
        return sum(c.contar_parametros() for c in self.capas)


# Ejemplos de uso para diferentes problemas
print("=" * 60)
print("REDES PARA DIFERENTES PROBLEMAS")
print("=" * 60)

# Clasificaci√≥n binaria (spam)
print("\n1. Clasificaci√≥n Binaria (spam/no-spam):")
red_binaria = RedNeuronalConActivaciones(
    arquitectura=[100, 64, 32, 1],
    activaciones=['relu', 'relu', 'sigmoid']
)
red_binaria.resumen()

X_spam = np.random.randn(16, 100)
pred = red_binaria.forward(X_spam)
print(f"   Entrada: {X_spam.shape} ‚Üí Salida: {pred.shape}")
print(f"   Rango de salida: [{pred.min():.4f}, {pred.max():.4f}] (esperado: [0,1])")

# Clasificaci√≥n multiclase (MNIST)
print("\n2. Clasificaci√≥n Multiclase (MNIST):")
red_multiclase = RedNeuronalConActivaciones(
    arquitectura=[784, 256, 128, 10],
    activaciones=['relu', 'relu', 'lineal']  # Softmax se aplica aparte
)
X_mnist = np.random.randn(32, 784)
logits = red_multiclase.forward(X_mnist)
probs = softmax(logits)
print(f"   Logits: {logits.shape} ‚Üí Probs: {probs.shape}")
print(f"   Sumas de probabilidades: {probs.sum(axis=1)[:3].round(4)} (todas deben ser 1)")

# Regresi√≥n
print("\n3. Regresi√≥n (predicci√≥n de precios):")
red_regresion = RedNeuronalConActivaciones(
    arquitectura=[20, 64, 32, 1],
    activaciones=['relu', 'relu', 'lineal']  # Sin activaci√≥n en salida
)
X_reg = np.random.randn(8, 20)
pred_reg = red_regresion.forward(X_reg)
print(f"   Salida: {pred_reg.shape}, sin restricci√≥n de rango")
```

**Actividad 2.1**: Crea una red `[10, 20, 15, 5]` usando todas las combinaciones de activaciones: (relu, relu, relu), (tanh, tanh, sigmoid), (leaky_relu, relu, lineal). Compara las distribuciones de salida.

**Actividad 2.2**: Implementa el m√©todo `analizar_activaciones(X)` en `RedNeuronalConActivaciones` que analice las estad√≠sticas de cada capa densa y de activaci√≥n por separado.

**Actividad 2.3**: Verifica que `red.backward()` funciona correctamente para la capa de activaci√≥n: el gradiente multiplicado por la derivada debe ser correcto.

### Preguntas de Reflexi√≥n

**Pregunta 2.1 (Concebir):** ¬øPor qu√© Softmax no se incluye como capa de activaci√≥n en la red sino que se aplica despu√©s de los logits?

**Pregunta 2.2 (Dise√±ar):** ¬øQu√© ventajas tiene separar `CapaDensa` de `CapaActivacion` frente a fusionarlas en una sola clase?

**Pregunta 2.3 (Implementar):** En el m√©todo `backward()` de `CapaActivacion`, ¬øpor qu√© multiplicamos `grad_salida * derivada(entradas)` (regla de la cadena)?

**Pregunta 2.4 (Operar):** Si al inferir en producci√≥n obtienes probabilidades de Softmax siempre muy uniformes (~0.1 para 10 clases), ¬øqu√© podr√≠a indicar esto sobre el estado del modelo?

---

## üî¨ Parte 3: Visualizaci√≥n y An√°lisis Comparativo (35 min)

### 3.1 Introducci√≥n Conceptual: Visualizar para Comprender

**¬øQu√© hacemos?** Graficar las funciones de activaci√≥n y sus derivadas, y comparar su comportamiento.

**¬øPor qu√© lo hacemos?** Las gr√°ficas revelan intuitivamente:
- En qu√© rangos satura cada funci√≥n (derivada ‚âà 0)
- C√≥mo se distribuyen los gradientes
- Por qu√© ReLU mitiga el gradiente desvaneciente
- El efecto de cada funci√≥n sobre la distribuci√≥n de activaciones

**¬øQu√© resultados esperar?** Gr√°ficas claras que muestren las curvas de cada funci√≥n y sus derivadas, con anotaciones que expliquen el comportamiento en regiones clave.

### 3.2 Comparaci√≥n Visual de Funciones y Derivadas

```python
import matplotlib.pyplot as plt

def graficar_funciones_activacion():
    """
    Crea una visualizaci√≥n completa de todas las funciones
    de activaci√≥n y sus derivadas.
    """
    x = np.linspace(-5, 5, 300)
    
    funciones = [
        ('ReLU',        relu(x),          relu_derivada(x),         'steelblue'),
        ('Sigmoid',     sigmoid(x),        sigmoid_derivada(x),      'darkorange'),
        ('Tanh',        tanh(x),           tanh_derivada(x),         'green'),
        ('Leaky ReLU',  leaky_relu(x),     leaky_relu_derivada(x),   'red'),
    ]
    
    fig, axes = plt.subplots(2, 4, figsize=(18, 9))
    fig.suptitle('Funciones de Activaci√≥n y sus Derivadas', 
                 fontsize=15, fontweight='bold', y=1.01)
    
    for i, (nombre, f_x, df_x, color) in enumerate(funciones):
        # Funci√≥n
        ax = axes[0, i]
        ax.plot(x, f_x, color=color, linewidth=2.5)
        ax.axhline(0, color='black', linewidth=0.5, linestyle='--')
        ax.axvline(0, color='black', linewidth=0.5, linestyle='--')
        ax.set_title(f'{nombre}', fontsize=12, fontweight='bold', color=color)
        ax.set_xlabel('x')
        ax.set_ylabel('f(x)')
        ax.grid(True, alpha=0.3)
        ax.set_xlim(-5, 5)
        
        # Derivada
        ax = axes[1, i]
        ax.plot(x, df_x, color=color, linewidth=2.5, linestyle='--')
        ax.axhline(0, color='black', linewidth=0.5, linestyle='--')
        ax.axvline(0, color='black', linewidth=0.5, linestyle='--')
        ax.set_title(f"Derivada de {nombre}", fontsize=11, color=color)
        ax.set_xlabel('x')
        ax.set_ylabel("f'(x)")
        ax.grid(True, alpha=0.3)
        ax.set_xlim(-5, 5)
    
    plt.tight_layout()
    plt.savefig('activaciones_comparacion.png', dpi=120, bbox_inches='tight')
    plt.show()
    print("‚úÖ Gr√°fico guardado como 'activaciones_comparacion.png'")

graficar_funciones_activacion()
```

### 3.3 El Problema del Gradiente que Desaparece

```python
def demostrar_gradiente_desaparece(n_capas=15):
    """
    Demuestra c√≥mo el gradiente se desvanece al propagarse hacia atr√°s
    en redes profundas con activaciones saturantes.
    
    Args:
        n_capas: N√∫mero de capas a simular
    """
    print("=" * 65)
    print("DEMOSTRACI√ìN: GRADIENTE QUE DESAPARECE")
    print("=" * 65)
    
    # Simular backpropagation con diferentes activaciones
    # Si el gradiente en cada capa es d, despu√©s de n capas: gradiente ‚âà d^n
    
    gradientes_sigmoid = []
    gradientes_tanh = []
    gradientes_relu = []
    
    # Punto de operaci√≥n: x=0 (donde los gradientes son m√°s favorables)
    x = np.array([0.0])
    
    grad_sigmoid = sigmoid_derivada(x)[0]   # ‚âà 0.25 (m√°ximo)
    grad_tanh = tanh_derivada(x)[0]         # = 1.0 (m√°ximo)
    grad_relu = relu_derivada(np.array([1.0]))[0]  # = 1.0
    
    print(f"\nGradiente por capa (en punto √≥ptimo x=0):")
    print(f"  Sigmoid:  œÉ'(0) = {grad_sigmoid:.4f}")
    print(f"  Tanh:    tanh'(0) = {grad_tanh:.4f}")
    print(f"  ReLU:   ReLU'(1) = {grad_relu:.4f}")
    
    print(f"\n{'Capa':<6} | {'Sigmoid':<15} | {'Tanh':<15} | {'ReLU':<15}")
    print("-" * 55)
    
    g_sig = 1.0
    g_tanh = 1.0
    g_relu = 1.0
    
    for capa in range(1, n_capas + 1):
        g_sig  *= grad_sigmoid
        g_tanh *= grad_tanh
        g_relu *= grad_relu
        
        gradientes_sigmoid.append(g_sig)
        gradientes_tanh.append(g_tanh)
        gradientes_relu.append(g_relu)
        
        if capa <= 10 or capa == n_capas:
            print(f"{capa:<6} | {g_sig:<15.2e} | {g_tanh:<15.2e} | {g_relu:<15.2e}")
    
    print(f"\n‚ö†Ô∏è  CONCLUSI√ìN:")
    print(f"  Sigmoid tras {n_capas} capas: gradiente = {g_sig:.2e}")
    print(f"  ‚Üí ¬°{n_capas} √≥rdenes de magnitud m√°s peque√±o!")
    print(f"  ‚Üí Las primeras capas CASI NO APRENDEN")
    print(f"\n  ReLU mantiene gradiente = {g_relu:.2e}")
    print(f"  ‚Üí Todas las capas aprenden a la misma tasa")
    
    # Visualizaci√≥n
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    capas = list(range(1, n_capas + 1))
    ax1.semilogy(capas, gradientes_sigmoid, 'o-', color='darkorange', 
                 linewidth=2, label='Sigmoid')
    ax1.semilogy(capas, gradientes_tanh, 's-', color='green', 
                 linewidth=2, label='Tanh')
    ax1.semilogy(capas, gradientes_relu, '^-', color='steelblue', 
                 linewidth=2, label='ReLU')
    ax1.set_xlabel('N√∫mero de capa (desde la salida)')
    ax1.set_ylabel('Magnitud del gradiente (escala log)')
    ax1.set_title('Gradiente que Desaparece', fontsize=13, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Comparaci√≥n de derivadas
    x_range = np.linspace(-4, 4, 200)
    ax2.plot(x_range, sigmoid_derivada(x_range), '--', color='darkorange', 
             linewidth=2, label=f"Sigmoid' (max={sigmoid_derivada(np.array([0.]))[0]:.3f})")
    ax2.plot(x_range, tanh_derivada(x_range), '--', color='green', 
             linewidth=2, label=f"Tanh' (max={tanh_derivada(np.array([0.]))[0]:.3f})")
    ax2.plot(x_range, relu_derivada(x_range), '-', color='steelblue', 
             linewidth=2, label=f"ReLU' (max=1.0)")
    ax2.set_xlabel('x')
    ax2.set_ylabel("f'(x)")
    ax2.set_title('Comparaci√≥n de Derivadas', fontsize=13, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(-0.05, 1.1)
    
    plt.tight_layout()
    plt.savefig('saturacion_gradientes.png', dpi=120, bbox_inches='tight')
    plt.show()
    print("‚úÖ Gr√°fico guardado como 'saturacion_gradientes.png'")

demostrar_gradiente_desaparece()
```

### 3.4 Neuronas Muertas en ReLU

```python
def analizar_neuronas_muertas(red, X, umbral=0.01):
    """
    Detecta y cuantifica neuronas muertas en redes con ReLU.
    
    Una neurona "muerta" es aquella cuya activaci√≥n es cero
    para TODAS las muestras del dataset. Una vez muerta,
    no puede aprender porque su gradiente es siempre 0.
    
    Args:
        red: RedNeuronalConActivaciones con ReLU
        X: Datos de entrada (batch_size, n_entradas)
        umbral: Porcentaje m√°ximo de activaciones > 0 para considerar "muerta"
    
    Returns:
        dict: Estad√≠sticas de neuronas muertas por capa
    """
    print("=" * 60)
    print("AN√ÅLISIS DE NEURONAS MUERTAS (ReLU)")
    print("=" * 60)
    
    stats = {}
    activacion = X
    
    for i, capa in enumerate(red.capas):
        activacion = capa.forward(activacion)
        
        if isinstance(capa, CapaActivacion) and capa.nombre == "ReLU":
            # Verificar cu√°ntas neuronas tienen activaci√≥n 0 en TODAS las muestras
            activa_por_neurona = (activacion > umbral).mean(axis=0)  # Shape: (n_neuronas,)
            muertas = (activa_por_neurona == 0).sum()
            total = activacion.shape[1]
            
            print(f"\n  Capa ReLU #{i+1}:")
            print(f"    Total neuronas:    {total}")
            print(f"    Neuronas muertas:  {muertas} ({muertas/total:.1%})")
            print(f"    Activadas siempre: {(activa_por_neurona == 1).sum()} "
                  f"({(activa_por_neurona == 1).sum()/total:.1%})")
            print(f"    Activaci√≥n media:  {activa_por_neurona.mean():.1%} de las muestras")
            
            stats[f'relu_{i}'] = {
                'muertas': muertas,
                'total': total,
                'porcentaje': muertas / total
            }
    
    return stats


# Demostraci√≥n con inicializaci√≥n que causa muchas neuronas muertas
print("\n--- Con inicializaci√≥n muy negativa (bias negativo grande) ---")
np.random.seed(42)
red_mala_init = RedNeuronalConActivaciones(
    arquitectura=[20, 50, 30, 5],
    activaciones=['relu', 'relu', 'relu']
)
# Forzar bias negativos grandes para crear neuronas muertas
for capa in red_mala_init.capas:
    if isinstance(capa, CapaDensa):
        capa.biases = np.full_like(capa.biases, -5.0)  # Bias muy negativo

X_test = np.random.randn(500, 20)
stats = analizar_neuronas_muertas(red_mala_init, X_test)

print("\n--- Con inicializaci√≥n est√°ndar ---")
red_buena_init = RedNeuronalConActivaciones(
    arquitectura=[20, 50, 30, 5],
    activaciones=['relu', 'relu', 'relu'],
    seed=42
)
stats2 = analizar_neuronas_muertas(red_buena_init, X_test)
```

**Actividad 3.1**: Grafica las 4 funciones de activaci√≥n en un mismo gr√°fico. ¬øCu√°l tiene el rango m√°s amplio de valores? ¬øCu√°l tiene la derivada m√°s simple?

**Actividad 3.2**: Ejecuta `demostrar_gradiente_desaparece()` con 20 capas. ¬øCu√°ntas capas puede soportar Sigmoid antes de que el gradiente sea menor que $10^{-10}$?

**Actividad 3.3**: Experimenta con diferentes bias negativos y mide cu√°ntas neuronas mueren. ¬øA partir de qu√© valor de bias aparecen neuronas muertas?

**Actividad 3.4**: Compara las distribuciones de activaciones de ReLU vs Leaky ReLU en las capas intermedias de una red profunda.

### Preguntas de Reflexi√≥n

**Pregunta 3.1 (Concebir):** ¬øPor qu√© el problema del gradiente desvaneciente es m√°s severo en redes muy profundas que en redes poco profundas?

**Pregunta 3.2 (Dise√±ar):** Si debes usar Sigmoid o Tanh (por requisitos del dominio), ¬øqu√© estrategias complementarias podr√≠as usar para mitigar el vanishing gradient?

**Pregunta 3.3 (Implementar):** ¬øC√≥mo podr√≠as modificar la inicializaci√≥n de biases para reducir la aparici√≥n de neuronas muertas al inicio del entrenamiento?

**Pregunta 3.4 (Operar):** En un sistema en producci√≥n, ¬øc√≥mo detectar√≠as en tiempo real si tu red est√° sufriendo de neuronas muertas o gradiente desvaneciente?

---

## üî¨ Parte 4: Casos de Uso y Selecci√≥n de Activaciones (30 min)

### 4.1 Introducci√≥n Conceptual: La Elecci√≥n Correcta Importa

**¬øQu√© hacemos?** Estudiar qu√© funci√≥n de activaci√≥n corresponde a cada tipo de problema y posici√≥n en la red.

**¬øPor qu√© lo hacemos?** Una elecci√≥n incorrecta puede hacer que la red no converja, produzca salidas sin sentido (probabilidades > 1), o aprenda muy lentamente.

**Reglas generales:**

| Posici√≥n en la red | Problema | Activaci√≥n recomendada |
|-------------------|----------|----------------------|
| Capas ocultas | Cualquiera | **ReLU** (o Leaky ReLU) |
| Capa de salida | Clasificaci√≥n binaria | **Sigmoid** |
| Capa de salida | Clasificaci√≥n multiclase | **Softmax** |
| Capa de salida | Regresi√≥n | **Lineal** (sin activaci√≥n) |
| Capas ocultas | RNNs, LSTMs | **Tanh** |
| Capas ocultas | Generativas (GANs) | **Leaky ReLU**, Tanh |

**¬øQu√© resultados esperar?** Redes cuyas salidas tienen el rango e interpretaci√≥n correctos para cada tipo de problema.

### 4.2 Comparaci√≥n Experimental de Combinaciones

```python
def comparar_configuraciones():
    """
    Compara experimentalmente diferentes combinaciones de activaciones
    y analiza su impacto en las distribuciones de salida.
    """
    print("=" * 65)
    print("COMPARACI√ìN EXPERIMENTAL DE CONFIGURACIONES")
    print("=" * 65)
    
    np.random.seed(42)
    X = np.random.randn(200, 10)
    
    configuraciones = [
        {
            'nombre': 'Solo Sigmoid (problem√°tico en capas ocultas)',
            'arq': [10, 20, 15, 5],
            'acts': ['sigmoid', 'sigmoid', 'sigmoid']
        },
        {
            'nombre': 'Solo ReLU (bueno para capas ocultas)',
            'arq': [10, 20, 15, 5],
            'acts': ['relu', 'relu', 'relu']
        },
        {
            'nombre': 'ReLU ocultas + Sigmoid salida (clasificaci√≥n binaria)',
            'arq': [10, 20, 15, 1],
            'acts': ['relu', 'relu', 'sigmoid']
        },
        {
            'nombre': 'ReLU ocultas + Lineal salida (regresi√≥n)',
            'arq': [10, 20, 15, 1],
            'acts': ['relu', 'relu', 'lineal']
        },
        {
            'nombre': 'Tanh ocultas + Lineal salida',
            'arq': [10, 20, 15, 5],
            'acts': ['tanh', 'tanh', 'lineal']
        },
    ]
    
    for config in configuraciones:
        red = RedNeuronalConActivaciones(
            arquitectura=config['arq'],
            activaciones=config['acts'],
            seed=42
        )
        salida = red.forward(X)
        
        print(f"\nüìä {config['nombre']}")
        print(f"   Shape salida: {salida.shape}")
        print(f"   Media: {salida.mean():.4f} | Std: {salida.std():.4f}")
        print(f"   Min:   {salida.min():.4f} | Max: {salida.max():.4f}")
        
        # Para salida con Softmax
        if config['acts'][-1] == 'lineal' and salida.shape[1] == 5:
            probs = softmax(salida)
            print(f"   (Softmax) Suma probs: {probs.sum(axis=1).mean():.6f}")

comparar_configuraciones()
```

### 4.3 Gu√≠a de Selecci√≥n Pr√°ctica

```python
def recomendar_activacion(tipo_problema, posicion_capa, info_adicional=None):
    """
    Recomienda la funci√≥n de activaci√≥n apropiada.
    
    Args:
        tipo_problema: 'binaria', 'multiclase', 'regresion', 'rnn'
        posicion_capa: 'oculta', 'salida'
        info_adicional: dict con contexto extra
    
    Returns:
        str: Nombre de la activaci√≥n recomendada y justificaci√≥n
    """
    reglas = {
        ('salida', 'binaria'):     ('sigmoid',  'Salida en (0,1) ‚Üí interpretable como probabilidad'),
        ('salida', 'multiclase'):  ('softmax',  'Distribuci√≥n de prob. que suma 1'),
        ('salida', 'regresion'):   ('lineal',   'Sin restricci√≥n de rango para valores continuos'),
        ('oculta', 'general'):     ('relu',     'Eficiente, evita vanishing gradient'),
        ('oculta', 'rnn'):         ('tanh',     'Centrada en 0, gradientes m√°s estables en RNNs'),
        ('oculta', 'profunda'):    ('relu',     'Ideal para redes muy profundas'),
    }
    
    key = (posicion_capa, tipo_problema)
    if key in reglas:
        act, justif = reglas[key]
        print(f"‚úÖ Recomendaci√≥n: {act.upper()}")
        print(f"   Justificaci√≥n: {justif}")
        return act
    else:
        print("‚ö†Ô∏è  Situaci√≥n no contemplada. Usa ReLU como punto de partida.")
        return 'relu'


# Ejemplos de uso
print("ü§î ¬øQu√© activaci√≥n usar?\n")
print("Caso 1: Capa oculta en clasificaci√≥n de im√°genes")
recomendar_activacion('general', 'oculta')

print("\nCaso 2: Capa de salida para clasificar 10 d√≠gitos")
recomendar_activacion('multiclase', 'salida')

print("\nCaso 3: Capa de salida para predecir temperatura")
recomendar_activacion('regresion', 'salida')

print("\nCaso 4: Capa oculta en red recurrente (RNN)")
recomendar_activacion('rnn', 'oculta')
```

**Actividad 4.1**: Dise√±a e implementa redes para los siguientes escenarios. Justifica cada elecci√≥n de activaci√≥n:
- Detector de fraude (binario): input=30 features, output=probabilidad de fraude
- Clasificador de sentimientos (5 clases): input=1000 features de texto
- Predictor de temperatura (regresi√≥n): input=10 variables meteorol√≥gicas

**Actividad 4.2**: Implementa un experimento que compare el tiempo de convergencia de una red con Sigmoid vs ReLU en capas ocultas usando gradiente descendente manual.

**Actividad 4.3**: Dise√±a una red para el problema XOR con activaciones no lineales. ¬øResuelve el problema que una red lineal no pod√≠a?

### Preguntas de Reflexi√≥n

**Pregunta 4.1 (Concebir):** ¬øPor qu√© usar Sigmoid en capas ocultas de redes profundas (m√°s de 5 capas) es generalmente una mala pr√°ctica?

**Pregunta 4.2 (Dise√±ar):** Si tienes una red para predicci√≥n de ratings (1-5 estrellas), ¬øqu√© activaci√≥n usar√≠as en la capa de salida? ¬øC√≥mo representar√≠as el problema?

**Pregunta 4.3 (Implementar):** ¬øC√≥mo implementar√≠as una funci√≥n de activaci√≥n personalizada que solo uses en un problema espec√≠fico?

**Pregunta 4.4 (Operar):** En un modelo de producci√≥n para clasificaci√≥n multiclase, ¬øcu√°ndo usar√≠as las probabilidades del Softmax directamente vs la clase predicha (`argmax`)?

---

## üìä An√°lisis Final de Rendimiento

### Benchmark: Velocidad de Funciones de Activaci√≥n

Las funciones de activaci√≥n se aplican millones de veces durante el entrenamiento. Su velocidad de ejecuci√≥n importa significativamente.

**Fundamento:** ReLU es simplemente `np.maximum(0, x)`, una operaci√≥n elementwise extremadamente r√°pida. Sigmoid y Tanh requieren c√°lculo de exponenciales, que es m√°s costoso. Este benchmark te mostrar√° el impacto pr√°ctico.

```python
import time

def benchmark_activaciones(n=10_000_000, repeticiones=5):
    """
    Compara el tiempo de ejecuci√≥n de cada funci√≥n de activaci√≥n.
    
    Args:
        n: N√∫mero de elementos en el array
        repeticiones: N√∫mero de mediciones para promediar
    """
    print("\n" + "=" * 65)
    print(f"BENCHMARK: VELOCIDAD DE FUNCIONES DE ACTIVACI√ìN")
    print(f"Array de {n:,} elementos, {repeticiones} repeticiones")
    print("=" * 65)
    
    x = np.random.randn(n)
    
    funciones_test = {
        'ReLU':        relu,
        'Sigmoid':     sigmoid,
        'Tanh':        tanh,
        'Leaky ReLU':  leaky_relu,
        'Softmax':     lambda x_: softmax(x_.reshape(100, -1)).ravel(),
    }
    
    tiempos = {}
    for nombre, func in funciones_test.items():
        mediciones = []
        for _ in range(repeticiones):
            start = time.perf_counter()
            _ = func(x)
            mediciones.append(time.perf_counter() - start)
        
        t_med = np.mean(mediciones[1:])  # Descartar primera medici√≥n (cold start)
        tiempos[nombre] = t_med
    
    # Normalizar respecto a ReLU
    t_relu = tiempos['ReLU']
    
    print(f"\n{'Funci√≥n':<15} | {'Tiempo (ms)':<15} | {'Relativo a ReLU'}")
    print("-" * 50)
    for nombre, t in sorted(tiempos.items(), key=lambda x: x[1]):
        print(f"{nombre:<15} | {t*1000:<15.3f} | {t/t_relu:.2f}x")
    
    print(f"\nüí° ReLU es la m√°s r√°pida por ser solo max(0,x)")
    print(f"   Sigmoid/Tanh son ~{tiempos['Sigmoid']/t_relu:.1f}x m√°s lentas por las exponenciales")

benchmark_activaciones()
```

### An√°lisis del Impacto en Distribuci√≥n de Activaciones

```python
def analizar_impacto_activaciones_en_red():
    """
    Analiza c√≥mo cada activaci√≥n afecta la distribuci√≥n de
    activaciones a trav√©s de una red profunda (10 capas).
    """
    import matplotlib.pyplot as plt
    
    np.random.seed(42)
    n_capas = 10
    n_neuronas = 100
    X = np.random.randn(1000, n_neuronas)
    
    activaciones_test = ['relu', 'sigmoid', 'tanh', 'leaky_relu']
    
    fig, axes = plt.subplots(len(activaciones_test), n_capas, 
                              figsize=(n_capas * 2.5, len(activaciones_test) * 2.5))
    
    for row, act_nombre in enumerate(activaciones_test):
        arquitectura = [n_neuronas] + [n_neuronas] * n_capas
        activaciones = [act_nombre] * n_capas
        red = RedNeuronalConActivaciones(arquitectura, activaciones, seed=42)
        
        activacion = X
        for col, capa in enumerate(red.capas):
            activacion = capa.forward(activacion)
            if isinstance(capa, CapaActivacion):
                idx_capa = col // 2  # Cada 2 capas hay una activaci√≥n
                ax = axes[row, idx_capa]
                ax.hist(activacion.ravel(), bins=30, alpha=0.7, 
                       color=plt.cm.tab10(row), edgecolor='black', linewidth=0.5)
                ax.set_xticks([])
                ax.set_yticks([])
                if idx_capa == 0:
                    ax.set_ylabel(act_nombre.upper(), fontsize=10, fontweight='bold')
                if row == 0:
                    ax.set_title(f'Capa {idx_capa+1}', fontsize=9)
    
    plt.suptitle('Distribuci√≥n de Activaciones por Capa y Funci√≥n\n'
                 '(Cada columna = una capa, cada fila = una activaci√≥n)',
                 fontsize=12, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig('distribucion_activaciones.png', dpi=120, bbox_inches='tight')
    plt.show()
    print("‚úÖ Gr√°fico guardado como 'distribucion_activaciones.png'")

analizar_impacto_activaciones_en_red()
```

---

## üéØ EJERCICIOS PROPUESTOS

### Ejercicio 1: Verificaci√≥n Num√©rica de Derivadas (B√°sico)

**Objetivo:** Verificar matem√°ticamente que las derivadas implementadas son correctas.

**Contexto:** La verificaci√≥n num√©rica de gradientes (gradient checking) es una t√©cnica est√°ndar en deep learning para detectar bugs en la implementaci√≥n de backpropagation.

**Tareas:**
1. Implementa `gradient_check(func, func_deriv, x, h=1e-5)` que compare derivada anal√≠tica vs num√©rica
2. Verifica las 4 funciones principales: ReLU, Sigmoid, Tanh, Leaky ReLU
3. Reporta el error relativo: `|df_analitica - df_numerica| / max(|df_analitica|, |df_numerica|, Œµ)`
4. Identifica en qu√© puntos el gradient check puede fallar (discontinuidades)

```python
def gradient_check(func, func_deriv, x, h=1e-5, verbose=True):
    """
    Verifica la derivada anal√≠tica contra la aproximaci√≥n num√©rica.
    
    Usa diferencias centradas: f'(x) ‚âà (f(x+h) - f(x-h)) / (2h)
    
    Args:
        func: Funci√≥n de activaci√≥n f(x)
        func_deriv: Derivada anal√≠tica f'(x)
        x: Puntos donde verificar
        h: Paso para diferencias finitas
        verbose: Si imprimir resultados detallados
    
    Returns:
        error_relativo: Error relativo m√°ximo
    """
    # Tu c√≥digo aqu√≠
    pass

# Verifica todas las funciones
x_test = np.array([-3.0, -1.0, -0.1, 0.1, 1.0, 3.0])
for nombre, (func, deriv) in ACTIVACIONES.items():
    print(f"\n{nombre}:")
    error = gradient_check(func, deriv, x_test)
```

### Ejercicio 2: An√°lisis de Saturaci√≥n (Intermedio)

**Objetivo:** Cuantificar y visualizar el problema de saturaci√≥n en redes profundas.

**Contexto:** La saturaci√≥n ocurre cuando las activaciones se concentran en los extremos de la funci√≥n, donde la derivada es pr√°cticamente cero. Esto "mata" los gradientes.

**Tareas:**
1. Para cada funci√≥n de activaci√≥n, define el rango "activo" donde `|f'(x)| > 0.01`
2. Genera datos con diferentes distribuciones (normal, uniforme, sesgada)
3. Mide el porcentaje de activaciones en la zona saturada
4. Grafica la relaci√≥n entre la escala de entrada y el porcentaje de saturaci√≥n

```python
def analizar_saturacion(func_deriv, nombre, umbral_grad=0.01):
    """
    Mide el porcentaje de saturaci√≥n para una funci√≥n de activaci√≥n.
    
    Args:
        func_deriv: Derivada de la funci√≥n
        nombre: Nombre para mostrar
        umbral_grad: Umbral m√≠nimo de gradiente para considerar "activo"
    
    Returns:
        dict: Estad√≠sticas de saturaci√≥n
    """
    # Tu c√≥digo aqu√≠
    pass

# Analiza las 4 funciones
for nombre, (func, deriv) in ACTIVACIONES.items():
    analizar_saturacion(deriv, nombre)
```

### Ejercicio 3: Red para el Problema XOR con Activaciones (Intermedio)

**Objetivo:** Demostrar que las funciones de activaci√≥n no lineales permiten resolver problemas que una red lineal no puede.

**Contexto:** En Lab 02 demostramos que sin activaciones, una red multicapa es lineal. Ahora verificaremos que con activaciones, una red puede resolver XOR.

**Tareas:**
1. Crear los datos XOR: 4 puntos con etiquetas 0 y 1
2. Implementar entrenamiento manual (gradiente descendente b√°sico, 1000 iteraciones)
3. Comparar la red lineal (sin activaci√≥n) vs la red con ReLU o Sigmoid
4. Visualizar la frontera de decisi√≥n aprendida

```python
# Datos XOR
X_xor = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=float)
y_xor = np.array([[0], [1], [1], [0]], dtype=float)

def entrenar_paso(red, X, y, lr=0.1):
    """
    Un paso de entrenamiento simplificado (sin backprop completo).
    Esta es una aproximaci√≥n educativa, no producci√≥n.
    """
    # Tu c√≥digo aqu√≠ (puede ser entrenamiento num√©rico de gradiente)
    pass

# Prueba con red lineal vs red con activaci√≥n
# ¬øCu√°l converge para XOR?
```

### Ejercicio 4: Softmax con Temperatura (Avanzado)

**Objetivo:** Entender el efecto de la temperatura en Softmax y su uso en modelos generativos.

**Contexto:** La temperatura $T$ en Softmax controla la "confianza" del modelo. Se usa en modelos de lenguaje (GPT, ChatGPT) para controlar la creatividad vs coherencia de las respuestas.

**Tareas:**
1. Implementa `softmax_temperatura(x, T)` = `softmax(x/T)`
2. Grafica la distribuci√≥n para temperaturas: T = 0.1, 0.5, 1.0, 2.0, 10.0
3. Calcula la entrop√≠a de la distribuci√≥n resultante para cada T
4. Explica: ¬øqu√© temperatura usar√≠as para un asistente de c√≥digo preciso? ¬øY para escritura creativa?

```python
def softmax_temperatura(x, T=1.0):
    """
    Softmax con temperatura.
    
    T ‚Üí 0: distribuci√≥n m√°s concentrada (greedy)
    T = 1: softmax est√°ndar
    T ‚Üí ‚àû: distribuci√≥n uniforme (aleatoria)
    
    Args:
        x: Logits (batch_size, n_clases)
        T: Temperatura (escalar positivo)
    """
    return softmax(x / T)


def entropia(probs):
    """
    Calcula la entrop√≠a de Shannon de una distribuci√≥n.
    H(p) = -sum(p * log(p))
    """
    # Tu c√≥digo aqu√≠
    pass

# An√°lisis completo con visualizaci√≥n
```

### Ejercicio 5: Funci√≥n de Activaci√≥n Personalizada ‚Äî GELU (Proyecto)

**Objetivo:** Implementar y analizar GELU, la funci√≥n de activaci√≥n usada en GPT y BERT.

**Contexto:** GELU (Gaussian Error Linear Unit) fue propuesta en 2016 y se ha convertido en el est√°ndar para transformers. Es una versi√≥n suavizada de ReLU que pondera cada activaci√≥n por su probabilidad bajo una distribuci√≥n gaussiana.

$$\text{GELU}(x) = x \cdot \Phi(x) \approx 0.5x\left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right)$$

donde $\Phi(x)$ es la CDF de la distribuci√≥n normal est√°ndar.

**Tareas:**
1. Implementa GELU usando la aproximaci√≥n de tanh
2. Implementa su derivada (puede ser num√©rica o anal√≠tica)
3. Compara con ReLU: rango, saturaci√≥n, suavidad
4. Implementa y prueba una red usando GELU en todas las capas ocultas
5. Integra GELU en `ACTIVACIONES` y en `RedNeuronalConActivaciones`

```python
def gelu(x):
    """
    Gaussian Error Linear Unit (GELU).
    Usada en GPT-2, GPT-3, BERT, ViT, etc.
    
    Aproximaci√≥n con tanh (m√°s eficiente que usar scipy.stats):
    GELU(x) ‚âà 0.5x(1 + tanh(‚àö(2/œÄ)(x + 0.044715x¬≥)))
    
    Args:
        x: Array NumPy de cualquier shape
    Returns:
        Array del mismo shape
    """
    # Tu c√≥digo aqu√≠
    pass


def gelu_derivada(x):
    """Derivada de GELU (usa gradient_check para verificar)."""
    # Tu c√≥digo aqu√≠ (puede ser aproximaci√≥n num√©rica)
    pass

# Integraci√≥n en el sistema
# Agrega GELU a ACTIVACIONES y prueba una red completa
```

---

## üìù Entregables

### 1. C√≥digo Implementado (60%)

**Requisitos m√≠nimos:**
- Todas las funciones de activaci√≥n y sus derivadas: `relu`, `sigmoid`, `tanh`, `softmax`, `leaky_relu`
- Clase `CapaActivacion` con `forward()` y `backward()`
- Clase `RedNeuronalConActivaciones` con soporte para m√∫ltiples activaciones
- Al menos 3 ejercicios propuestos implementados y verificados
- Tests con `gradient_check` para verificar derivadas

**Criterios de calidad:**
- C√≥digo limpio, PEP8, con docstrings completos
- Manejo de casos borde (overflow en sigmoid, distribuciones extremas en softmax)
- Tests que verifican shapes, rangos y propiedades matem√°ticas

### 2. Notebook de Experimentaci√≥n (25%)

**Debe incluir:**
- Todas las actividades de las partes 1-4 ejecutadas y analizadas
- Visualizaciones de funciones, derivadas, y distribuciones de activaciones
- Comparativa experimental de configuraciones (diferentes activaciones en capas ocultas)
- Demostraci√≥n del problema del gradiente desvaneciente con gr√°ficas
- An√°lisis de neuronas muertas con diferentes inicializaciones
- Respuestas escritas a todas las Preguntas de Reflexi√≥n

### 3. Reporte T√©cnico (15%)

**Secciones requeridas:**
1. Introducci√≥n: por qu√© las activaciones son esenciales
2. Marco te√≥rico: descripci√≥n matem√°tica de cada funci√≥n y sus derivadas
3. Metodolog√≠a: experimentos dise√±ados y realizados
4. Resultados: tablas comparativas, gr√°ficas, gradient checks
5. An√°lisis y discusi√≥n: ventajas y limitaciones de cada activaci√≥n
6. Conclusiones y recomendaciones: gu√≠a personal de selecci√≥n de activaciones

**Extensi√≥n:** 3-5 p√°ginas, formato PDF

### Formato de Entrega

```
Lab03_Entrega_NombreApellido/
‚îú‚îÄ‚îÄ codigo/
‚îÇ   ‚îú‚îÄ‚îÄ activaciones.py          # Funciones de activaci√≥n y derivadas
‚îÇ   ‚îú‚îÄ‚îÄ red_con_activaciones.py  # Clases CapaActivacion y RedNeuronalConActivaciones
‚îÇ   ‚îî‚îÄ‚îÄ tests.py                 # Gradient checks y tests de propiedades
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ experimentos.ipynb
‚îú‚îÄ‚îÄ reporte/
‚îÇ   ‚îî‚îÄ‚îÄ reporte_lab03.pdf
‚îî‚îÄ‚îÄ README.md
```

---

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Concebir (25%)

**Comprensi√≥n conceptual:**
- ‚úÖ Explica por qu√© la no-linealidad es necesaria en redes profundas
- ‚úÖ Comprende el problema del gradiente desvaneciente y su causa
- ‚úÖ Distingue cu√°ndo usar cada funci√≥n de activaci√≥n
- ‚úÖ Entiende la relaci√≥n entre activaciones y las derivadas en backpropagation

**Evidencia:** Respuestas a preguntas de reflexi√≥n, introducci√≥n del reporte

### Dise√±ar (25%)

**Planificaci√≥n:**
- ‚úÖ Dise√±a redes con activaciones apropiadas para cada tipo de problema
- ‚úÖ Planifica experimentos para comparar funciones de activaci√≥n
- ‚úÖ Propone soluciones para neuronas muertas y vanishing gradient
- ‚úÖ Considera stabilidad num√©rica en implementaciones

**Evidencia:** Ejercicios 1-5, secci√≥n de metodolog√≠a del reporte

### Implementar (30%)

**Construcci√≥n:**
- ‚úÖ Funciones de activaci√≥n correctas (verificadas con gradient check)
- ‚úÖ `CapaActivacion` con forward y backward funcionales
- ‚úÖ `RedNeuronalConActivaciones` extensible y correcta
- ‚úÖ C√≥digo documentado, limpio, con manejo de errores

**Evidencia:** C√≥digo fuente, resultados de tests

### Operar (20%)

**Validaci√≥n y an√°lisis:**
- ‚úÖ Ejecuta benchmarks comparativos de velocidad
- ‚úÖ Analiza distribuciones de activaciones en redes profundas
- ‚úÖ Diagnostica y cuantifica neuronas muertas
- ‚úÖ Extrae conclusiones pr√°cticas sobre selecci√≥n de activaciones

**Evidencia:** Notebook de experimentos, secci√≥n de resultados del reporte

### R√∫brica Detallada

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|----------------|----------------------|-------------------|
| **Implementaci√≥n** | Todas las funciones correctas, gradient check <1e-6, c√≥digo impecable | Funciones correctas, documentaci√≥n b√°sica | Mayoria de funciones correctas, errores menores | Funciones incorrectas o incompletas |
| **Comprensi√≥n te√≥rica** | Explica intuici√≥n, derivadas, limitaciones con detalle | Correcto, aplica bien | Comprensi√≥n b√°sica | Comprensi√≥n incorrecta o ausente |
| **Experimentaci√≥n** | Experimentos creativos, hip√≥tesis, conclusiones profundas | Todos los experimentos requeridos | Experimentos b√°sicos | Experimentos incompletos |
| **Documentaci√≥n** | Excelente: clara, matem√°ticamente rigurosa | Buena y completa | B√°sica | Pobre o ausente |

---

## üìö Referencias Adicionales

### Libros

1. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep Learning*
   - Cap√≠tulo 6, Secci√≥n 6.3: Hidden Units (funciones de activaci√≥n)
   - Cap√≠tulo 8, Secci√≥n 8.1: Vanishing and Exploding Gradients
   - http://www.deeplearningbook.org

2. **Nielsen, M.** (2015). *Neural Networks and Deep Learning*
   - Cap√≠tulo 1: Sigmoid neurons y el problema de activaciones
   - http://neuralnetworksanddeeplearning.com

3. **Zhang, A. et al.** (2023). *Dive into Deep Learning*
   - Cap√≠tulo 5: Multilayer Perceptrons (con activaciones)
   - https://d2l.ai

### Art√≠culos Acad√©micos

1. **Hochreiter, S.** (1991). "Untersuchungen zu dynamischen neuronalen Netzen"
   - Primera documentaci√≥n del problema del gradiente desvaneciente

2. **Nair, V., & Hinton, G.E.** (2010). "Rectified linear units improve restricted Boltzmann machines"
   - Introducci√≥n de ReLU como funci√≥n de activaci√≥n pr√°ctica
   - *Proceedings of ICML*

3. **Glorot, X., Bordes, A., & Bengio, Y.** (2011). "Deep sparse rectifier neural networks"
   - An√°lisis de por qu√© ReLU funciona mejor que Sigmoid
   - *Proceedings of AISTATS*

4. **Hendrycks, D., & Gimpel, K.** (2016). "Gaussian Error Linear Units (GELUs)"
   - Propuesta de GELU, usada en GPT y BERT
   - arXiv:1606.08415

5. **Klambauer, G. et al.** (2017). "Self-Normalizing Neural Networks"
   - Propuesta de SELU, una alternativa auto-normalizante
   - *Proceedings of NeurIPS*

### Recursos Online

1. **3Blue1Brown ‚Äî "Neural Networks" series (Cap√≠tulo 2)**
   - Gradiente descendente y backpropagation visualizados
   - https://www.youtube.com/watch?v=IHZwWFHWa-w

2. **CS231n: Activation Functions**
   - An√°lisis detallado de activaciones con visualizaciones
   - https://cs231n.github.io/neural-networks-1/#actfun

3. **Distill.pub ‚Äî "Visualizing Neural Networks"**
   - Art√≠culos interactivos de alta calidad
   - https://distill.pub

### Tutoriales Interactivos

1. **TensorFlow Playground**
   - Experimenta con activaciones en tiempo real
   - https://playground.tensorflow.org

2. **Seeing Theory ‚Äî Probability and Statistics**
   - Para entender la base estad√≠stica de las funciones de activaci√≥n
   - https://seeing-theory.brown.edu

### Documentaci√≥n T√©cnica

- **NumPy**: https://numpy.org/doc/stable/reference/ufuncs.html ‚Äî Operaciones elementwise
- **SciPy**: https://docs.scipy.org/doc/scipy/reference/special.html ‚Äî Funciones especiales (erf, erfcinv)
- **Python**: https://docs.python.org/3/library/math.html ‚Äî math.tanh, math.exp

---

## üéì Notas Finales

### Conceptos Clave para Recordar

1. **Sin no-linealidad = regresi√≥n lineal**
   - Cualquier red sin activaciones se reduce a $Y = XW + b$
   - Las activaciones son lo que diferencia el deep learning de la √°lgebra lineal

2. **ReLU: el est√°ndar moderno** ‚Äî `max(0, x)`
   - Extremadamente eficiente (solo una comparaci√≥n)
   - Derivada constante (1) en regi√≥n positiva ‚Üí sin vanishing gradient
   - Genera sparsity (neuronas inactivas ‚âà computaci√≥n gratuita)
   - Problema: neuronas muertas (solucionable con Leaky ReLU o He init)

3. **Sigmoid** ‚Äî `1/(1+e^(-x))`
   - Solo para la capa de salida de clasificaci√≥n binaria
   - Nunca en capas ocultas de redes profundas (vanishing gradient)
   - Salida en (0,1): directamente interpretable como probabilidad

4. **Softmax** ‚Äî normalizaci√≥n exponencial
   - Siempre en la capa de salida de clasificaci√≥n multiclase
   - Produce distribuci√≥n de probabilidad v√°lida (suma = 1)
   - Sensible a escala: usa estabilizaci√≥n num√©rica (restar m√°ximo)

5. **Tanh** ‚Äî versi√≥n sim√©trica de Sigmoid
   - Mejor que Sigmoid para capas ocultas (centrada en 0)
   - A√∫n sufre vanishing gradient en redes muy profundas
   - Preferida en RNNs y LSTMs

6. **Las derivadas importan**: deben ser correctas para que backpropagation funcione
   - Verifica siempre con `gradient_check` antes de entrenar

7. **La elecci√≥n de activaci√≥n afecta**:
   - Velocidad de convergencia
   - Estabilidad del entrenamiento
   - Capacidad representacional
   - Interpretabilidad de las salidas

8. **Temperatura en Softmax**: par√°metro clave en modelos generativos
   - T < 1: predicciones m√°s deterministas
   - T > 1: predicciones m√°s diversas

### Preparaci√≥n para el Siguiente Lab

**Lab 04: Funciones de P√©rdida** te ense√±ar√° c√≥mo medir el error de la red y c√≥mo usarlo para ajustar los par√°metros.

Aprender√°s:
- **MSE** (Mean Squared Error): para regresi√≥n
- **MAE** (Mean Absolute Error): m√°s robusto a outliers
- **Binary Cross-Entropy**: para clasificaci√≥n binaria (con Sigmoid)
- **Categorical Cross-Entropy**: para clasificaci√≥n multiclase (con Softmax)
- Por qu√© Sigmoid + Binary Cross-Entropy funcionan juntos naturalmente

**Para prepararte:**
1. Revisa logaritmos naturales y sus derivadas: $\frac{d}{dx}\ln(x) = \frac{1}{x}$
2. Piensa en qu√© significa "medir el error" entre probabilidades
3. Reflexiona: ¬øpor qu√© `argmax(softmax(x)) == argmax(x)`?
4. Investiga qu√© es "cross-entropy" en teor√≠a de la informaci√≥n

### Consejos de Estudio

1. **Verifica tus derivadas**: usa `gradient_check` siempre
2. **Visualiza todo**: grafica funciones y sus derivadas juntas
3. **Experimenta con temperatura**: observa c√≥mo cambia la distribuci√≥n
4. **Diagnostica activamente**: ejecuta `analizar_neuronas_muertas` en tus redes
5. **Lee c√≥digo de otros**: TensorFlow y PyTorch tienen implementaciones de referencia
6. **Comprende las discontinuidades**: ¬øen qu√© puntos ReLU no es diferenciable?
7. **Usa `np.clip` con sabidur√≠a**: evita overflow en Sigmoid con valores muy extremos

### Soluci√≥n de Problemas Comunes

**Problema: `RuntimeWarning: overflow encountered in exp`**
- **Causa**: Sigmoid aplicada a valores muy grandes (e.g., 1000)
- **Diagn√≥stico**: Verificar rango de valores de entrada: `print(x.min(), x.max())`
- **Soluci√≥n**: Usar la implementaci√≥n num√©ricamente estable con `np.where`

**Problema: Softmax devuelve `nan` o `inf`**
- **Causa**: Overflow en `np.exp(x)` para valores grandes
- **Diagn√≥stico**: `np.max(x)` es muy grande
- **Soluci√≥n**: Aplicar estabilizaci√≥n num√©rica: restar `np.max(x, axis=-1, keepdims=True)`

**Problema: Muchas neuronas muertas (activaci√≥n siempre 0)**
- **Causa**: Biases negativos grandes o learning rate muy alto
- **Diagn√≥stico**: `analizar_neuronas_muertas(red, X)`
- **Soluci√≥n 1**: Usar Leaky ReLU o ELU en lugar de ReLU
- **Soluci√≥n 2**: Reducir learning rate
- **Soluci√≥n 3**: Inicializaci√≥n He para biases positivos

**Problema: El gradient check falla para ReLU en x=0**
- **Causa**: ReLU no es diferenciable en x=0
- **Diagn√≥stico**: El punto de evaluaci√≥n est√° exactamente en 0
- **Soluci√≥n**: Es un comportamiento esperado; el gradient check es v√°lido para x ‚â† 0

**Problema: Entrenamiento muy lento (sospecha de vanishing gradient)**
- **Diagn√≥stico**: `demostrar_gradiente_desaparece(n_capas=len(red.capas)//2)`
- **Soluci√≥n 1**: Cambiar Sigmoid/Tanh por ReLU en capas ocultas
- **Soluci√≥n 2**: Reducir n√∫mero de capas
- **Soluci√≥n 3**: Usar t√©cnicas avanzadas: Batch Normalization, residual connections

### Comunidad y Soporte

- **Foro del curso**: Para preguntas conceptuales sobre activaciones
- **Horas de oficina**: Para revisi√≥n de implementaciones y gradient check
- **Papers with Code**: Implementaciones de referencia para todas las activaciones
  - https://paperswithcode.com/methods/category/activation-functions
- **Stack Overflow**: Para errores espec√≠ficos de NumPy/overflow

### Certificaci√≥n de Completitud

Has completado exitosamente el Lab 03 cuando puedas:

- [ ] Explicar intuitivamente por qu√© sin activaciones una red es lineal
- [ ] Implementar ReLU, Sigmoid, Tanh, Softmax y Leaky ReLU desde cero
- [ ] Verificar las derivadas con gradient check (error < 1e-5)
- [ ] Demostrar el vanishing gradient con Sigmoid en 10+ capas
- [ ] Identificar neuronas muertas en una red con ReLU
- [ ] Elegir la activaci√≥n correcta para la capa de salida seg√∫n el tipo de problema
- [ ] Integrar activaciones en la arquitectura modular del Lab 02
- [ ] Comparar la velocidad de ejecuci√≥n de diferentes activaciones
- [ ] Implementar Softmax num√©ricamente estable y explicar por qu√©

---

**¬°Felicitaciones por completar el Lab 03!** Ahora tus redes neuronales tienen la capacidad de aprender patrones no lineales complejos.

**Siguiente parada**: Lab 04 ‚Äî Funciones de P√©rdida üöÄ

---

*Versi√≥n: 2.0 | Actualizado: 2024 | Licencia: MIT ‚Äî Uso educativo*
