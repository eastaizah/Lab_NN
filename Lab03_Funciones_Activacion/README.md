# Lab 03: Funciones de Activaci贸n

## Objetivos de Aprendizaje

Al completar este laboratorio, ser谩s capaz de:

1. Comprender el rol de las funciones de activaci贸n en redes neuronales
2. Implementar desde cero las funciones de activaci贸n m谩s importantes
3. Calcular derivadas para backpropagation
4. Visualizar y comparar diferentes funciones de activaci贸n
5. Elegir la funci贸n de activaci贸n apropiada para diferentes problemas

## Estructura del Laboratorio

```
Lab03_Funciones_Activacion/
 README.md                 # Esta gu铆a
 teoria.md                 # Fundamentos te贸ricos
 practica.ipynb           # Notebook interactivo
 codigo/
     activaciones.py      # Implementaciones completas
```

## Requisitos Previos

- Completar Lab 01 y Lab 02
- Comprensi贸n b谩sica de derivadas
- Familiaridad con NumPy

## Contenido Te贸rico

El archivo `teoria.md` cubre:

- **Introducci贸n a funciones de activaci贸n**: Por qu茅 son necesarias
- **Sigmoid**: Ecuaci贸n, derivada, ventajas y desventajas
- **Tanh**: Funci贸n hiperb贸lica tangente
- **ReLU**: La funci贸n m谩s popular en deep learning
- **Leaky ReLU**: Soluci贸n al problema de neuronas muertas
- **Softmax**: Para clasificaci贸n multiclase
- **Comparaciones**: Cu谩ndo usar cada una
- **Problema del gradiente que desaparece**: C贸mo evitarlo

## Pr谩ctica

### Parte 1: Implementaci贸n B谩sica (30 min)

Implementa las funciones de activaci贸n desde cero:

```python
# Ejecutar el c贸digo principal
cd codigo/
python activaciones.py
```

Esto generar谩:
- Visualizaciones de funciones y derivadas
- Comparaci贸n de saturaci贸n de gradientes
- Ejemplos de Softmax
- Verificaci贸n de gradientes num茅ricos

### Parte 2: Notebook Interactivo (45 min)

Abre `practica.ipynb` y completa los ejercicios:

1. **Visualizaci贸n**: Graficar funciones de activaci贸n
2. **Derivadas**: Calcular y verificar derivadas
3. **Experimentos**: Comparar comportamiento en redes
4. **Casos de uso**: Ejercicios pr谩cticos

```bash
jupyter notebook practica.ipynb
```

### Parte 3: Experimentos (30 min)

1. **Experimento 1**: Comparar ReLU vs Sigmoid en una red profunda
2. **Experimento 2**: Observar el problema del gradiente que desaparece
3. **Experimento 3**: Evaluar el problema de neuronas muertas

## Conceptos Clave

### 1. No Linealidad

Sin funciones de activaci贸n, una red neuronal profunda es equivalente a una regresi贸n lineal:

```
Red sin activaci贸n:  y = W3 * W2 * W1 * x = W_combinado * x
Red con activaci贸n:  y = (W3 * (W2 * (W1 * x)))
```

### 2. Elecci贸n de Activaci贸n

| Capa | Problema | Funci贸n Recomendada |
|------|----------|---------------------|
| Oculta | General | ReLU |
| Oculta | Neuronas muertas | Leaky ReLU |
| Salida | Clasificaci贸n binaria | Sigmoid |
| Salida | Clasificaci贸n multiclase | Softmax |
| Salida | Regresi贸n | Lineal |

### 3. Gradientes

Las derivadas son cruciales para backpropagation:

```python
# ReLU es simple:
df/dx = 1 si x > 0, 0 si x <= 0

# Sigmoid es m谩s compleja:
df/dx = f(x) * (1 - f(x))
```

## Ejercicios

### Ejercicio 1: Implementar ELU

Implementa la funci贸n ELU (Exponential Linear Unit):

```python
ELU(x) = x si x > 0
       = 伪(e^x - 1) si x <= 0
```

### Ejercicio 2: An谩lisis de Saturaci贸n

Grafica las derivadas de Sigmoid y Tanh para x en [-10, 10]. 驴En qu茅 rangos se saturan?

### Ejercicio 3: Red con Diferentes Activaciones

Crea una red simple y entr茅nala con:
- Solo Sigmoid
- Solo ReLU
- Mezcla de ambas

Compara la velocidad de convergencia.

### Ejercicio 4: Softmax Temperature

Implementa Softmax con temperatura:

```python
Softmax(x/T) donde T es la temperatura
```

Observa c贸mo T afecta la distribuci贸n de probabilidades.

## Preguntas de Reflexi贸n

1. **驴Por qu茅 ReLU es tan efectiva a pesar de su simpleza?**
   
   Pista: Piensa en eficiencia computacional y gradientes.

2. **驴Cu谩ndo preferir铆as Sigmoid sobre ReLU?**
   
   Pista: Considera el tipo de problema y la capa.

3. **驴Qu茅 significa que una neurona "muera"?**
   
   Pista: Piensa en t茅rminos de gradientes.

4. **驴Por qu茅 Softmax suma 1?**
   
   Pista: Interpretaci贸n probabil铆stica.

## Verificaci贸n de Comprensi贸n

Despu茅s de completar el laboratorio, deber铆as poder:

- [ ] Explicar por qu茅 necesitamos funciones de activaci贸n
- [ ] Implementar ReLU, Sigmoid, Tanh y Softmax desde cero
- [ ] Calcular las derivadas de estas funciones
- [ ] Identificar cu谩ndo usar cada funci贸n
- [ ] Reconocer el problema del gradiente que desaparece
- [ ] Visualizar y comparar diferentes activaciones

## Recursos Adicionales

### Lecturas Recomendadas

1. **Paper original de ReLU**: "Rectified Linear Units Improve Restricted Boltzmann Machines" (Nair & Hinton, 2010)
2. **Understanding activations**: Deep Learning Book, Chapter 6
3. **Visualizaci贸n interactiva**: [playground.tensorflow.org](https://playground.tensorflow.org)

### Videos

- 3Blue1Brown: "But what is a neural network?" (visualizaci贸n excelente)
- Stanford CS231n: Lecture 6 (Training Neural Networks I)

### Herramientas

- [Neural Network Playground](https://playground.tensorflow.org)
- [Distill.pub - Activation Functions](https://distill.pub)

## Soluci贸n de Problemas

### Error: "RuntimeWarning: overflow encountered in exp"

**Causa**: Valores muy grandes en la exponencial de Sigmoid/Softmax

**Soluci贸n**: Usar estabilizaci贸n num茅rica:
```python
# En lugar de: exp(x)
# Usar: exp(x - max(x))
```

### Neuronas muertas en ReLU

**S铆ntoma**: Muchas salidas son cero

**Soluciones**:
1. Reducir learning rate
2. Usar Leaky ReLU
3. Verificar inicializaci贸n de pesos

### Gradientes que desaparecen

**S铆ntoma**: Red profunda no aprende en capas iniciales

**Soluciones**:
1. Cambiar de Sigmoid a ReLU
2. Usar batch normalization (en labs posteriores)
3. Reducir profundidad de la red

## Pr贸ximo Laboratorio

En **Lab 04: Funciones de P茅rdida**, aprenderemos:
- Mean Squared Error (MSE)
- Cross-Entropy Loss
- C贸mo combinar p茅rdida con activaci贸n
- Optimizaci贸n b谩sica

## Notas Finales

Las funciones de activaci贸n son fundamentales en deep learning. ReLU ha revolucionado el campo por su simplicidad y efectividad. Sin embargo, entender todas las opciones te permitir谩 tomar mejores decisiones arquitect贸nicas.

**Recuerda**: No hay una funci贸n "perfecta". La elecci贸n depende del problema, la arquitectura y la experimentaci贸n.

---

**驴Preguntas o problemas?** Revisa la teor铆a, experimenta con el c贸digo, y recuerda: la mejor forma de aprender es implementando desde cero.

**隆Buena suerte! **
