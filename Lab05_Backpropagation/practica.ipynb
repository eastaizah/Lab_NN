{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05: Backpropagation - Práctica\n",
    "\n",
    "## Objetivos\n",
    "1. Implementar backpropagation desde cero\n",
    "2. Verificar con gradientes numéricos\n",
    "3. Visualizar grafos computacionales\n",
    "4. Entrenar una red completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('codigo/')\n",
    "from backprop import *\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Grafo Computacional Simple\n",
    "\n",
    "Ejemplo: z = (x + y) * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizar_grafo_computacional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Verificación de Gradientes\n",
    "\n",
    "Comparar gradientes analíticos vs numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verificar_gradientes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Red Neuronal con Backprop\n",
    "\n",
    "Entrenar en problema XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenar_ejemplo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Implementación Manual\n",
    "\n",
    "Implementa tu propia versión paso a paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red simple: 2 → 3 → 1\n",
    "np.random.seed(42)\n",
    "\n",
    "# Datos XOR\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "# Inicializar pesos\n",
    "W1 = np.random.randn(3, 2) * 0.1\n",
    "b1 = np.zeros((3, 1))\n",
    "W2 = np.random.randn(1, 3) * 0.1\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "learning_rate = 0.5\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # FORWARD PASS\n",
    "    z1 = W1 @ X + b1\n",
    "    a1 = np.maximum(0, z1)  # ReLU\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = 1 / (1 + np.exp(-z2))  # Sigmoid\n",
    "    \n",
    "    # Loss\n",
    "    epsilon = 1e-15\n",
    "    loss = -np.mean(y * np.log(a2 + epsilon) + (1 - y) * np.log(1 - a2 + epsilon))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # BACKWARD PASS\n",
    "    # Gradiente de la pérdida\n",
    "    da2 = a2 - y\n",
    "    \n",
    "    # Capa 2\n",
    "    dz2 = da2 * a2 * (1 - a2)  # Derivada sigmoid\n",
    "    dW2 = dz2 @ a1.T / X.shape[1]\n",
    "    db2 = np.sum(dz2, axis=1, keepdims=True) / X.shape[1]\n",
    "    da1 = W2.T @ dz2\n",
    "    \n",
    "    # Capa 1\n",
    "    dz1 = da1 * (z1 > 0)  # Derivada ReLU\n",
    "    dW1 = dz1 @ X.T / X.shape[1]\n",
    "    db1 = np.sum(dz1, axis=1, keepdims=True) / X.shape[1]\n",
    "    \n",
    "    # UPDATE\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Época {epoch}: Loss = {loss:.6f}\")\n",
    "\n",
    "print(f\"\\nPredicciones finales: {a2}\")\n",
    "print(f\"Valores reales:       {y}\")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Entrenamiento Manual con Backpropagation')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "**Backpropagation** es:\n",
    "- Aplicación de la regla de la cadena\n",
    "- Eficiente (un pase calcula todos los gradientes)\n",
    "- Fundamental para deep learning\n",
    "\n",
    "**Proceso**:\n",
    "1. Forward: calcular predicciones (guardar intermedios)\n",
    "2. Loss: calcular error\n",
    "3. Backward: calcular gradientes (regla de la cadena)\n",
    "4. Update: ajustar parámetros\n",
    "\n",
    "**¡La magia del deep learning! ✨**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
