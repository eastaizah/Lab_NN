# Gu√≠a de Laboratorio: Backpropagation - El Coraz√≥n del Deep Learning

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Fundamentos de Deep Learning - Backpropagation  
**C√≥digo:** Lab 05  
**Duraci√≥n:** 2-3 horas  
**Nivel:** Intermedio  

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender profundamente la regla de la cadena y su aplicaci√≥n en deep learning
2. Interpretar y construir grafos computacionales para operaciones matem√°ticas
3. Implementar el algoritmo de backpropagation desde cero en Python
4. Calcular gradientes anal√≠ticos para funciones de activaci√≥n y p√©rdida
5. Verificar implementaciones mediante gradient checking (gradientes num√©ricos)
6. Identificar y guardar valores intermedios necesarios para backpropagation
7. Entrenar una red neuronal completa usando backpropagation
8. Diagnosticar problemas comunes: gradientes que explotan o desaparecen
9. Optimizar el c√≥digo usando vectorizaci√≥n y operaciones matriciales

## üìö Prerrequisitos

### Conocimientos

- Python intermedio (funciones, clases, NumPy)
- C√°lculo diferencial (derivadas, regla de la cadena)
- √Ålgebra lineal (multiplicaci√≥n matricial, transposici√≥n)
- Redes neuronales b√°sicas (forward pass, funciones de activaci√≥n)
- Funciones de p√©rdida (MSE, cross-entropy)

### Software

- Python 3.8+
- NumPy 1.19+
- Matplotlib (para visualizaciones)
- Jupyter Notebook (recomendado)

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` - Marco te√≥rico completo sobre backpropagation
- `README.md` - Estructura del laboratorio y recursos disponibles
- Revisar Labs anteriores (01-04) sobre neuronas, funciones de activaci√≥n y p√©rdida

## üìñ Introducci√≥n

### El Problema Fundamental del Aprendizaje

Hasta ahora hemos visto c√≥mo las redes neuronales hacen predicciones mediante el **forward pass**: los datos fluyen de entrada a salida a trav√©s de capas de neuronas. Pero, ¬øc√≥mo aprende la red? ¬øC√≥mo ajustamos los millones de par√°metros (pesos y bias) para mejorar las predicciones?

La respuesta es **Backpropagation** - el algoritmo que revolucion√≥ el deep learning.

### ¬øQu√© es Backpropagation?

**Backpropagation** (propagaci√≥n hacia atr√°s) es un algoritmo eficiente para calcular gradientes de la funci√≥n de p√©rdida con respecto a todos los par√°metros de una red neuronal. Es la aplicaci√≥n inteligente de la **regla de la cadena del c√°lculo diferencial**.

```
ENTRENAMIENTO DE UNA RED NEURONAL:

1. Forward Pass:
   Entrada ‚Üí Capa1 ‚Üí Capa2 ‚Üí ... ‚Üí Salida ‚Üí P√©rdida
   
2. Backward Pass (Backpropagation):
   ‚àÇL/‚àÇW‚ÇÅ ‚Üê ‚àÇL/‚àÇW‚ÇÇ ‚Üê ... ‚Üê ‚àÇL/‚àÇW‚Çô ‚Üê Gradiente de P√©rdida
   
3. Actualizaci√≥n de Par√°metros:
   W_nuevo = W_viejo - Œ± * ‚àÇL/‚àÇW
```

### Contexto Hist√≥rico

Aunque las redes neuronales fueron propuestas en los a√±os 40-50, su verdadero potencial no se liber√≥ hasta que backpropagation fue popularizado en 1986 por Rumelhart, Hinton y Williams. Este algoritmo:

- Hace posible entrenar redes con m√∫ltiples capas ocultas
- Calcula gradientes de manera eficiente (complejidad lineal)
- Es la base de todas las redes neuronales modernas
- Permite el aprendizaje autom√°tico de representaciones complejas

### La Regla de la Cadena: Fundamento Matem√°tico

Backpropagation se basa en una idea simple del c√°lculo: **la regla de la cadena**.

**Para funciones compuestas:**
Si tenemos `y = f(g(x))`, entonces:
$$\frac{dy}{dx} = \frac{dy}{dg} \times \frac{dg}{dx}$$

**Ejemplo intuitivo:**

Imagina que conduces un auto:
- La distancia depende de la velocidad: `d = v * t`
- La velocidad depende del acelerador: `v = k * a`
- Por tanto: `d = (k * a) * t`

¬øC√≥mo afecta el acelerador a la distancia? Usando la regla de la cadena:
$$\frac{‚àÇd}{‚àÇa} = \frac{‚àÇd}{‚àÇv} \times \frac{‚àÇv}{‚àÇa} = t \times k$$

En redes neuronales, la "distancia" es la p√©rdida, y el "acelerador" son los pesos.

### Grafos Computacionales

Una forma poderosa de entender backpropagation es mediante **grafos computacionales** - diagramas que muestran c√≥mo se calculan las salidas a partir de las entradas.

**Ejemplo simple: z = (x + y) √ó w**

```
    x ‚îÄ‚îÄ‚îÄ‚îê
         ‚îú‚îÄ‚îÄ‚Üí [+] ‚îÄ‚îÄ‚Üí q ‚îÄ‚îÄ‚îÄ‚îê
    y ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
                           ‚îú‚îÄ‚îÄ‚Üí [√ó] ‚îÄ‚îÄ‚Üí z
                       w ‚îÄ‚îÄ‚îÄ‚îò
```

**Forward pass** (izquierda ‚Üí derecha): Calcular z
- `q = x + y`
- `z = q √ó w`

**Backward pass** (derecha ‚Üê izquierda): Calcular gradientes
- `‚àÇz/‚àÇz = 1` (empezamos aqu√≠)
- `‚àÇz/‚àÇq = w`, `‚àÇz/‚àÇw = q`
- `‚àÇz/‚àÇx = (‚àÇz/‚àÇq) √ó (‚àÇq/‚àÇx) = w √ó 1 = w`
- `‚àÇz/‚àÇy = (‚àÇz/‚àÇq) √ó (‚àÇq/‚àÇy) = w √ó 1 = w`

### ¬øPor qu√© es Tan Importante?

**Sin backpropagation:**
- Calcular gradientes manualmente es tedioso y propenso a errores
- Complejidad computacional prohibitiva para redes grandes
- Imposible entrenar redes profundas eficientemente

**Con backpropagation:**
- C√°lculo autom√°tico de todos los gradientes
- Un solo pase hacia atr√°s calcula todos los gradientes necesarios
- Complejidad lineal O(n) donde n es el n√∫mero de par√°metros
- Permite entrenar redes con millones de par√°metros

### Aplicaciones Pr√°cticas

Backpropagation es el motor detr√°s de:
- **Visi√≥n por Computadora**: Redes que reconocen objetos, rostros, escenas
- **PLN**: Modelos de lenguaje como GPT, BERT, traducci√≥n autom√°tica
- **Generaci√≥n**: GANs que crean im√°genes realistas, m√∫sica, texto
- **Juegos**: AlphaGo, agentes que aprenden a jugar videojuegos
- **Ciencia**: Predicci√≥n de estructuras de prote√≠nas, simulaciones f√≠sicas

Todos estos avances ser√≠an imposibles sin backpropagation.

## ü§î Preguntas de Reflexi√≥n Iniciales

Antes de comenzar, reflexiona sobre estas preguntas:

1. ¬øPor qu√© necesitamos calcular gradientes para entrenar una red neuronal?
2. ¬øQu√© significa "eficiencia computacional" en el contexto de c√°lculo de gradientes?
3. Si una red tiene 1 mill√≥n de par√°metros, ¬øcu√°ntas derivadas parciales necesitamos calcular?
4. ¬øC√≥mo podr√≠amos verificar que nuestro c√°lculo de gradientes es correcto?
5. ¬øQu√© informaci√≥n del forward pass necesitamos guardar para backpropagation?

## üî¨ Parte 1: Regla de la Cadena y Grafos Computacionales (40 min)

### 1.1 Repaso de la Regla de la Cadena

Comencemos con ejemplos matem√°ticos simples antes de aplicarlo a redes neuronales.

**Ejemplo 1: Funci√≥n compuesta simple**

```python
import numpy as np

# Funci√≥n: y = (3x + 2)¬≤
# Queremos: dy/dx

def forward_example1(x):
    """Forward pass: calcular y"""
    u = 3 * x + 2
    y = u ** 2
    return y, u  # Guardamos u para backprop

def backward_example1(x):
    """Backward pass: calcular dy/dx"""
    # Forward (calcular y guardar valores intermedios)
    u = 3 * x + 2
    y = u ** 2
    
    # Backward (aplicar regla de la cadena)
    dy_du = 2 * u      # ‚àÇy/‚àÇu = 2u
    du_dx = 3          # ‚àÇu/‚àÇx = 3
    dy_dx = dy_du * du_dx  # Regla de la cadena
    
    return dy_dx

# Probar
x = 5.0
y, u = forward_example1(x)
print(f"x = {x}")
print(f"u = 3x + 2 = {u}")
print(f"y = u¬≤ = {y}")

gradient = backward_example1(x)
print(f"dy/dx = {gradient}")
# Resultado: dy/dx = 2(3x + 2) * 3 = 6(17) = 102
```

**Actividad 1.1:** Implementa y verifica el gradiente de `y = sin(2x + 1)`

**Ejemplo 2: M√∫ltiples variables**

```python
# Funci√≥n: z = x¬≤ + y¬≤ + 2xy
# Queremos: ‚àÇz/‚àÇx, ‚àÇz/‚àÇy

def forward_example2(x, y):
    """Forward pass"""
    z = x**2 + y**2 + 2*x*y
    return z

def backward_example2(x, y):
    """Backward pass"""
    # Derivadas parciales
    dz_dx = 2*x + 2*y  # ‚àÇz/‚àÇx = 2x + 2y
    dz_dy = 2*y + 2*x  # ‚àÇz/‚àÇy = 2y + 2x
    
    return dz_dx, dz_dy

# Probar
x, y = 3.0, 4.0
z = forward_example2(x, y)
dz_dx, dz_dy = backward_example2(x, y)

print(f"z({x}, {y}) = {z}")
print(f"‚àÇz/‚àÇx = {dz_dx}")
print(f"‚àÇz/‚àÇy = {dz_dy}")
```

### 1.2 Grafos Computacionales

Los grafos computacionales son herramientas visuales poderosas para entender backpropagation.

**Ejemplo: z = (x + y) √ó w**

```python
class ComputationNode:
    """Nodo en un grafo computacional"""
    def __init__(self, name):
        self.name = name
        self.value = None
        self.grad = 0
    
    def __repr__(self):
        return f"{self.name}={self.value:.4f}, grad={self.grad:.4f}"

def forward_graph_example():
    """Forward pass con grafo computacional"""
    # Crear nodos
    x = ComputationNode('x')
    y = ComputationNode('y')
    w = ComputationNode('w')
    q = ComputationNode('q')  # q = x + y
    z = ComputationNode('z')  # z = q * w
    
    # Valores de entrada
    x.value = 2.0
    y.value = 3.0
    w.value = 4.0
    
    # Forward pass
    q.value = x.value + y.value  # q = 5.0
    z.value = q.value * w.value  # z = 20.0
    
    return x, y, w, q, z

def backward_graph_example(x, y, w, q, z):
    """Backward pass con grafo computacional"""
    # Inicializar gradiente de salida
    z.grad = 1.0  # dL/dz = 1 (asumimos L = z)
    
    # Backward pass (en orden inverso)
    # z = q * w
    q.grad += z.grad * w.value  # ‚àÇz/‚àÇq = w
    w.grad += z.grad * q.value  # ‚àÇz/‚àÇw = q
    
    # q = x + y
    x.grad += q.grad * 1.0  # ‚àÇq/‚àÇx = 1
    y.grad += q.grad * 1.0  # ‚àÇq/‚àÇy = 1

# Ejecutar
x, y, w, q, z = forward_graph_example()
print("Forward pass:")
print(x, y, w, q, z)

backward_graph_example(x, y, w, q, z)
print("\nBackward pass:")
print(x, y, w, q, z)
```

**Actividad 1.2:** Dibuja el grafo computacional para `f = (x + y) √ó (x - y)` y calcula todos los gradientes.

### 1.3 Operaciones B√°sicas y sus Gradientes

Tabla de referencia para operaciones comunes:

```python
class GradientOperations:
    """Colecci√≥n de operaciones con sus gradientes"""
    
    @staticmethod
    def add_forward(x, y):
        return x + y
    
    @staticmethod
    def add_backward(dout, x, y):
        """‚àÇ(x+y)/‚àÇx = 1, ‚àÇ(x+y)/‚àÇy = 1"""
        dx = dout * 1
        dy = dout * 1
        return dx, dy
    
    @staticmethod
    def mul_forward(x, y):
        return x * y
    
    @staticmethod
    def mul_backward(dout, x, y):
        """‚àÇ(x*y)/‚àÇx = y, ‚àÇ(x*y)/‚àÇy = x"""
        dx = dout * y
        dy = dout * x
        return dx, dy
    
    @staticmethod
    def square_forward(x):
        return x ** 2
    
    @staticmethod
    def square_backward(dout, x):
        """‚àÇ(x¬≤)/‚àÇx = 2x"""
        dx = dout * 2 * x
        return dx
    
    @staticmethod
    def exp_forward(x):
        return np.exp(x)
    
    @staticmethod
    def exp_backward(dout, x):
        """‚àÇ(e^x)/‚àÇx = e^x"""
        dx = dout * np.exp(x)
        return dx

# Ejemplo de uso
ops = GradientOperations()

# Forward
x, y = 3.0, 4.0
z = ops.mul_forward(x, y)  # z = 12
print(f"z = {z}")

# Backward (asumiendo dL/dz = 1)
dout = 1.0
dx, dy = ops.mul_backward(dout, x, y)
print(f"‚àÇz/‚àÇx = {dx}, ‚àÇz/‚àÇy = {dy}")  # ‚àÇz/‚àÇx = 4, ‚àÇz/‚àÇy = 3
```

**Actividad 1.3:** Implementa gradientes para divisi√≥n, exponencial y logaritmo.

## üî¨ Parte 2: Backpropagation en una Neurona (45 min)

### 2.1 Anatom√≠a de una Neurona con Backpropagation

Implementemos una neurona que puede hacer forward y backward pass.

```python
class NeuronWithBackprop:
    """Neurona con capacidad de backpropagation"""
    
    def __init__(self, n_inputs):
        """Inicializar pesos y bias aleatoriamente"""
        self.w = np.random.randn(n_inputs) * 0.1
        self.b = np.random.randn() * 0.1
        
        # Cache para backpropagation
        self.cache = {}
        
        # Gradientes
        self.dw = np.zeros_like(self.w)
        self.db = 0
    
    def forward(self, x):
        """
        Forward pass: z = w¬∑x + b
        Guardamos x para usarlo en backprop
        """
        self.cache['x'] = x
        z = np.dot(self.w, x) + self.b
        self.cache['z'] = z
        return z
    
    def backward(self, dz):
        """
        Backward pass
        Entrada: dz = ‚àÇL/‚àÇz (gradiente de la p√©rdida respecto a z)
        Salida: dx = ‚àÇL/‚àÇx (gradiente para propagar hacia atr√°s)
        
        Derivadas:
        ‚àÇz/‚àÇw = x  ‚Üí  ‚àÇL/‚àÇw = ‚àÇL/‚àÇz * ‚àÇz/‚àÇw = dz * x
        ‚àÇz/‚àÇb = 1  ‚Üí  ‚àÇL/‚àÇb = ‚àÇL/‚àÇz * ‚àÇz/‚àÇb = dz * 1
        ‚àÇz/‚àÇx = w  ‚Üí  ‚àÇL/‚àÇx = ‚àÇL/‚àÇz * ‚àÇz/‚àÇx = dz * w
        """
        x = self.cache['x']
        
        # Calcular gradientes
        self.dw = dz * x  # ‚àÇL/‚àÇw
        self.db = dz      # ‚àÇL/‚àÇb
        dx = dz * self.w  # ‚àÇL/‚àÇx (para propagar)
        
        return dx
    
    def update(self, learning_rate=0.01):
        """Actualizar par√°metros usando gradient descent"""
        self.w -= learning_rate * self.dw
        self.b -= learning_rate * self.db

# Ejemplo de uso
neuron = NeuronWithBackprop(n_inputs=3)

# Forward pass
x = np.array([1.0, 2.0, 3.0])
z = neuron.forward(x)
print(f"Salida de la neurona: {z}")

# Backward pass (simulando dL/dz = 1)
dz = 1.0
dx = neuron.backward(dz)

print(f"Gradientes:")
print(f"  dL/dw = {neuron.dw}")
print(f"  dL/db = {neuron.db}")
print(f"  dL/dx = {dx}")

# Actualizar par√°metros
neuron.update(learning_rate=0.1)
print(f"\nPesos actualizados: {neuron.w}")
print(f"Bias actualizado: {neuron.b}")
```

### 2.2 Neurona con Funci√≥n de Activaci√≥n

Agreguemos una funci√≥n de activaci√≥n (ReLU):

```python
class NeuronWithActivation:
    """Neurona con funci√≥n de activaci√≥n y backprop"""
    
    def __init__(self, n_inputs):
        self.w = np.random.randn(n_inputs) * 0.1
        self.b = np.random.randn() * 0.1
        self.cache = {}
        self.dw = np.zeros_like(self.w)
        self.db = 0
    
    def relu(self, z):
        """ReLU: max(0, z)"""
        return np.maximum(0, z)
    
    def relu_derivative(self, z):
        """Derivada de ReLU: 1 si z > 0, else 0"""
        return (z > 0).astype(float)
    
    def forward(self, x):
        """Forward pass: a = ReLU(w¬∑x + b)"""
        self.cache['x'] = x
        
        # Suma ponderada
        z = np.dot(self.w, x) + self.b
        self.cache['z'] = z
        
        # Activaci√≥n
        a = self.relu(z)
        self.cache['a'] = a
        
        return a
    
    def backward(self, da):
        """
        Backward pass con activaci√≥n
        
        Entrada: da = ‚àÇL/‚àÇa
        
        Pasos:
        1. dz = da * ReLU'(z)  (gradiente local de ReLU)
        2. dw = dz * x
        3. db = dz
        4. dx = dz * w
        """
        x = self.cache['x']
        z = self.cache['z']
        
        # Gradiente a trav√©s de ReLU
        dz = da * self.relu_derivative(z)
        
        # Gradientes de par√°metros
        self.dw = dz * x
        self.db = dz
        
        # Gradiente para propagar
        dx = dz * self.w
        
        return dx
    
    def update(self, lr=0.01):
        self.w -= lr * self.dw
        self.b -= lr * self.db

# Ejemplo
neuron = NeuronWithActivation(n_inputs=3)

# Forward
x = np.array([1.0, -2.0, 3.0])
a = neuron.forward(x)
print(f"Input: {x}")
print(f"z (before ReLU): {neuron.cache['z']}")
print(f"a (after ReLU): {a}")

# Backward
da = 1.0  # Gradiente de entrada
dx = neuron.backward(da)

print(f"\nGradientes:")
print(f"  dL/dw = {neuron.dw}")
print(f"  dL/db = {neuron.db}")
print(f"  dL/dx = {dx}")
```

**Actividad 2.1:** Implementa `NeuronWithSigmoid` que use funci√≥n sigmoid en lugar de ReLU.

### 2.3 Ejemplo Completo: Entrenar una Neurona

Entrenemos una neurona para aprender la funci√≥n AND:

```python
# Datos: AND l√≥gico
X_train = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

y_train = np.array([0, 0, 0, 1])  # AND

# Crear neurona
neuron = NeuronWithActivation(n_inputs=2)

# Funci√≥n de p√©rdida MSE
def mse_loss(pred, target):
    return 0.5 * (pred - target) ** 2

def mse_derivative(pred, target):
    return pred - target

# Entrenar
learning_rate = 0.1
epochs = 1000

for epoch in range(epochs):
    total_loss = 0
    
    for x, y_true in zip(X_train, y_train):
        # Forward
        y_pred = neuron.forward(x)
        loss = mse_loss(y_pred, y_true)
        total_loss += loss
        
        # Backward
        dloss = mse_derivative(y_pred, y_true)
        neuron.backward(dloss)
        
        # Update
        neuron.update(learning_rate)
    
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {total_loss:.4f}")

# Evaluar
print("\nResultados finales:")
for x, y_true in zip(X_train, y_train):
    y_pred = neuron.forward(x)
    print(f"Input: {x}, Predicci√≥n: {y_pred:.4f}, Real: {y_true}")
```

**Actividad 2.2:** Entrena una neurona para aprender OR y XOR. ¬øQu√© observas con XOR?

## üî¨ Parte 3: Backpropagation en Redes Multicapa (60 min)

### 3.1 Red de 2 Capas con Backpropagation

Implementemos una red completa con dos capas:

```python
class Layer:
    """Capa densa con backpropagation"""
    
    def __init__(self, n_inputs, n_neurons, activation='relu'):
        # Inicializaci√≥n He para ReLU
        self.W = np.random.randn(n_inputs, n_neurons) * np.sqrt(2.0 / n_inputs)
        self.b = np.zeros((1, n_neurons))
        self.activation = activation
        self.cache = {}
        
    def forward(self, X):
        """
        Forward pass
        X: (batch_size, n_inputs)
        Salida: (batch_size, n_neurons)
        """
        self.cache['X'] = X
        
        # Z = X @ W + b
        Z = np.dot(X, self.W) + self.b
        self.cache['Z'] = Z
        
        # Activaci√≥n
        if self.activation == 'relu':
            A = np.maximum(0, Z)
        elif self.activation == 'sigmoid':
            A = 1 / (1 + np.exp(-Z))
        elif self.activation == 'linear':
            A = Z
        
        self.cache['A'] = A
        return A
    
    def backward(self, dA):
        """
        Backward pass
        dA: gradiente de la p√©rdida respecto a A (salida de esta capa)
        Retorna: dX (gradiente para propagar a capa anterior)
        """
        X = self.cache['X']
        Z = self.cache['Z']
        m = X.shape[0]  # batch size
        
        # Gradiente de la activaci√≥n
        if self.activation == 'relu':
            dZ = dA * (Z > 0)
        elif self.activation == 'sigmoid':
            A = self.cache['A']
            dZ = dA * A * (1 - A)
        elif self.activation == 'linear':
            dZ = dA
        
        # Gradientes de par√°metros
        self.dW = (1/m) * np.dot(X.T, dZ)
        self.db = (1/m) * np.sum(dZ, axis=0, keepdims=True)
        
        # Gradiente para propagar
        dX = np.dot(dZ, self.W.T)
        
        return dX
    
    def update(self, lr):
        """Actualizar par√°metros"""
        self.W -= lr * self.dW
        self.b -= lr * self.db


class TwoLayerNetwork:
    """Red neuronal de 2 capas con backpropagation"""
    
    def __init__(self, n_inputs, n_hidden, n_outputs):
        self.layer1 = Layer(n_inputs, n_hidden, activation='relu')
        self.layer2 = Layer(n_hidden, n_outputs, activation='sigmoid')
    
    def forward(self, X):
        """Forward pass a trav√©s de ambas capas"""
        A1 = self.layer1.forward(X)
        A2 = self.layer2.forward(A1)
        return A2
    
    def backward(self, dA2):
        """Backward pass a trav√©s de ambas capas"""
        dA1 = self.layer2.backward(dA2)
        dX = self.layer1.backward(dA1)
        return dX
    
    def update(self, lr):
        """Actualizar par√°metros de ambas capas"""
        self.layer1.update(lr)
        self.layer2.update(lr)
    
    def train_step(self, X, y, lr=0.01):
        """Un paso de entrenamiento completo"""
        # Forward
        y_pred = self.forward(X)
        
        # Calcular p√©rdida (Binary Cross-Entropy)
        m = y.shape[0]
        loss = -np.mean(y * np.log(y_pred + 1e-8) + (1-y) * np.log(1-y_pred + 1e-8))
        
        # Backward
        dA2 = y_pred - y  # Gradiente de BCE con sigmoid
        self.backward(dA2)
        
        # Update
        self.update(lr)
        
        return loss

# Ejemplo: Entrenar en XOR (¬°ahora s√≠ funciona!)
X_train = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])
y_train = np.array([[0], [1], [1], [0]])  # XOR

# Crear red
net = TwoLayerNetwork(n_inputs=2, n_hidden=4, n_outputs=1)

# Entrenar
print("Entrenando en XOR...")
for epoch in range(5000):
    loss = net.train_step(X_train, y_train, lr=0.5)
    
    if epoch % 500 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# Evaluar
print("\nResultados finales:")
predictions = net.forward(X_train)
for i, (x, y_true, y_pred) in enumerate(zip(X_train, y_train, predictions)):
    print(f"Input: {x}, Predicci√≥n: {y_pred[0]:.4f}, Real: {y_true[0]}")
```

### 3.2 Visualizaci√≥n de Gradientes

Es √∫til visualizar c√≥mo fluyen los gradientes:

```python
import matplotlib.pyplot as plt

def visualize_gradients(network, X, y):
    """Visualiza magnitudes de gradientes en cada capa"""
    # Forward
    y_pred = network.forward(X)
    
    # Backward
    dA2 = y_pred - y
    network.backward(dA2)
    
    # Recopilar magnitudes de gradientes
    grad_layer1 = np.mean(np.abs(network.layer1.dW))
    grad_layer2 = np.mean(np.abs(network.layer2.dW))
    
    # Graficar
    layers = ['Layer 1', 'Layer 2']
    gradients = [grad_layer1, grad_layer2]
    
    plt.figure(figsize=(10, 5))
    plt.bar(layers, gradients, color=['blue', 'red'])
    plt.ylabel('Magnitud Promedio del Gradiente')
    plt.title('Flujo de Gradientes en la Red')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    print(f"Gradiente capa 1: {grad_layer1:.6f}")
    print(f"Gradiente capa 2: {grad_layer2:.6f}")

# Usar
visualize_gradients(net, X_train, y_train)
```

**Actividad 3.1:** Crea una red de 3 capas y entr√©nala en un dataset de clasificaci√≥n simple.

## üî¨ Parte 4: Verificaci√≥n de Gradientes (30 min)

### 4.1 Gradient Checking

La verificaci√≥n num√©rica de gradientes es CRUCIAL para asegurar que backpropagation est√© implementado correctamente.

```python
def numerical_gradient(f, x, epsilon=1e-5):
    """
    Calcula gradiente num√©rico usando diferencias finitas
    
    f: funci√≥n que toma x y retorna un escalar
    x: punto donde calcular el gradiente
    epsilon: peque√±o valor para la diferencia finita
    """
    grad = np.zeros_like(x)
    
    # Iterar sobre cada dimensi√≥n
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    
    while not it.finished:
        idx = it.multi_index
        old_value = x[idx]
        
        # f(x + epsilon)
        x[idx] = old_value + epsilon
        fxplus = f(x)
        
        # f(x - epsilon)
        x[idx] = old_value - epsilon
        fxminus = f(x)
        
        # Gradiente num√©rico
        grad[idx] = (fxplus - fxminus) / (2 * epsilon)
        
        # Restaurar valor
        x[idx] = old_value
        it.iternext()
    
    return grad

def gradient_check(network, X, y, epsilon=1e-5):
    """
    Verifica que los gradientes anal√≠ticos coincidan con los num√©ricos
    """
    # Forward y backward para obtener gradientes anal√≠ticos
    y_pred = network.forward(X)
    loss_initial = -np.mean(y * np.log(y_pred + 1e-8) + (1-y) * np.log(1-y_pred + 1e-8))
    
    dA2 = y_pred - y
    network.backward(dA2)
    
    # Gradientes anal√≠ticos
    analytical_dW1 = network.layer1.dW.copy()
    analytical_dW2 = network.layer2.dW.copy()
    
    # Funci√≥n de p√©rdida para gradient checking
    def loss_function(params):
        # Desempaquetar par√°metros
        W1, b1, W2, b2 = params
        
        # Forward temporal
        A1 = np.maximum(0, np.dot(X, W1) + b1)
        A2 = 1 / (1 + np.exp(-(np.dot(A1, W2) + b2)))
        
        # P√©rdida
        loss = -np.mean(y * np.log(A2 + 1e-8) + (1-y) * np.log(1-A2 + 1e-8))
        return loss
    
    # Calcular gradiente num√©rico solo para W1 (por simplicidad)
    print("Verificando gradientes de W1...")
    
    numerical_dW1 = np.zeros_like(network.layer1.W)
    
    for i in range(network.layer1.W.shape[0]):
        for j in range(network.layer1.W.shape[1]):
            # Perturbar W1[i,j]
            network.layer1.W[i, j] += epsilon
            loss_plus = loss_function([network.layer1.W, network.layer1.b, 
                                      network.layer2.W, network.layer2.b])
            
            network.layer1.W[i, j] -= 2 * epsilon
            loss_minus = loss_function([network.layer1.W, network.layer1.b,
                                       network.layer2.W, network.layer2.b])
            
            # Gradiente num√©rico
            numerical_dW1[i, j] = (loss_plus - loss_minus) / (2 * epsilon)
            
            # Restaurar
            network.layer1.W[i, j] += epsilon
    
    # Comparar
    diff = np.linalg.norm(analytical_dW1 - numerical_dW1) / (
           np.linalg.norm(analytical_dW1) + np.linalg.norm(numerical_dW1))
    
    print(f"\nDiferencia relativa: {diff:.10f}")
    
    if diff < 1e-7:
        print("‚úì ¬°Gradientes correctos!")
    elif diff < 1e-5:
        print("‚ö† Gradientes probablemente correctos (diferencia peque√±a)")
    else:
        print("‚úó ERROR: Gradientes incorrectos")
    
    return diff

# Ejecutar gradient checking
print("=== GRADIENT CHECKING ===")
difference = gradient_check(net, X_train, y_train)
```

### 4.2 Consejos para Debugging

```python
def debug_backprop(network, X, y):
    """Herramienta de debugging para backpropagation"""
    
    print("=== DEBUG BACKPROPAGATION ===\n")
    
    # Forward
    print("1. FORWARD PASS")
    A1 = network.layer1.forward(X)
    A2 = network.layer2.forward(A1)
    print(f"   Salida capa 1: shape={A1.shape}, min={A1.min():.4f}, max={A1.max():.4f}")
    print(f"   Salida capa 2: shape={A2.shape}, min={A2.min():.4f}, max={A2.max():.4f}")
    
    # P√©rdida
    loss = -np.mean(y * np.log(A2 + 1e-8) + (1-y) * np.log(1-A2 + 1e-8))
    print(f"   P√©rdida: {loss:.6f}")
    
    # Backward
    print("\n2. BACKWARD PASS")
    dA2 = A2 - y
    print(f"   Gradiente inicial (dA2): shape={dA2.shape}, mean={np.mean(np.abs(dA2)):.6f}")
    
    dA1 = network.layer2.backward(dA2)
    print(f"   Gradiente capa 2 -> 1 (dA1): shape={dA1.shape}, mean={np.mean(np.abs(dA1)):.6f}")
    print(f"   Gradiente W2: mean={np.mean(np.abs(network.layer2.dW)):.6f}")
    
    dX = network.layer1.backward(dA1)
    print(f"   Gradiente capa 1 -> entrada: mean={np.mean(np.abs(dX)):.6f}")
    print(f"   Gradiente W1: mean={np.mean(np.abs(network.layer1.dW)):.6f}")
    
    # Verificar NaN o Inf
    print("\n3. VERIFICACIONES")
    has_nan = np.isnan(network.layer1.dW).any() or np.isnan(network.layer2.dW).any()
    has_inf = np.isinf(network.layer1.dW).any() or np.isinf(network.layer2.dW).any()
    
    if has_nan:
        print("   ‚úó ¬°ADVERTENCIA! Gradientes contienen NaN")
    if has_inf:
        print("   ‚úó ¬°ADVERTENCIA! Gradientes contienen Inf")
    if not has_nan and not has_inf:
        print("   ‚úì Gradientes son valores num√©ricos v√°lidos")

# Usar
debug_backprop(net, X_train, y_train)
```

**Actividad 4.1:** Introduce un bug intencional en tu c√≥digo de backpropagation y usa gradient checking para encontrarlo.

## üìä An√°lisis Final de Rendimiento

### Comparaci√≥n: Antes vs Despu√©s de Backpropagation

```python
# Sin entrenar (pesos aleatorios)
net_untrained = TwoLayerNetwork(n_inputs=2, n_hidden=4, n_outputs=1)
pred_before = net_untrained.forward(X_train)

# Entrenar
net_trained = TwoLayerNetwork(n_inputs=2, n_hidden=4, n_outputs=1)
losses = []

for epoch in range(5000):
    loss = net_trained.train_step(X_train, y_train, lr=0.5)
    losses.append(loss)

pred_after = net_trained.forward(X_train)

# Comparar
print("=== ANTES DEL ENTRENAMIENTO ===")
for i, (x, y_true, y_pred) in enumerate(zip(X_train, y_train, pred_before)):
    print(f"Input: {x}, Pred: {y_pred[0]:.4f}, Real: {y_true[0]}")

print("\n=== DESPU√âS DEL ENTRENAMIENTO ===")
for i, (x, y_true, y_pred) in enumerate(zip(X_train, y_train, pred_after)):
    print(f"Input: {x}, Pred: {y_pred[0]:.4f}, Real: {y_true[0]}")

# Graficar curva de aprendizaje
plt.figure(figsize=(10, 5))
plt.plot(losses)
plt.xlabel('√âpoca')
plt.ylabel('P√©rdida')
plt.title('Curva de Aprendizaje - Backpropagation en Acci√≥n')
plt.grid(True, alpha=0.3)
plt.show()

print(f"\nP√©rdida inicial: {losses[0]:.6f}")
print(f"P√©rdida final: {losses[-1]:.6f}")
print(f"Mejora: {(1 - losses[-1]/losses[0]) * 100:.2f}%")
```

### Problemas Comunes y Soluciones

```python
class BackpropDiagnostics:
    """Herramientas para diagnosticar problemas en backpropagation"""
    
    @staticmethod
    def check_vanishing_gradients(network, threshold=1e-7):
        """Detecta gradientes que desaparecen"""
        grad1 = np.mean(np.abs(network.layer1.dW))
        grad2 = np.mean(np.abs(network.layer2.dW))
        
        print("=== DIAGN√ìSTICO: VANISHING GRADIENTS ===")
        print(f"Gradiente promedio capa 1: {grad1:.10f}")
        print(f"Gradiente promedio capa 2: {grad2:.10f}")
        
        if grad1 < threshold or grad2 < threshold:
            print("‚ö† ¬°ADVERTENCIA! Posible vanishing gradients")
            print("Soluciones:")
            print("  - Usar ReLU en lugar de sigmoid/tanh")
            print("  - Reducir n√∫mero de capas")
            print("  - Usar batch normalization")
            print("  - Mejor inicializaci√≥n de pesos")
        else:
            print("‚úì Gradientes en rango saludable")
    
    @staticmethod
    def check_exploding_gradients(network, threshold=1.0):
        """Detecta gradientes que explotan"""
        grad1 = np.mean(np.abs(network.layer1.dW))
        grad2 = np.mean(np.abs(network.layer2.dW))
        
        print("\n=== DIAGN√ìSTICO: EXPLODING GRADIENTS ===")
        
        if grad1 > threshold or grad2 > threshold:
            print("‚ö† ¬°ADVERTENCIA! Posible exploding gradients")
            print("Soluciones:")
            print("  - Reducir learning rate")
            print("  - Usar gradient clipping")
            print("  - Mejor inicializaci√≥n de pesos")
        else:
            print("‚úì Gradientes bajo control")

# Usar diagn√≥sticos
diag = BackpropDiagnostics()
diag.check_vanishing_gradients(net_trained)
diag.check_exploding_gradients(net_trained)
```

## üéØ EJERCICIOS PROPUESTOS

### Nivel B√°sico

**Ejercicio 1:** Implementaci√≥n de Operaciones B√°sicas
```
Implementa una clase para cada operaci√≥n (suma, multiplicaci√≥n, divisi√≥n)
con m√©todos forward() y backward(). Verifica con gradient checking.
```

**Ejercicio 2:** Grafo Computacional Manual
```
Para la funci√≥n f(x,y,z) = (x + y) * z:
a) Dibuja el grafo computacional
b) Calcula el forward pass con x=2, y=3, z=4
c) Calcula el backward pass manualmente
d) Verifica con c√≥digo
```

**Ejercicio 3:** Funciones de Activaci√≥n
```
Implementa forward y backward para:
- Sigmoid
- Tanh
- Leaky ReLU (con Œ±=0.01)
Verifica cada una con gradient checking.
```

### Nivel Intermedio

**Ejercicio 4:** Red de 3 Capas
```
Implementa una red con arquitectura: 4 ‚Üí 8 ‚Üí 4 ‚Üí 2
- Usa ReLU en capas ocultas, softmax en salida
- Entrena en un dataset de clasificaci√≥n multiclase
- Visualiza la evoluci√≥n de los gradientes
```

**Ejercicio 5:** Regularizaci√≥n L2
```
Agrega regularizaci√≥n L2 a tu red:
- Modifica la funci√≥n de p√©rdida: L = L_data + Œª||W||¬≤
- Implementa el gradiente correspondiente
- Compara resultados con y sin regularizaci√≥n
```

**Ejercicio 6:** Mini-batch SGD
```
Implementa entrenamiento con mini-batches:
- Divide los datos en batches de tama√±o 32
- Implementa un epoch completo iterando sobre batches
- Compara tiempo de entrenamiento vs batch completo
```

### Nivel Avanzado

**Ejercicio 7:** Arquitectura Profunda
```
Crea una red de 5+ capas:
- Implementa desde cero (no usar frameworks)
- Entrena en MNIST
- Diagnostica y soluciona vanishing gradients
- Usa diferentes inicializaciones (He, Xavier)
```

**Ejercicio 8:** Gradient Checking Completo
```
Implementa gradient checking para:
- Todos los par√°metros (W y b) de todas las capas
- Diferentes funciones de p√©rdida
- Crea un informe detallado de diferencias
```

**Ejercicio 9:** Optimizador con Momentum
```
Implementa backpropagation con momentum:
- v = Œ≤*v + (1-Œ≤)*gradiente
- W = W - Œ±*v
- Compara convergencia con SGD vanilla
```

## üìù Entregables

### 1. C√≥digo Fuente
- `backprop.py`: Implementaci√≥n de backpropagation
- `layers.py`: Clases de capas con forward/backward
- `network.py`: Red neuronal completa
- `gradient_check.py`: Verificaci√≥n de gradientes
- `experiments.ipynb`: Notebook con experimentos

### 2. Documentaci√≥n
- README explicando tu implementaci√≥n
- Comentarios detallados en el c√≥digo
- Diagramas de grafos computacionales

### 3. Resultados
- Curvas de aprendizaje
- Resultados de gradient checking
- Comparaci√≥n de diferentes configuraciones
- An√°lisis de errores

### 4. Reporte T√©cnico (2-3 p√°ginas)
Incluir:
- Explicaci√≥n de tu implementaci√≥n
- Decisiones de dise√±o
- Resultados experimentales
- Dificultades encontradas y soluciones
- Conclusiones

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Conceive (Concebir) - 25%
- [ ] Comprensi√≥n profunda de la regla de la cadena
- [ ] Identificaci√≥n correcta de gradientes necesarios
- [ ] Dise√±o apropiado de la arquitectura de c√≥digo
- [ ] Planificaci√≥n de estrategia de verificaci√≥n

### Design (Dise√±ar) - 25%
- [ ] Implementaci√≥n correcta del algoritmo de backpropagation
- [ ] C√≥digo modular y reutilizable
- [ ] Manejo apropiado de dimensiones matriciales
- [ ] Implementaci√≥n eficiente (vectorizaci√≥n)

### Implement (Implementar) - 30%
- [ ] C√≥digo funcional sin errores
- [ ] Gradient checking pasa (diferencia < 1e-7)
- [ ] Red neuronal entrena correctamente
- [ ] Resultados reproducibles

### Operate (Operar) - 20%
- [ ] Experimentaci√≥n con diferentes configuraciones
- [ ] An√°lisis cr√≠tico de resultados
- [ ] Identificaci√≥n y soluci√≥n de problemas
- [ ] Documentaci√≥n clara y completa

## üìã R√∫brica de Evaluaci√≥n

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|--------------|---------------------|------------------|
| **Implementaci√≥n** | Backprop perfecto, gradient check < 1e-9 | Backprop correcto, < 1e-7 | Backprop funciona, < 1e-5 | Errores en implementaci√≥n |
| **Comprensi√≥n** | Explica detalladamente cada paso | Explica conceptos principales | Explica parcialmente | Comprensi√≥n limitada |
| **C√≥digo** | Muy limpio, modular, documentado | Bien estructurado | Funcional pero b√°sico | Desorganizado o con errores |
| **Experimentos** | An√°lisis profundo, m√∫ltiples experimentos | Buenos experimentos | Experimentos b√°sicos | Experimentos insuficientes |
| **Documentaci√≥n** | Excelente, clara, completa | Buena documentaci√≥n | Documentaci√≥n b√°sica | Documentaci√≥n pobre |

## üìö Referencias Adicionales

### Art√≠culos Fundamentales
1. Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). "Learning representations by back-propagating errors". Nature.
2. LeCun, Y., et al. (1998). "Gradient-based learning applied to document recognition"
3. Glorot, X., & Bengio, Y. (2010). "Understanding the difficulty of training deep feedforward neural networks"

### Recursos Online
- **CS231n Stanford**: http://cs231n.stanford.edu/ (especialmente m√≥dulo sobre backpropagation)
- **Deep Learning Book** (Goodfellow): Cap√≠tulo 6 - Deep Feedforward Networks
- **3Blue1Brown**: Serie de videos sobre backpropagation (muy visual)
- **Andrej Karpathy**: "Yes you should understand backprop" (blog post)

### Herramientas
- NumPy documentation: https://numpy.org/doc/
- Matplotlib para visualizaci√≥n
- autograd (para verificar gradientes autom√°ticamente)

### Papers Adicionales
- "Delving Deep into Rectifiers" (He et al., 2015) - Sobre inicializaci√≥n
- "Batch Normalization" (Ioffe & Szegedy, 2015)
- "Understanding the difficulty of training deep feedforward neural networks" (Glorot & Bengio, 2010)

## üéì Notas Finales

### Consejos para el √âxito

1. **Siempre verifica tus gradientes**: Gradient checking es tu mejor amigo. Un error peque√±o en backprop puede arruinar todo el entrenamiento.

2. **Dibuja grafos computacionales**: Antes de programar, dibuja el grafo. Te ayudar√° a visualizar el flujo de gradientes.

3. **Empieza simple**: Implementa y verifica operaciones simples antes de construir redes complejas.

4. **Usa dimensiones expl√≠citas**: Siempre conoce las dimensiones de tus tensores. Muchos bugs vienen de errores de dimensi√≥n.

5. **Guarda valores intermedios**: En forward pass, guarda todo lo que necesitar√°s en backward pass.

### Errores Comunes

‚ùå **Olvidar transponer matrices**: `dW = X.T @ dZ` (no `X @ dZ`)
‚ùå **No sumar gradientes**: Al calcular `db`, hay que sumar sobre el batch
‚ùå **Confundir `*` y `@`**: `*` es elemento-wise, `@` es producto matricial
‚ùå **No inicializar gradientes a cero**: Acumular gradientes sin limpiar
‚ùå **Gradient checking con batch grande**: Usa batches peque√±os para verificar

### Reflexi√≥n Final

**Backpropagation es el coraz√≥n del deep learning**. Es la diferencia entre "redes neuronales interesantes en teor√≠a" y "redes neuronales que revolucionan la tecnolog√≠a".

Dominar backpropagation te da:
- Comprensi√≥n profunda de c√≥mo aprenden las redes
- Capacidad de debuggear problemas de entrenamiento
- Habilidad para implementar arquitecturas personalizadas
- Base s√≥lida para frameworks modernos (PyTorch, TensorFlow)

**Un mensaje importante**: Una vez que entiendas backpropagation a fondo, probablemente nunca lo implementar√°s manualmente otra vez. Los frameworks modernos lo hacen autom√°ticamente. Pero ese conocimiento profundo te har√° un practicante mucho mejor de deep learning.

### Pr√≥ximos Pasos

En el siguiente laboratorio (Lab 06), usaremos backpropagation para:
- Implementar loops de entrenamiento completos
- Trabajar con datasets reales
- Implementar t√©cnicas avanzadas (early stopping, regularizaci√≥n)
- Optimizar el rendimiento del entrenamiento

¬°Ahora tienes la herramienta m√°s poderosa en deep learning. √ösala sabiamente! üöÄ

---

**"Understanding backpropagation is the difference between being a machine learning user and a machine learning practitioner."** - Andrej Karpathy

**¬°Backpropagation es la magia del deep learning! ‚ú®**
