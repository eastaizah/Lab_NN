# Gu√≠a de Laboratorio: Backpropagation - El Coraz√≥n del Deep Learning

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Fundamentos de Deep Learning - Backpropagation  
**C√≥digo:** Lab 05  
**Duraci√≥n:** 2-3 horas  
**Nivel:** Intermedio  

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender profundamente la regla de la cadena y su aplicaci√≥n en deep learning
2. Interpretar y construir grafos computacionales para operaciones matem√°ticas
3. Implementar el algoritmo de backpropagation desde cero en Python
4. Calcular gradientes anal√≠ticos para funciones de activaci√≥n y p√©rdida
5. Verificar implementaciones mediante gradient checking (gradientes num√©ricos)
6. Identificar y guardar valores intermedios necesarios para backpropagation
7. Entrenar una red neuronal completa usando backpropagation
8. Diagnosticar problemas comunes: gradientes que explotan o desaparecen
9. Optimizar el c√≥digo usando vectorizaci√≥n y operaciones matriciales

## üìö Prerrequisitos

### Conocimientos

- Python intermedio (funciones, clases, NumPy)
- C√°lculo diferencial (derivadas, regla de la cadena)
- √Ålgebra lineal (multiplicaci√≥n matricial, transposici√≥n)
- Redes neuronales b√°sicas (forward pass, funciones de activaci√≥n)
- Funciones de p√©rdida (MSE, cross-entropy)

### Software

- Python 3.8+
- NumPy 1.19+
- Matplotlib (para visualizaciones)
- Jupyter Notebook (recomendado)

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` - Marco te√≥rico completo sobre backpropagation
- `README.md` - Estructura del laboratorio y recursos disponibles
- Revisar Labs anteriores (01-04) sobre neuronas, funciones de activaci√≥n y p√©rdida

## üìñ Introducci√≥n

### El Problema Fundamental del Aprendizaje

Hasta ahora hemos visto c√≥mo las redes neuronales hacen predicciones mediante el **forward pass**: los datos fluyen de entrada a salida a trav√©s de capas de neuronas. Pero, ¬øc√≥mo aprende la red? ¬øC√≥mo ajustamos los millones de par√°metros (pesos y bias) para mejorar las predicciones?

La respuesta es **Backpropagation** - el algoritmo que revolucion√≥ el deep learning.

### ¬øQu√© es Backpropagation?

**Backpropagation** (propagaci√≥n hacia atr√°s) es un algoritmo eficiente para calcular gradientes de la funci√≥n de p√©rdida con respecto a todos los par√°metros de una red neuronal. Es la aplicaci√≥n inteligente de la **regla de la cadena del c√°lculo diferencial**.

```
ENTRENAMIENTO DE UNA RED NEURONAL:

1. Forward Pass:
   Entrada ‚Üí Capa1 ‚Üí Capa2 ‚Üí ... ‚Üí Salida ‚Üí P√©rdida
   
2. Backward Pass (Backpropagation):
   ‚àÇL/‚àÇW‚ÇÅ ‚Üê ‚àÇL/‚àÇW‚ÇÇ ‚Üê ... ‚Üê ‚àÇL/‚àÇW‚Çô ‚Üê Gradiente de P√©rdida
   
3. Actualizaci√≥n de Par√°metros:
   W_nuevo = W_viejo - Œ± * ‚àÇL/‚àÇW
```

### Contexto Hist√≥rico

Aunque las redes neuronales fueron propuestas en los a√±os 40-50, su verdadero potencial no se liber√≥ hasta que backpropagation fue popularizado en 1986 por Rumelhart, Hinton y Williams. Este algoritmo:

- Hace posible entrenar redes con m√∫ltiples capas ocultas
- Calcula gradientes de manera eficiente (complejidad lineal)
- Es la base de todas las redes neuronales modernas
- Permite el aprendizaje autom√°tico de representaciones complejas

### La Regla de la Cadena: Fundamento Matem√°tico

Backpropagation se basa en una idea simple del c√°lculo: **la regla de la cadena**.

**Para funciones compuestas:**
Si tenemos `y = f(g(x))`, entonces:
$$\frac{dy}{dx} = \frac{dy}{dg} \times \frac{dg}{dx}$$

**Ejemplo intuitivo:**

Imagina que conduces un auto:
- La distancia depende de la velocidad: `d = v * t`
- La velocidad depende del acelerador: `v = k * a`
- Por tanto: `d = (k * a) * t`

¬øC√≥mo afecta el acelerador a la distancia? Usando la regla de la cadena:
$$\frac{‚àÇd}{‚àÇa} = \frac{‚àÇd}{‚àÇv} \times \frac{‚àÇv}{‚àÇa} = t \times k$$

En redes neuronales, la "distancia" es la p√©rdida, y el "acelerador" son los pesos.

### Grafos Computacionales

Una forma poderosa de entender backpropagation es mediante **grafos computacionales** - diagramas que muestran c√≥mo se calculan las salidas a partir de las entradas.

**Ejemplo simple: z = (x + y) √ó w**

```
    x ‚îÄ‚îÄ‚îÄ‚îê
         ‚îú‚îÄ‚îÄ‚Üí [+] ‚îÄ‚îÄ‚Üí q ‚îÄ‚îÄ‚îÄ‚îê
    y ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
                           ‚îú‚îÄ‚îÄ‚Üí [√ó] ‚îÄ‚îÄ‚Üí z
                       w ‚îÄ‚îÄ‚îÄ‚îò
```

**Forward pass** (izquierda ‚Üí derecha): Calcular z
- `q = x + y`
- `z = q √ó w`

**Backward pass** (derecha ‚Üê izquierda): Calcular gradientes
- `‚àÇz/‚àÇz = 1` (empezamos aqu√≠)
- `‚àÇz/‚àÇq = w`, `‚àÇz/‚àÇw = q`
- `‚àÇz/‚àÇx = (‚àÇz/‚àÇq) √ó (‚àÇq/‚àÇx) = w √ó 1 = w`
- `‚àÇz/‚àÇy = (‚àÇz/‚àÇq) √ó (‚àÇq/‚àÇy) = w √ó 1 = w`

### ¬øPor qu√© es Tan Importante?

**Sin backpropagation:**
- Calcular gradientes manualmente es tedioso y propenso a errores
- Complejidad computacional prohibitiva para redes grandes
- Imposible entrenar redes profundas eficientemente

**Con backpropagation:**
- C√°lculo autom√°tico de todos los gradientes
- Un solo pase hacia atr√°s calcula todos los gradientes necesarios
- Complejidad lineal O(n) donde n es el n√∫mero de par√°metros
- Permite entrenar redes con millones de par√°metros

### Aplicaciones Pr√°cticas

Backpropagation es el motor detr√°s de:
- **Visi√≥n por Computadora**: Redes que reconocen objetos, rostros, escenas
- **PLN**: Modelos de lenguaje como GPT, BERT, traducci√≥n autom√°tica
- **Generaci√≥n**: GANs que crean im√°genes realistas, m√∫sica, texto
- **Juegos**: AlphaGo, agentes que aprenden a jugar videojuegos
- **Ciencia**: Predicci√≥n de estructuras de prote√≠nas, simulaciones f√≠sicas

Todos estos avances ser√≠an imposibles sin backpropagation.

## ü§î Preguntas de Reflexi√≥n Iniciales

Antes de comenzar, reflexiona sobre estas preguntas:

1. ¬øPor qu√© necesitamos calcular gradientes para entrenar una red neuronal?
2. ¬øQu√© significa "eficiencia computacional" en el contexto de c√°lculo de gradientes?
3. Si una red tiene 1 mill√≥n de par√°metros, ¬øcu√°ntas derivadas parciales necesitamos calcular?
4. ¬øC√≥mo podr√≠amos verificar que nuestro c√°lculo de gradientes es correcto?
5. ¬øQu√© informaci√≥n del forward pass necesitamos guardar para backpropagation?

## üî¨ Parte 1: Regla de la Cadena y Grafos Computacionales (40 min)

### 1.1 Repaso de la Regla de la Cadena

**¬øQu√© hacemos?** Revisamos la regla de la cadena del c√°lculo diferencial y la aplicamos a funciones compuestas como las que aparecen en redes neuronales.

**¬øPor qu√© lo hacemos?** Una red neuronal es esencialmente una funci√≥n compuesta de muchas capas: `L = f_n(f_{n-1}(... f_1(x)))`. Para ajustar cualquier par√°metro, necesitamos calcular `‚àÇL/‚àÇw`. La regla de la cadena nos permite **descomponer** esa derivada compleja en una cadena de derivadas locales simples:

$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial a_n} \cdot \frac{\partial a_n}{\partial a_{n-1}} \cdots \frac{\partial a_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial w}$$

Sin la regla de la cadena, calcular gradientes en una red de 100 capas ser√≠a matem√°ticamente intratable. Con ella, basta con que cada capa conozca su **gradiente local** y sepa multiplicarlo por el gradiente que llega desde las capas superiores.

**¬øC√≥mo lo hacemos?** Introducimos la notaci√≥n de variable intermedia `u`:

$$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u} \cdot \frac{\partial u}{\partial x}$$

**Analog√≠a del termostato:** Imagina que el consumo el√©ctrico `E` depende de la temperatura de la habitaci√≥n `T`, y `T` depende de la posici√≥n del termostato `p`. Para saber cu√°nto afecta el termostato al consumo ‚Äîes decir, `‚àÇE/‚àÇp`‚Äî multiplicamos "cu√°nto cambia el consumo por grado" (`‚àÇE/‚àÇT`) por "cu√°nto cambia la temperatura por posici√≥n" (`‚àÇT/‚àÇp`). Eso es exactamente la regla de la cadena aplicada a una neurona.

**¬øQu√© resultados esperar?** Gradientes que coincidan exactamente con los calculados por diferenciaci√≥n anal√≠tica directa. La regla de la cadena no es una aproximaci√≥n: es matem√°ticamente exacta.

Comencemos con ejemplos matem√°ticos simples antes de aplicarlo a redes neuronales.

**Ejemplo 1: Funci√≥n compuesta simple**

```python
import numpy as np

# Funci√≥n: y = (3x + 2)¬≤
# Queremos: dy/dx

def forward_example1(x):
    """Forward pass: calcular y"""
    u = 3 * x + 2
    y = u ** 2
    return y, u  # Guardamos u para backprop

def backward_example1(x):
    """Backward pass: calcular dy/dx"""
    # Forward (calcular y guardar valores intermedios)
    u = 3 * x + 2
    y = u ** 2
    
    # Backward (aplicar regla de la cadena)
    dy_du = 2 * u      # ‚àÇy/‚àÇu = 2u
    du_dx = 3          # ‚àÇu/‚àÇx = 3
    dy_dx = dy_du * du_dx  # Regla de la cadena
    
    return dy_dx

# Probar
x = 5.0
y, u = forward_example1(x)
print(f"x = {x}")
print(f"u = 3x + 2 = {u}")
print(f"y = u¬≤ = {y}")

gradient = backward_example1(x)
print(f"dy/dx = {gradient}")
# Resultado: dy/dx = 2(3x + 2) * 3 = 6(17) = 102
```

**Actividad 1.1:** Implementa y verifica el gradiente de `y = sin(2x + 1)`

**Ejemplo 2: M√∫ltiples variables**

```python
# Funci√≥n: z = x¬≤ + y¬≤ + 2xy
# Queremos: ‚àÇz/‚àÇx, ‚àÇz/‚àÇy

def forward_example2(x, y):
    """Forward pass"""
    z = x**2 + y**2 + 2*x*y
    return z

def backward_example2(x, y):
    """Backward pass"""
    # Derivadas parciales
    dz_dx = 2*x + 2*y  # ‚àÇz/‚àÇx = 2x + 2y
    dz_dy = 2*y + 2*x  # ‚àÇz/‚àÇy = 2y + 2x
    
    return dz_dx, dz_dy

# Probar
x, y = 3.0, 4.0
z = forward_example2(x, y)
dz_dx, dz_dy = backward_example2(x, y)

print(f"z({x}, {y}) = {z}")
print(f"‚àÇz/‚àÇx = {dz_dx}")
print(f"‚àÇz/‚àÇy = {dz_dy}")
```

### 1.2 Grafos Computacionales

**¬øQu√© hacemos?** Representamos c√°lculos matem√°ticos como un grafo dirigido donde los nodos son operaciones y las aristas son el flujo de datos (y de gradientes).

**¬øPor qu√© lo hacemos?** Los grafos computacionales convierten backpropagation en un procedimiento **sistem√°tico y autom√°tico**. En lugar de derivar manualmente una funci√≥n monol√≠tica compleja, cada nodo del grafo solo necesita conocer su operaci√≥n local y aplicar la regla de la cadena hacia atr√°s:

```
Forward pass  ‚Üí‚Üí‚Üí‚Üí‚Üí‚Üí‚Üí  (izquierda a derecha): calcular salidas
Backward pass ‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê  (derecha a izquierda): propagar gradientes
```

Esta separaci√≥n limpia es la raz√≥n por la que frameworks como PyTorch o TensorFlow pueden calcular gradientes autom√°ticamente para cualquier arquitectura: construyen el grafo en el forward pass y lo recorren en reversa durante el backward pass.

**¬øC√≥mo lo hacemos?** Cada nodo almacena:
1. Su **valor** (calculado en el forward pass)
2. Su **gradiente acumulado** (calculado en el backward pass)
3. C√≥mo propagar el gradiente hacia sus entradas (la "puerta local")

**¬øQu√© resultados esperar?** Al final del backward pass, cada nodo tendr√° el gradiente correcto `‚àÇL/‚àÇnodo`, que es exactamente lo que necesitamos para actualizar los par√°metros.

Los grafos computacionales son herramientas visuales poderosas para entender backpropagation.

**Ejemplo: z = (x + y) √ó w**

```python
class ComputationNode:
    """Nodo en un grafo computacional"""
    def __init__(self, name):
        self.name = name
        self.value = None
        self.grad = 0
    
    def __repr__(self):
        return f"{self.name}={self.value:.4f}, grad={self.grad:.4f}"

def forward_graph_example():
    """Forward pass con grafo computacional"""
    # Crear nodos
    x = ComputationNode('x')
    y = ComputationNode('y')
    w = ComputationNode('w')
    q = ComputationNode('q')  # q = x + y
    z = ComputationNode('z')  # z = q * w
    
    # Valores de entrada
    x.value = 2.0
    y.value = 3.0
    w.value = 4.0
    
    # Forward pass
    q.value = x.value + y.value  # q = 5.0
    z.value = q.value * w.value  # z = 20.0
    
    return x, y, w, q, z

def backward_graph_example(x, y, w, q, z):
    """Backward pass con grafo computacional"""
    # Inicializar gradiente de salida
    z.grad = 1.0  # dL/dz = 1 (asumimos L = z)
    
    # Backward pass (en orden inverso)
    # z = q * w
    q.grad += z.grad * w.value  # ‚àÇz/‚àÇq = w
    w.grad += z.grad * q.value  # ‚àÇz/‚àÇw = q
    
    # q = x + y
    x.grad += q.grad * 1.0  # ‚àÇq/‚àÇx = 1
    y.grad += q.grad * 1.0  # ‚àÇq/‚àÇy = 1

# Ejecutar
x, y, w, q, z = forward_graph_example()
print("Forward pass:")
print(x, y, w, q, z)

backward_graph_example(x, y, w, q, z)
print("\nBackward pass:")
print(x, y, w, q, z)
```

**Actividad 1.2:** Dibuja el grafo computacional para `f = (x + y) √ó (x - y)` y calcula todos los gradientes.

### 1.3 Operaciones B√°sicas y sus Gradientes

**¬øQu√© hacemos?** Catalogamos las operaciones primitivas m√°s frecuentes en redes neuronales junto con sus gradientes locales.

**¬øPor qu√© lo hacemos?** Cualquier funci√≥n compleja ‚Äîpor ejemplo `sigmoid(w¬∑x + b)`‚Äî puede descomponerse en una cadena de operaciones primitivas (suma, multiplicaci√≥n, exponencial). Si conocemos el gradiente local de cada primitiva, podemos calcular el gradiente de cualquier composici√≥n simplemente multiplicando los gradientes locales (regla de la cadena).

**Tabla de gradientes de operaciones primitivas:**

| Operaci√≥n | Forward: `z = f(x, y)` | Gradiente `‚àÇz/‚àÇx` | Gradiente `‚àÇz/‚àÇy` | Notas |
|-----------|------------------------|-------------------|-------------------|-------|
| Suma      | `z = x + y`            | `1`               | `1`               | Distribuye el gradiente igual a ambas entradas |
| Resta     | `z = x - y`            | `1`               | `-1`              | Invierte el signo hacia la segunda entrada |
| Multiplicaci√≥n | `z = x * y`       | `y`               | `x`               | Cada entrada recibe el valor de la otra |
| Divisi√≥n  | `z = x / y`            | `1/y`             | `-x/y¬≤`           | Asim√©trico: la entrada denominador tiene gradiente negativo |
| Cuadrado  | `z = x¬≤`               | `2x`              | ‚Äî                 | Requiere guardar `x` en cach√© |
| Exponencial | `z = eÀ£`             | `eÀ£`              | ‚Äî                 | La derivada es ella misma; requiere guardar `z` en cach√© |
| Logaritmo | `z = ln(x)`            | `1/x`             | ‚Äî                 | Solo v√°lido para `x > 0`; gradiente explota cerca de 0 |
| ReLU      | `z = max(0, x)`        | `1 si x>0, 0 si x‚â§0` | ‚Äî             | Corta el gradiente para activaciones negativas |
| Sigmoid   | `z = œÉ(x)`             | `œÉ(x)(1-œÉ(x))`   | ‚Äî                 | Se satura en extremos ‚Üí gradiente ‚âà 0 |

**¬øC√≥mo lo hacemos?** Implementamos cada operaci√≥n como una clase con m√©todos `forward()` y `backward()`. Esto nos permite componerlas libremente para construir funciones arbitrariamente complejas.

**¬øQu√© resultados esperar?** Para cada operaci√≥n, los gradientes num√©ricos y anal√≠ticos deben coincidir con una diferencia relativa menor a `1e-7`.

Tabla de referencia para operaciones comunes:

```python
class GradientOperations:
    """Colecci√≥n de operaciones con sus gradientes"""
    
    @staticmethod
    def add_forward(x, y):
        return x + y
    
    @staticmethod
    def add_backward(dout, x, y):
        """‚àÇ(x+y)/‚àÇx = 1, ‚àÇ(x+y)/‚àÇy = 1"""
        dx = dout * 1
        dy = dout * 1
        return dx, dy
    
    @staticmethod
    def mul_forward(x, y):
        return x * y
    
    @staticmethod
    def mul_backward(dout, x, y):
        """‚àÇ(x*y)/‚àÇx = y, ‚àÇ(x*y)/‚àÇy = x"""
        dx = dout * y
        dy = dout * x
        return dx, dy
    
    @staticmethod
    def square_forward(x):
        return x ** 2
    
    @staticmethod
    def square_backward(dout, x):
        """‚àÇ(x¬≤)/‚àÇx = 2x"""
        dx = dout * 2 * x
        return dx
    
    @staticmethod
    def exp_forward(x):
        return np.exp(x)
    
    @staticmethod
    def exp_backward(dout, x):
        """‚àÇ(e^x)/‚àÇx = e^x"""
        dx = dout * np.exp(x)
        return dx

# Ejemplo de uso
ops = GradientOperations()

# Forward
x, y = 3.0, 4.0
z = ops.mul_forward(x, y)  # z = 12
print(f"z = {z}")

# Backward (asumiendo dL/dz = 1)
dout = 1.0
dx, dy = ops.mul_backward(dout, x, y)
print(f"‚àÇz/‚àÇx = {dx}, ‚àÇz/‚àÇy = {dy}")  # ‚àÇz/‚àÇx = 4, ‚àÇz/‚àÇy = 3
```

**Actividad 1.3:** Implementa gradientes para divisi√≥n, exponencial y logaritmo.

## üî¨ Parte 2: Backpropagation en una Neurona (45 min)

### 2.1 Anatom√≠a de una Neurona con Backpropagation

**¬øQu√© hacemos?** Implementamos una neurona que, adem√°s de realizar el forward pass (`z = w¬∑x + b`), puede ejecutar el backward pass para calcular gradientes con respecto a sus par√°metros.

**¬øPor qu√© lo hacemos?** Una neurona tiene tres tipos de par√°metros que necesitan gradientes:
- `‚àÇL/‚àÇw` ‚Üí para actualizar los pesos y mejorar la predicci√≥n
- `‚àÇL/‚àÇb` ‚Üí para actualizar el bias
- `‚àÇL/‚àÇx` ‚Üí para **propagar** el gradiente hacia las capas anteriores (esta neurona no es la primera)

**La clave del cach√©:** Durante el forward pass, debemos guardar los valores intermedios que necesitaremos en el backward pass. Para la operaci√≥n `z = w¬∑x + b`:
- Necesitamos `x` para calcular `‚àÇL/‚àÇw = ‚àÇL/‚àÇz ¬∑ x`
- Necesitamos `w` para calcular `‚àÇL/‚àÇx = ‚àÇL/‚àÇz ¬∑ w`

Si no guardamos `x` durante el forward pass, no podemos calcular `‚àÇL/‚àÇw` durante el backward pass.

**¬øC√≥mo lo hacemos?** Usamos un diccionario `cache` para almacenar los valores intermedios del forward pass. El backward pass recibe `dz = ‚àÇL/‚àÇz` (el gradiente que llega desde la capa siguiente) y calcula los tres gradientes usando la regla de la cadena:

$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w} = dz \cdot x$$

$$\frac{\partial L}{\partial b} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial b} = dz \cdot 1 = dz$$

$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial x} = dz \cdot w$$

**¬øQu√© resultados esperar?** Los gradientes calculados deben coincidir con los gradientes num√©ricos con precisi√≥n de al menos `1e-7`.

Implementemos una neurona que puede hacer forward y backward pass.

```python
class NeuronWithBackprop:
    """Neurona con capacidad de backpropagation"""
    
    def __init__(self, n_inputs):
        """Inicializar pesos y bias aleatoriamente"""
        self.w = np.random.randn(n_inputs) * 0.1
        self.b = np.random.randn() * 0.1
        
        # Cache para backpropagation
        self.cache = {}
        
        # Gradientes
        self.dw = np.zeros_like(self.w)
        self.db = 0
    
    def forward(self, x):
        """
        Forward pass: z = w¬∑x + b
        Guardamos x para usarlo en backprop
        """
        self.cache['x'] = x
        z = np.dot(self.w, x) + self.b
        self.cache['z'] = z
        return z
    
    def backward(self, dz):
        """
        Backward pass
        Entrada: dz = ‚àÇL/‚àÇz (gradiente de la p√©rdida respecto a z)
        Salida: dx = ‚àÇL/‚àÇx (gradiente para propagar hacia atr√°s)
        
        Derivadas:
        ‚àÇz/‚àÇw = x  ‚Üí  ‚àÇL/‚àÇw = ‚àÇL/‚àÇz * ‚àÇz/‚àÇw = dz * x
        ‚àÇz/‚àÇb = 1  ‚Üí  ‚àÇL/‚àÇb = ‚àÇL/‚àÇz * ‚àÇz/‚àÇb = dz * 1
        ‚àÇz/‚àÇx = w  ‚Üí  ‚àÇL/‚àÇx = ‚àÇL/‚àÇz * ‚àÇz/‚àÇx = dz * w
        """
        x = self.cache['x']
        
        # Calcular gradientes
        self.dw = dz * x  # ‚àÇL/‚àÇw
        self.db = dz      # ‚àÇL/‚àÇb
        dx = dz * self.w  # ‚àÇL/‚àÇx (para propagar)
        
        return dx
    
    def update(self, learning_rate=0.01):
        """Actualizar par√°metros usando gradient descent"""
        self.w -= learning_rate * self.dw
        self.b -= learning_rate * self.db

# Ejemplo de uso
neuron = NeuronWithBackprop(n_inputs=3)

# Forward pass
x = np.array([1.0, 2.0, 3.0])
z = neuron.forward(x)
print(f"Salida de la neurona: {z}")

# Backward pass (simulando dL/dz = 1)
dz = 1.0
dx = neuron.backward(dz)

print(f"Gradientes:")
print(f"  dL/dw = {neuron.dw}")
print(f"  dL/db = {neuron.db}")
print(f"  dL/dx = {dx}")

# Actualizar par√°metros
neuron.update(learning_rate=0.1)
print(f"\nPesos actualizados: {neuron.w}")
print(f"Bias actualizado: {neuron.b}")
```

### 2.2 Neurona con Funci√≥n de Activaci√≥n

**¬øQu√© hacemos?** Extendemos la neurona para incluir una funci√≥n de activaci√≥n no lineal (ReLU) en el forward pass y su derivada en el backward pass.

**¬øPor qu√© lo hacemos?** La funci√≥n de activaci√≥n introduce **no-linealidad** en la red, pero tambi√©n crea una "compuerta" por la que debe pasar el gradiente. Sin considerar la activaci√≥n en el backward pass, los gradientes ser√≠an incorrectos.

La cadena completa para una neurona con activaci√≥n es:

$$\text{Forward:} \quad z = w \cdot x + b \xrightarrow{\text{ReLU}} a = \max(0, z)$$

$$\text{Backward:} \quad \frac{\partial L}{\partial a} \xrightarrow{\cdot \text{ReLU}'(z)} \frac{\partial L}{\partial z} \xrightarrow{\text{neurona}} \frac{\partial L}{\partial w}, \frac{\partial L}{\partial b}, \frac{\partial L}{\partial x}$$

El paso clave es `dz = da * ReLU'(z)`, donde `ReLU'(z) = 1 si z > 0, 0 si z ‚â§ 0`. Esto significa que cuando `z ‚â§ 0`, el gradiente se **bloquea completamente** (la neurona est√° "muerta" y no aprende). Para `z > 0`, el gradiente fluye sin modificaci√≥n.

**Impacto en el flujo de gradientes:**
- **ReLU**: Flujo binario (0 o 1). Puede causar neuronas muertas, pero evita saturaci√≥n.
- **Sigmoid**: Flujo suavizado (`œÉ(1-œÉ)`). Para valores extremos de `z`, el gradiente se acerca a cero ‚Üí **gradiente desvaneciente**.
- **Tanh**: Similar a sigmoid pero con mejor simetr√≠a; a√∫n puede saturarse.

**¬øQu√© resultados esperar?** Cuando `z < 0`, los gradientes `dw`, `db` y `dx` deben ser cero porque ReLU bloque√≥ el flujo. Cuando `z > 0`, el comportamiento debe ser id√©ntico al de la neurona sin activaci√≥n.

Agreguemos una funci√≥n de activaci√≥n (ReLU):

```python
class NeuronWithActivation:
    """Neurona con funci√≥n de activaci√≥n y backprop"""
    
    def __init__(self, n_inputs):
        self.w = np.random.randn(n_inputs) * 0.1
        self.b = np.random.randn() * 0.1
        self.cache = {}
        self.dw = np.zeros_like(self.w)
        self.db = 0
    
    def relu(self, z):
        """ReLU: max(0, z)"""
        return np.maximum(0, z)
    
    def relu_derivative(self, z):
        """Derivada de ReLU: 1 si z > 0, else 0"""
        return (z > 0).astype(float)
    
    def forward(self, x):
        """Forward pass: a = ReLU(w¬∑x + b)"""
        self.cache['x'] = x
        
        # Suma ponderada
        z = np.dot(self.w, x) + self.b
        self.cache['z'] = z
        
        # Activaci√≥n
        a = self.relu(z)
        self.cache['a'] = a
        
        return a
    
    def backward(self, da):
        """
        Backward pass con activaci√≥n
        
        Entrada: da = ‚àÇL/‚àÇa
        
        Pasos:
        1. dz = da * ReLU'(z)  (gradiente local de ReLU)
        2. dw = dz * x
        3. db = dz
        4. dx = dz * w
        """
        x = self.cache['x']
        z = self.cache['z']
        
        # Gradiente a trav√©s de ReLU
        dz = da * self.relu_derivative(z)
        
        # Gradientes de par√°metros
        self.dw = dz * x
        self.db = dz
        
        # Gradiente para propagar
        dx = dz * self.w
        
        return dx
    
    def update(self, lr=0.01):
        self.w -= lr * self.dw
        self.b -= lr * self.db

# Ejemplo
neuron = NeuronWithActivation(n_inputs=3)

# Forward
x = np.array([1.0, -2.0, 3.0])
a = neuron.forward(x)
print(f"Input: {x}")
print(f"z (before ReLU): {neuron.cache['z']}")
print(f"a (after ReLU): {a}")

# Backward
da = 1.0  # Gradiente de entrada
dx = neuron.backward(da)

print(f"\nGradientes:")
print(f"  dL/dw = {neuron.dw}")
print(f"  dL/db = {neuron.db}")
print(f"  dL/dx = {dx}")
```

**Actividad 2.1:** Implementa `NeuronWithSigmoid` que use funci√≥n sigmoid en lugar de ReLU.

### 2.3 Ejemplo Completo: Entrenar una Neurona

**¬øQu√© hacemos?** Usamos nuestra neurona con backpropagation para aprender la funci√≥n l√≥gica AND mediante descenso por gradiente.

**¬øPor qu√© AND y no XOR?** Una neurona individual ‚Äîincluso con funci√≥n de activaci√≥n‚Äî solo puede aprender problemas **linealmente separables**: aquellos donde las clases pueden separarse con un hiperplano (una l√≠nea en 2D). Esto es una limitaci√≥n fundamental:

```
AND: linealmente separable        XOR: NO linealmente separable
(0,0)‚Üí0  (0,1)‚Üí0                  (0,0)‚Üí0  (0,1)‚Üí1
(1,0)‚Üí0  (1,1)‚Üí1                  (1,0)‚Üí1  (1,1)‚Üí0

  y                                 y
  1 | . .                           1 | . x
  0 | . x          /l√≠nea/          0 | x .
     ------                            ------
     0  1  x                           0  1  x

Leyenda: x = clase 1, . = clase 0

‚úì Una l√≠nea puede separar           ‚úó Ninguna l√≠nea puede separar
  el "1" de los "0"                   los "1" de los "0"
```

La neurona con AND aprender√° correctamente. Con XOR, la p√©rdida nunca llegar√° a cero y las predicciones ser√°n incorrectas. Esto demuestra por qu√© necesitamos **m√∫ltiples capas**: para aprender fronteras de decisi√≥n no lineales.

**¬øC√≥mo lo hacemos?** Realizamos descenso por gradiente estoc√°stico (SGD): para cada ejemplo de entrenamiento, ejecutamos forward pass, calculamos la p√©rdida MSE, ejecutamos backward pass y actualizamos los par√°metros.

**¬øQu√© resultados esperar?**
- Para **AND**: La p√©rdida debe decrecer y los outputs deben acercarse a 0 para `[0,0]`, `[0,1]`, `[1,0]` y a 1 para `[1,1]`.
- Para **XOR** (Actividad 2.2): La p√©rdida se estancar√° y las predicciones ser√°n imprecisas, evidenciando la limitaci√≥n de las neuronas simples.

Entrenemos una neurona para aprender la funci√≥n AND:

```python
# Datos: AND l√≥gico
X_train = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

y_train = np.array([0, 0, 0, 1])  # AND

# Crear neurona
neuron = NeuronWithActivation(n_inputs=2)

# Funci√≥n de p√©rdida MSE
def mse_loss(pred, target):
    return 0.5 * (pred - target) ** 2

def mse_derivative(pred, target):
    return pred - target

# Entrenar
learning_rate = 0.1
epochs = 1000

for epoch in range(epochs):
    total_loss = 0
    
    for x, y_true in zip(X_train, y_train):
        # Forward
        y_pred = neuron.forward(x)
        loss = mse_loss(y_pred, y_true)
        total_loss += loss
        
        # Backward
        dloss = mse_derivative(y_pred, y_true)
        neuron.backward(dloss)
        
        # Update
        neuron.update(learning_rate)
    
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {total_loss:.4f}")

# Evaluar
print("\nResultados finales:")
for x, y_true in zip(X_train, y_train):
    y_pred = neuron.forward(x)
    print(f"Input: {x}, Predicci√≥n: {y_pred:.4f}, Real: {y_true}")
```

**Actividad 2.2:** Entrena una neurona para aprender OR y XOR. ¬øQu√© observas con XOR?

## üî¨ Parte 3: Backpropagation en Redes Multicapa (60 min)

### 3.1 Red de 2 Capas con Backpropagation

**¬øQu√© hacemos?** Implementamos una red neuronal con dos capas densas, cada una con su propio forward y backward pass, formando una cadena completa de backpropagation.

**¬øPor qu√© m√∫ltiples capas?** Una capa oculta transforma los datos a un **nuevo espacio de representaci√≥n** donde el problema puede volverse linealmente separable. Geom√©tricamente:
- Una capa = un hiperplano (frontera lineal)
- Dos capas = m√∫ltiples hiperplanos combinados (regiones convexas)
- Tres o m√°s capas = fronteras arbitrariamente complejas

Por esto la red de 2 capas puede aprender XOR (imposible para una sola neurona): la primera capa transforma el espacio, y la segunda separa linealmente la representaci√≥n resultante.

**Inicializaci√≥n He: ¬øPor qu√© importa?**

La inicializaci√≥n correcta de los pesos es cr√≠tica para evitar problemas desde el inicio del entrenamiento:

$$W \sim \mathcal{N}\left(0, \sqrt{\frac{2}{n_{in}}}\right)$$

Si los pesos son demasiado **peque√±os** (ej. todos cero): todas las neuronas aprenden lo mismo (simetr√≠a perfecta), los gradientes son id√©nticos, y la red no puede aprender representaciones diversas.

Si los pesos son demasiado **grandes**: las activaciones se saturan desde el primer forward pass, los gradientes desaparecen o explotan antes de que empiece el entrenamiento.

La inicializaci√≥n **He** (tambi√©n conocida como **inicializaci√≥n Kaiming**, propuesta por Kaiming He et al., 2015) est√° dise√±ada espec√≠ficamente para ReLU: el factor `‚àö(2/n_in)` compensa que ReLU desactiva aproximadamente la mitad de las neuronas, manteniendo la varianza de las activaciones constante a lo largo de la red durante las primeras iteraciones.

**¬øC√≥mo lo hacemos?** El backward pass de la red sigue el orden inverso de las capas. El gradiente fluye de salida a entrada:

```
Forward:  X ‚Üí [Capa1] ‚Üí A1 ‚Üí [Capa2] ‚Üí A2 ‚Üí L
Backward: dX ‚Üê [Capa1] ‚Üê dA1 ‚Üê [Capa2] ‚Üê dA2 ‚Üê dL
```

Cada capa calcula `dW`, `db` para actualizar sus propios par√°metros, y `dX` para pasarlo a la capa anterior.

**¬øQu√© resultados esperar?** Con 5000 √©pocas en XOR, la p√©rdida debe bajar por debajo de 0.01 y las predicciones deben ser inequ√≠vocas: cerca de 0 para entradas iguales y cerca de 1 para entradas distintas.

Implementemos una red completa con dos capas:

```python
class Layer:
    """Capa densa con backpropagation"""
    
    def __init__(self, n_inputs, n_neurons, activation='relu'):
        # Inicializaci√≥n He para ReLU
        self.W = np.random.randn(n_inputs, n_neurons) * np.sqrt(2.0 / n_inputs)
        self.b = np.zeros((1, n_neurons))
        self.activation = activation
        self.cache = {}
        
    def forward(self, X):
        """
        Forward pass
        X: (batch_size, n_inputs)
        Salida: (batch_size, n_neurons)
        """
        self.cache['X'] = X
        
        # Z = X @ W + b
        Z = np.dot(X, self.W) + self.b
        self.cache['Z'] = Z
        
        # Activaci√≥n
        if self.activation == 'relu':
            A = np.maximum(0, Z)
        elif self.activation == 'sigmoid':
            A = 1 / (1 + np.exp(-Z))
        elif self.activation == 'linear':
            A = Z
        
        self.cache['A'] = A
        return A
    
    def backward(self, dA):
        """
        Backward pass
        dA: gradiente de la p√©rdida respecto a A (salida de esta capa)
        Retorna: dX (gradiente para propagar a capa anterior)
        """
        X = self.cache['X']
        Z = self.cache['Z']
        m = X.shape[0]  # batch size
        
        # Gradiente de la activaci√≥n
        if self.activation == 'relu':
            dZ = dA * (Z > 0)
        elif self.activation == 'sigmoid':
            A = self.cache['A']
            dZ = dA * A * (1 - A)
        elif self.activation == 'linear':
            dZ = dA
        
        # Gradientes de par√°metros
        self.dW = (1/m) * np.dot(X.T, dZ)
        self.db = (1/m) * np.sum(dZ, axis=0, keepdims=True)
        
        # Gradiente para propagar
        dX = np.dot(dZ, self.W.T)
        
        return dX
    
    def update(self, lr):
        """Actualizar par√°metros"""
        self.W -= lr * self.dW
        self.b -= lr * self.db


class TwoLayerNetwork:
    """Red neuronal de 2 capas con backpropagation"""
    
    def __init__(self, n_inputs, n_hidden, n_outputs):
        self.layer1 = Layer(n_inputs, n_hidden, activation='relu')
        self.layer2 = Layer(n_hidden, n_outputs, activation='sigmoid')
    
    def forward(self, X):
        """Forward pass a trav√©s de ambas capas"""
        A1 = self.layer1.forward(X)
        A2 = self.layer2.forward(A1)
        return A2
    
    def backward(self, dA2):
        """Backward pass a trav√©s de ambas capas"""
        dA1 = self.layer2.backward(dA2)
        dX = self.layer1.backward(dA1)
        return dX
    
    def update(self, lr):
        """Actualizar par√°metros de ambas capas"""
        self.layer1.update(lr)
        self.layer2.update(lr)
    
    def train_step(self, X, y, lr=0.01):
        """Un paso de entrenamiento completo"""
        # Forward
        y_pred = self.forward(X)
        
        # Calcular p√©rdida (Binary Cross-Entropy)
        m = y.shape[0]
        loss = -np.mean(y * np.log(y_pred + 1e-8) + (1-y) * np.log(1-y_pred + 1e-8))
        
        # Backward
        dA2 = y_pred - y  # Gradiente de BCE con sigmoid
        self.backward(dA2)
        
        # Update
        self.update(lr)
        
        return loss

# Ejemplo: Entrenar en XOR (¬°ahora s√≠ funciona!)
X_train = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])
y_train = np.array([[0], [1], [1], [0]])  # XOR

# Crear red
net = TwoLayerNetwork(n_inputs=2, n_hidden=4, n_outputs=1)

# Entrenar
print("Entrenando en XOR...")
for epoch in range(5000):
    loss = net.train_step(X_train, y_train, lr=0.5)
    
    if epoch % 500 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# Evaluar
print("\nResultados finales:")
predictions = net.forward(X_train)
for i, (x, y_true, y_pred) in enumerate(zip(X_train, y_train, predictions)):
    print(f"Input: {x}, Predicci√≥n: {y_pred[0]:.4f}, Real: {y_true[0]}")
```

### 3.2 Visualizaci√≥n de Gradientes

**¬øQu√© hacemos?** Medimos y graficamos la magnitud promedio de los gradientes en cada capa de la red para diagnosticar el estado del proceso de aprendizaje.

**¬øPor qu√© lo hacemos?** La magnitud del gradiente es un **indicador de salud** del entrenamiento. Nos dice cu√°nto est√° "aprendiendo" cada capa:

| Magnitud del gradiente | Diagn√≥stico | Causa probable |
|------------------------|-------------|----------------|
| `~1e-1` a `~1e-3` | ‚úÖ Saludable | Aprendizaje activo en todas las capas |
| `< 1e-7` (capas profundas) | ‚ö†Ô∏è Vanishing gradients | Activaciones saturadas (sigmoid/tanh), red muy profunda |
| `> 10` | ‚ö†Ô∏è Exploding gradients | Learning rate muy alto, pesos mal inicializados |
| `NaN` o `Inf` | ‚ùå Colapso num√©rico | Overflow, divisi√≥n por cero, log de negativo |

**Patr√≥n esperado en redes saludables:** Los gradientes deben ser **similares en magnitud** en todas las capas. Si la capa 1 tiene gradientes 1000 veces m√°s peque√±os que la capa 2, la red solo est√° aprendiendo en las capas cercanas a la salida, y las capas profundas est√°n pr√°cticamente congeladas.

**¬øC√≥mo lo hacemos?** Ejecutamos un forward+backward pass y luego inspeccionamos las magnitudes promedio de `dW` en cada capa usando `np.mean(np.abs(dW))`.

**¬øQu√© resultados esperar?** En una red bien inicializada con ReLU entrenando XOR, ambas capas deben mostrar gradientes no nulos de magnitud comparable, y estos deben decrecer suavemente a medida que la red converge.

Es √∫til visualizar c√≥mo fluyen los gradientes:

```python
import matplotlib.pyplot as plt

def visualize_gradients(network, X, y):
    """Visualiza magnitudes de gradientes en cada capa"""
    # Forward
    y_pred = network.forward(X)
    
    # Backward
    dA2 = y_pred - y
    network.backward(dA2)
    
    # Recopilar magnitudes de gradientes
    grad_layer1 = np.mean(np.abs(network.layer1.dW))
    grad_layer2 = np.mean(np.abs(network.layer2.dW))
    
    # Graficar
    layers = ['Layer 1', 'Layer 2']
    gradients = [grad_layer1, grad_layer2]
    
    plt.figure(figsize=(10, 5))
    plt.bar(layers, gradients, color=['blue', 'red'])
    plt.ylabel('Magnitud Promedio del Gradiente')
    plt.title('Flujo de Gradientes en la Red')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    print(f"Gradiente capa 1: {grad_layer1:.6f}")
    print(f"Gradiente capa 2: {grad_layer2:.6f}")

# Usar
visualize_gradients(net, X_train, y_train)
```

**Actividad 3.1:** Crea una red de 3 capas y entr√©nala en un dataset de clasificaci√≥n simple.

## üî¨ Parte 4: Verificaci√≥n de Gradientes (30 min)

### 4.1 Gradient Checking

**¬øQu√© hacemos?** Verificamos matem√°ticamente que nuestra implementaci√≥n anal√≠tica de backpropagation es correcta, compar√°ndola con gradientes calculados num√©ricamente.

**¬øPor qu√© lo hacemos?** Los bugs en backpropagation son insidiosos: la red puede seguir entrenando, la p√©rdida puede incluso bajar, pero los gradientes incorrectos llevan a un aprendizaje sub√≥ptimo o a fallos sutiles. El gradient checking es la √∫nica forma confiable de garantizar que la implementaci√≥n es correcta.

**El m√©todo de diferencias finitas centradas:**

La derivada en un punto `x` se puede aproximar num√©ricamente usando la f√≥rmula:

$$f'(x) \approx \frac{f(x + \varepsilon) - f(x - \varepsilon)}{2\varepsilon}$$

Esta es m√°s precisa que la diferencia hacia adelante `[f(x+Œµ) - f(x)] / Œµ` porque el error de aproximaci√≥n es `O(Œµ¬≤)` vs `O(Œµ)`. Para `Œµ = 1e-5`, el error es del orden de `1e-10`, mucho m√°s peque√±o que las diferencias que observar√≠amos en un bug real.

**La m√©trica de diferencia relativa:**

No comparamos la diferencia absoluta `|grad_anal√≠tico - grad_num√©rico|` porque los gradientes pueden tener magnitudes muy diferentes. Usamos:

$$\text{diferencia relativa} = \frac{\|g_{\text{anal√≠tico}} - g_{\text{num√©rico}}\|_2}{\|g_{\text{anal√≠tico}}\|_2 + \|g_{\text{num√©rico}}\|_2}$$

Interpretaci√≥n:
- `< 1e-7` ‚Üí ‚úÖ Implementaci√≥n correcta
- `1e-7` a `1e-5` ‚Üí ‚ö†Ô∏è Probablemente correcto (puede ser error num√©rico)
- `> 1e-5` ‚Üí ‚ùå Hay un bug en backpropagation

**¬øQu√© resultados esperar?** Con una implementaci√≥n correcta, la diferencia relativa debe ser menor a `1e-7`. Si introduces un bug intencional (como olvidar trasponer una matriz), la diferencia subir√° a `1e-3` o mayor.

La verificaci√≥n num√©rica de gradientes es CRUCIAL para asegurar que backpropagation est√© implementado correctamente.

```python
def numerical_gradient(f, x, epsilon=1e-5):
    """
    Calcula gradiente num√©rico usando diferencias finitas
    
    f: funci√≥n que toma x y retorna un escalar
    x: punto donde calcular el gradiente
    epsilon: peque√±o valor para la diferencia finita
    """
    grad = np.zeros_like(x)
    
    # Iterar sobre cada dimensi√≥n
    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    
    while not it.finished:
        idx = it.multi_index
        old_value = x[idx]
        
        # f(x + epsilon)
        x[idx] = old_value + epsilon
        fxplus = f(x)
        
        # f(x - epsilon)
        x[idx] = old_value - epsilon
        fxminus = f(x)
        
        # Gradiente num√©rico
        grad[idx] = (fxplus - fxminus) / (2 * epsilon)
        
        # Restaurar valor
        x[idx] = old_value
        it.iternext()
    
    return grad

def gradient_check(network, X, y, epsilon=1e-5):
    """
    Verifica que los gradientes anal√≠ticos coincidan con los num√©ricos
    """
    # Forward y backward para obtener gradientes anal√≠ticos
    y_pred = network.forward(X)
    loss_initial = -np.mean(y * np.log(y_pred + 1e-8) + (1-y) * np.log(1-y_pred + 1e-8))
    
    dA2 = y_pred - y
    network.backward(dA2)
    
    # Gradientes anal√≠ticos
    analytical_dW1 = network.layer1.dW.copy()
    analytical_dW2 = network.layer2.dW.copy()
    
    # Funci√≥n de p√©rdida para gradient checking
    def loss_function(params):
        # Desempaquetar par√°metros
        W1, b1, W2, b2 = params
        
        # Forward temporal
        A1 = np.maximum(0, np.dot(X, W1) + b1)
        A2 = 1 / (1 + np.exp(-(np.dot(A1, W2) + b2)))
        
        # P√©rdida
        loss = -np.mean(y * np.log(A2 + 1e-8) + (1-y) * np.log(1-A2 + 1e-8))
        return loss
    
    # Calcular gradiente num√©rico solo para W1 (por simplicidad)
    print("Verificando gradientes de W1...")
    
    numerical_dW1 = np.zeros_like(network.layer1.W)
    
    for i in range(network.layer1.W.shape[0]):
        for j in range(network.layer1.W.shape[1]):
            # Perturbar W1[i,j]
            network.layer1.W[i, j] += epsilon
            loss_plus = loss_function([network.layer1.W, network.layer1.b, 
                                      network.layer2.W, network.layer2.b])
            
            network.layer1.W[i, j] -= 2 * epsilon
            loss_minus = loss_function([network.layer1.W, network.layer1.b,
                                       network.layer2.W, network.layer2.b])
            
            # Gradiente num√©rico
            numerical_dW1[i, j] = (loss_plus - loss_minus) / (2 * epsilon)
            
            # Restaurar
            network.layer1.W[i, j] += epsilon
    
    # Comparar
    diff = np.linalg.norm(analytical_dW1 - numerical_dW1) / (
           np.linalg.norm(analytical_dW1) + np.linalg.norm(numerical_dW1))
    
    print(f"\nDiferencia relativa: {diff:.10f}")
    
    if diff < 1e-7:
        print("‚úì ¬°Gradientes correctos!")
    elif diff < 1e-5:
        print("‚ö† Gradientes probablemente correctos (diferencia peque√±a)")
    else:
        print("‚úó ERROR: Gradientes incorrectos")
    
    return diff

# Ejecutar gradient checking
print("=== GRADIENT CHECKING ===")
difference = gradient_check(net, X_train, y_train)
```

### 4.2 Consejos para Debugging

**¬øQu√© hacemos?** Aplicamos una estrategia sistem√°tica para encontrar y corregir bugs en implementaciones de backpropagation.

**¬øPor qu√© lo hacemos?** Backpropagation tiene varios puntos de falla comunes que son dif√≠ciles de detectar a simple vista porque el c√≥digo puede ejecutarse sin errores pero producir gradientes incorrectos. Conocer los bugs m√°s frecuentes acelera enormemente el proceso de depuraci√≥n.

**Los bugs m√°s comunes en backpropagation:**

1. **Transpuesta incorrecta:** En capas densas, `dW = X.T @ dZ` (no `X @ dZ`). Un fallo de dimensiones a veces se "resuelve" transponi√©ndola en el lugar equivocado.

2. **No dividir por batch size:** Los gradientes deben promediar sobre el batch: `dW = (1/m) * X.T @ dZ`. Sin este factor, el learning rate efectivo escala con el tama√±o del batch.

3. **No sumar sobre el batch en bias:** `db = (1/m) * np.sum(dZ, axis=0)`. Olvidar el `sum` produce dimensiones incorrectas o gradientes escalonados.

4. **Confundir `*` y `@`:** `*` es elemento-a-elemento (Hadamard), `@` es multiplicaci√≥n matricial. Intercambiarlos produce resultados con dimensiones incorrectas o incorrectos silenciosamente.

5. **No guardar el cach√©:** Si en el forward pass no guardas `x` o `z`, no puedes calcular los gradientes correctos en el backward pass.

6. **Olvidar el gradiente de la activaci√≥n:** Para una capa con ReLU, el backward pass es `dZ = dA * relu_prime(Z)`, no simplemente `dZ = dA`.

**Estrategia de debugging recomendada:**
1. Verifica las **formas (shapes)** de todos los tensores en forward y backward
2. Comprueba que no hay `NaN` ni `Inf` en ning√∫n punto
3. Ejecuta gradient checking con un batch peque√±o (4-8 ejemplos)
4. Si falla, a√≠sla la capa problem√°tica verificando capa por capa
5. Usa prints de la magnitud media (`np.mean(np.abs(tensor))`) para detectar valores an√≥malos

**¬øQu√© resultados esperar?** La herramienta de debugging debe mostrar formas consistentes, valores sin NaN/Inf, y magnitudes de gradiente en rango razonable (ni cercanas a 0 ni superiores a 10 en las primeras iteraciones).

```python
def debug_backprop(network, X, y):
    """Herramienta de debugging para backpropagation"""
    
    print("=== DEBUG BACKPROPAGATION ===\n")
    
    # Forward
    print("1. FORWARD PASS")
    A1 = network.layer1.forward(X)
    A2 = network.layer2.forward(A1)
    print(f"   Salida capa 1: shape={A1.shape}, min={A1.min():.4f}, max={A1.max():.4f}")
    print(f"   Salida capa 2: shape={A2.shape}, min={A2.min():.4f}, max={A2.max():.4f}")
    
    # P√©rdida
    loss = -np.mean(y * np.log(A2 + 1e-8) + (1-y) * np.log(1-A2 + 1e-8))
    print(f"   P√©rdida: {loss:.6f}")
    
    # Backward
    print("\n2. BACKWARD PASS")
    dA2 = A2 - y
    print(f"   Gradiente inicial (dA2): shape={dA2.shape}, mean={np.mean(np.abs(dA2)):.6f}")
    
    dA1 = network.layer2.backward(dA2)
    print(f"   Gradiente capa 2 -> 1 (dA1): shape={dA1.shape}, mean={np.mean(np.abs(dA1)):.6f}")
    print(f"   Gradiente W2: mean={np.mean(np.abs(network.layer2.dW)):.6f}")
    
    dX = network.layer1.backward(dA1)
    print(f"   Gradiente capa 1 -> entrada: mean={np.mean(np.abs(dX)):.6f}")
    print(f"   Gradiente W1: mean={np.mean(np.abs(network.layer1.dW)):.6f}")
    
    # Verificar NaN o Inf
    print("\n3. VERIFICACIONES")
    has_nan = np.isnan(network.layer1.dW).any() or np.isnan(network.layer2.dW).any()
    has_inf = np.isinf(network.layer1.dW).any() or np.isinf(network.layer2.dW).any()
    
    if has_nan:
        print("   ‚úó ¬°ADVERTENCIA! Gradientes contienen NaN")
    if has_inf:
        print("   ‚úó ¬°ADVERTENCIA! Gradientes contienen Inf")
    if not has_nan and not has_inf:
        print("   ‚úì Gradientes son valores num√©ricos v√°lidos")

# Usar
debug_backprop(net, X_train, y_train)
```

**Actividad 4.1:** Introduce un bug intencional en tu c√≥digo de backpropagation y usa gradient checking para encontrarlo.

## üìä An√°lisis Final de Rendimiento

### Comparaci√≥n: Antes vs Despu√©s de Backpropagation

**¬øQu√© hacemos?** Comparamos el comportamiento de una red con pesos aleatorios (sin entrenar) contra la misma red despu√©s del proceso de backpropagation + descenso por gradiente.

**¬øPor qu√© lo hacemos?** Esta comparaci√≥n cuantifica directamente el **valor del aprendizaje**: transforma una funci√≥n aleatoria e in√∫til en una funci√≥n que modela correctamente el patr√≥n en los datos. Las m√©tricas que observamos son:

- **P√©rdida inicial vs final**: Mide cu√°nto mejor√≥ la funci√≥n de predicci√≥n. Una reducci√≥n del 95%+ indica aprendizaje exitoso.
- **Predicciones antes/despu√©s**: Confirma que la red pas√≥ de respuestas aleatorias a respuestas correctas.
- **Curva de aprendizaje**: Revela la din√°mica del entrenamiento ‚Äî ¬øbaja suavemente? ¬øtiene mesetas? ¬øoscila?

**Interpretaci√≥n de la curva de aprendizaje:**

| Forma de la curva | Diagn√≥stico |
|-------------------|-------------|
| Descenso suave y estable | ‚úÖ Learning rate adecuado |
| Descenso en "escalones" (plateaus) | Posible m√≠nimo local o learning rate muy peque√±o |
| Oscilaciones grandes | Learning rate demasiado alto |
| Descenso r√°pido inicial, luego estancamiento | Red convergiendo a un m√≠nimo, puede necesitar m√°s capacidad |
| P√©rdida constante (no baja) | Bug en backpropagation o arquitectura insuficiente |

**¬øQu√© resultados esperar?** Para XOR con la arquitectura 2‚Üí4‚Üí1, esperamos:
- P√©rdida inicial: ~0.25 (equivalente a predicciones aleatorias para clasificaci√≥n binaria)
- P√©rdida final: < 0.01 despu√©s de ~5000 √©pocas
- Mejora porcentual: > 95%

```python
# Sin entrenar (pesos aleatorios)
net_untrained = TwoLayerNetwork(n_inputs=2, n_hidden=4, n_outputs=1)
pred_before = net_untrained.forward(X_train)

# Entrenar
net_trained = TwoLayerNetwork(n_inputs=2, n_hidden=4, n_outputs=1)
losses = []

for epoch in range(5000):
    loss = net_trained.train_step(X_train, y_train, lr=0.5)
    losses.append(loss)

pred_after = net_trained.forward(X_train)

# Comparar
print("=== ANTES DEL ENTRENAMIENTO ===")
for i, (x, y_true, y_pred) in enumerate(zip(X_train, y_train, pred_before)):
    print(f"Input: {x}, Pred: {y_pred[0]:.4f}, Real: {y_true[0]}")

print("\n=== DESPU√âS DEL ENTRENAMIENTO ===")
for i, (x, y_true, y_pred) in enumerate(zip(X_train, y_train, pred_after)):
    print(f"Input: {x}, Pred: {y_pred[0]:.4f}, Real: {y_true[0]}")

# Graficar curva de aprendizaje
plt.figure(figsize=(10, 5))
plt.plot(losses)
plt.xlabel('√âpoca')
plt.ylabel('P√©rdida')
plt.title('Curva de Aprendizaje - Backpropagation en Acci√≥n')
plt.grid(True, alpha=0.3)
plt.show()

print(f"\nP√©rdida inicial: {losses[0]:.6f}")
print(f"P√©rdida final: {losses[-1]:.6f}")
print(f"Mejora: {(1 - losses[-1]/losses[0]) * 100:.2f}%")
```

### Problemas Comunes y Soluciones

```python
class BackpropDiagnostics:
    """Herramientas para diagnosticar problemas en backpropagation"""
    
    @staticmethod
    def check_vanishing_gradients(network, threshold=1e-7):
        """Detecta gradientes que desaparecen"""
        grad1 = np.mean(np.abs(network.layer1.dW))
        grad2 = np.mean(np.abs(network.layer2.dW))
        
        print("=== DIAGN√ìSTICO: VANISHING GRADIENTS ===")
        print(f"Gradiente promedio capa 1: {grad1:.10f}")
        print(f"Gradiente promedio capa 2: {grad2:.10f}")
        
        if grad1 < threshold or grad2 < threshold:
            print("‚ö† ¬°ADVERTENCIA! Posible vanishing gradients")
            print("Soluciones:")
            print("  - Usar ReLU en lugar de sigmoid/tanh")
            print("  - Reducir n√∫mero de capas")
            print("  - Usar batch normalization")
            print("  - Mejor inicializaci√≥n de pesos")
        else:
            print("‚úì Gradientes en rango saludable")
    
    @staticmethod
    def check_exploding_gradients(network, threshold=1.0):
        """Detecta gradientes que explotan"""
        grad1 = np.mean(np.abs(network.layer1.dW))
        grad2 = np.mean(np.abs(network.layer2.dW))
        
        print("\n=== DIAGN√ìSTICO: EXPLODING GRADIENTS ===")
        
        if grad1 > threshold or grad2 > threshold:
            print("‚ö† ¬°ADVERTENCIA! Posible exploding gradients")
            print("Soluciones:")
            print("  - Reducir learning rate")
            print("  - Usar gradient clipping")
            print("  - Mejor inicializaci√≥n de pesos")
        else:
            print("‚úì Gradientes bajo control")

# Usar diagn√≥sticos
diag = BackpropDiagnostics()
diag.check_vanishing_gradients(net_trained)
diag.check_exploding_gradients(net_trained)
```

## üéØ EJERCICIOS PROPUESTOS

### Nivel B√°sico

**Ejercicio 1:** Implementaci√≥n de Operaciones B√°sicas
```
Implementa una clase para cada operaci√≥n (suma, multiplicaci√≥n, divisi√≥n)
con m√©todos forward() y backward(). Verifica con gradient checking.
```

**Ejercicio 2:** Grafo Computacional Manual
```
Para la funci√≥n f(x,y,z) = (x + y) * z:
a) Dibuja el grafo computacional
b) Calcula el forward pass con x=2, y=3, z=4
c) Calcula el backward pass manualmente
d) Verifica con c√≥digo
```

**Ejercicio 3:** Funciones de Activaci√≥n
```
Implementa forward y backward para:
- Sigmoid
- Tanh
- Leaky ReLU (con Œ±=0.01)
Verifica cada una con gradient checking.
```

### Nivel Intermedio

**Ejercicio 4:** Red de 3 Capas
```
Implementa una red con arquitectura: 4 ‚Üí 8 ‚Üí 4 ‚Üí 2
- Usa ReLU en capas ocultas, softmax en salida
- Entrena en un dataset de clasificaci√≥n multiclase
- Visualiza la evoluci√≥n de los gradientes
```

**Ejercicio 5:** Regularizaci√≥n L2
```
Agrega regularizaci√≥n L2 a tu red:
- Modifica la funci√≥n de p√©rdida: L = L_data + Œª||W||¬≤
- Implementa el gradiente correspondiente
- Compara resultados con y sin regularizaci√≥n
```

**Ejercicio 6:** Mini-batch SGD
```
Implementa entrenamiento con mini-batches:
- Divide los datos en batches de tama√±o 32
- Implementa un epoch completo iterando sobre batches
- Compara tiempo de entrenamiento vs batch completo
```

### Nivel Avanzado

**Ejercicio 7:** Arquitectura Profunda
```
Crea una red de 5+ capas:
- Implementa desde cero (no usar frameworks)
- Entrena en MNIST
- Diagnostica y soluciona vanishing gradients
- Usa diferentes inicializaciones (He, Xavier)
```

**Ejercicio 8:** Gradient Checking Completo
```
Implementa gradient checking para:
- Todos los par√°metros (W y b) de todas las capas
- Diferentes funciones de p√©rdida
- Crea un informe detallado de diferencias
```

**Ejercicio 9:** Optimizador con Momentum
```
Implementa backpropagation con momentum:
- v = Œ≤*v + (1-Œ≤)*gradiente
- W = W - Œ±*v
- Compara convergencia con SGD vanilla
```

## üìù Entregables

### 1. C√≥digo Fuente
- `backprop.py`: Implementaci√≥n de backpropagation
- `layers.py`: Clases de capas con forward/backward
- `network.py`: Red neuronal completa
- `gradient_check.py`: Verificaci√≥n de gradientes
- `experiments.ipynb`: Notebook con experimentos

### 2. Documentaci√≥n
- README explicando tu implementaci√≥n
- Comentarios detallados en el c√≥digo
- Diagramas de grafos computacionales

### 3. Resultados
- Curvas de aprendizaje
- Resultados de gradient checking
- Comparaci√≥n de diferentes configuraciones
- An√°lisis de errores

### 4. Reporte T√©cnico (2-3 p√°ginas)
Incluir:
- Explicaci√≥n de tu implementaci√≥n
- Decisiones de dise√±o
- Resultados experimentales
- Dificultades encontradas y soluciones
- Conclusiones

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Conceive (Concebir) - 25%
- [ ] Comprensi√≥n profunda de la regla de la cadena
- [ ] Identificaci√≥n correcta de gradientes necesarios
- [ ] Dise√±o apropiado de la arquitectura de c√≥digo
- [ ] Planificaci√≥n de estrategia de verificaci√≥n

### Design (Dise√±ar) - 25%
- [ ] Implementaci√≥n correcta del algoritmo de backpropagation
- [ ] C√≥digo modular y reutilizable
- [ ] Manejo apropiado de dimensiones matriciales
- [ ] Implementaci√≥n eficiente (vectorizaci√≥n)

### Implement (Implementar) - 30%
- [ ] C√≥digo funcional sin errores
- [ ] Gradient checking pasa (diferencia < 1e-7)
- [ ] Red neuronal entrena correctamente
- [ ] Resultados reproducibles

### Operate (Operar) - 20%
- [ ] Experimentaci√≥n con diferentes configuraciones
- [ ] An√°lisis cr√≠tico de resultados
- [ ] Identificaci√≥n y soluci√≥n de problemas
- [ ] Documentaci√≥n clara y completa

## üìã R√∫brica de Evaluaci√≥n

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|--------------|---------------------|------------------|
| **Implementaci√≥n** | Backprop perfecto, gradient check < 1e-9 | Backprop correcto, < 1e-7 | Backprop funciona, < 1e-5 | Errores en implementaci√≥n |
| **Comprensi√≥n** | Explica detalladamente cada paso | Explica conceptos principales | Explica parcialmente | Comprensi√≥n limitada |
| **C√≥digo** | Muy limpio, modular, documentado | Bien estructurado | Funcional pero b√°sico | Desorganizado o con errores |
| **Experimentos** | An√°lisis profundo, m√∫ltiples experimentos | Buenos experimentos | Experimentos b√°sicos | Experimentos insuficientes |
| **Documentaci√≥n** | Excelente, clara, completa | Buena documentaci√≥n | Documentaci√≥n b√°sica | Documentaci√≥n pobre |

## üìö Referencias Adicionales

### Art√≠culos Fundamentales
1. Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). "Learning representations by back-propagating errors". Nature.
2. LeCun, Y., et al. (1998). "Gradient-based learning applied to document recognition"
3. Glorot, X., & Bengio, Y. (2010). "Understanding the difficulty of training deep feedforward neural networks"

### Recursos Online
- **CS231n Stanford**: http://cs231n.stanford.edu/ (especialmente m√≥dulo sobre backpropagation)
- **Deep Learning Book** (Goodfellow): Cap√≠tulo 6 - Deep Feedforward Networks
- **3Blue1Brown**: Serie de videos sobre backpropagation (muy visual)
- **Andrej Karpathy**: "Yes you should understand backprop" (blog post)

### Herramientas
- NumPy documentation: https://numpy.org/doc/
- Matplotlib para visualizaci√≥n
- autograd (para verificar gradientes autom√°ticamente)

### Papers Adicionales
- "Delving Deep into Rectifiers" (He et al., 2015) - Sobre inicializaci√≥n
- "Batch Normalization" (Ioffe & Szegedy, 2015)
- "Understanding the difficulty of training deep feedforward neural networks" (Glorot & Bengio, 2010)

## üéì Notas Finales

### Consejos para el √âxito

1. **Siempre verifica tus gradientes**: Gradient checking es tu mejor amigo. Un error peque√±o en backprop puede arruinar todo el entrenamiento.

2. **Dibuja grafos computacionales**: Antes de programar, dibuja el grafo. Te ayudar√° a visualizar el flujo de gradientes.

3. **Empieza simple**: Implementa y verifica operaciones simples antes de construir redes complejas.

4. **Usa dimensiones expl√≠citas**: Siempre conoce las dimensiones de tus tensores. Muchos bugs vienen de errores de dimensi√≥n.

5. **Guarda valores intermedios**: En forward pass, guarda todo lo que necesitar√°s en backward pass.

### Errores Comunes

‚ùå **Olvidar transponer matrices**: `dW = X.T @ dZ` (no `X @ dZ`)
‚ùå **No sumar gradientes**: Al calcular `db`, hay que sumar sobre el batch
‚ùå **Confundir `*` y `@`**: `*` es elemento-wise, `@` es producto matricial
‚ùå **No inicializar gradientes a cero**: Acumular gradientes sin limpiar
‚ùå **Gradient checking con batch grande**: Usa batches peque√±os para verificar

### Reflexi√≥n Final

**Backpropagation es el coraz√≥n del deep learning**. Es la diferencia entre "redes neuronales interesantes en teor√≠a" y "redes neuronales que revolucionan la tecnolog√≠a".

Dominar backpropagation te da:
- Comprensi√≥n profunda de c√≥mo aprenden las redes
- Capacidad de debuggear problemas de entrenamiento
- Habilidad para implementar arquitecturas personalizadas
- Base s√≥lida para frameworks modernos (PyTorch, TensorFlow)

**Un mensaje importante**: Una vez que entiendas backpropagation a fondo, probablemente nunca lo implementar√°s manualmente otra vez. Los frameworks modernos lo hacen autom√°ticamente. Pero ese conocimiento profundo te har√° un practicante mucho mejor de deep learning.

### Pr√≥ximos Pasos

En el siguiente laboratorio (Lab 06), usaremos backpropagation para:
- Implementar loops de entrenamiento completos
- Trabajar con datasets reales
- Implementar t√©cnicas avanzadas (early stopping, regularizaci√≥n)
- Optimizar el rendimiento del entrenamiento

¬°Ahora tienes la herramienta m√°s poderosa en deep learning. √ösala sabiamente! üöÄ

---

**"Understanding backpropagation is the difference between being a machine learning user and a machine learning practitioner."** - Andrej Karpathy

**¬°Backpropagation es la magia del deep learning! ‚ú®**
