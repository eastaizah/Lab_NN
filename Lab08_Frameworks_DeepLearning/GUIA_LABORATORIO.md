# Gu√≠a de Laboratorio: Frameworks de Deep Learning

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Fundamentos de Deep Learning - Frameworks Modernos  
**C√≥digo:** Lab 08  
**Duraci√≥n:** 3-4 horas  
**Nivel:** Intermedio-Avanzado  

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender las ventajas de usar frameworks de deep learning
2. Implementar redes neuronales en PyTorch desde cero
3. Implementar redes neuronales en TensorFlow/Keras
4. Utilizar diferenciaci√≥n autom√°tica (Autograd)
5. Aprovechar aceleraci√≥n con GPU
6. Comparar PyTorch y TensorFlow en casos pr√°cticos
7. Migrar c√≥digo desde implementaciones NumPy a frameworks
8. Usar utilidades modernas (DataLoaders, Optimizers, etc.)
9. Entrenar modelos de manera eficiente
10. Guardar, cargar y desplegar modelos

## üìö Prerrequisitos

### Conocimientos

- Python avanzado (POO, decoradores)
- NumPy s√≥lido (todos los labs anteriores)
- Redes neuronales, backpropagation, entrenamiento
- Conceptos de GPU computing (b√°sicos)

### Software

- Python 3.8+
- PyTorch 1.9+ (`pip install torch torchvision`)
- TensorFlow 2.6+ (`pip install tensorflow`)
- NumPy, Matplotlib
- CUDA (opcional, para GPU)

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` - Comparaci√≥n de frameworks
- `README.md` - Recursos y estructura
- Documentaci√≥n oficial de PyTorch y TensorFlow

## üìñ Introducci√≥n

### Del C√≥digo Manual a los Frameworks

Felicidades! Has llegado lejos implementando todo desde cero:
- ‚úì Neuronas y capas (Lab 01)
- ‚úì Funciones de activaci√≥n (Lab 03)
- ‚úì Funciones de p√©rdida (Lab 04)
- ‚úì Backpropagation (Lab 05)
- ‚úì Entrenamiento completo (Lab 06)
- ‚úì Evaluaci√≥n rigurosa (Lab 07)

**Ahora es tiempo de usar las herramientas profesionales.**

### ¬øPor Qu√© Usar Frameworks?

**Sin frameworks (lo que has hecho):**
```python
# Implementar forward pass
z = np.dot(W, x) + b
a = sigmoid(z)

# Implementar backward pass
dz = a - y
dW = np.dot(dz, x.T)
db = np.sum(dz)

# Actualizar manualmente
W -= learning_rate * dW
b -= learning_rate * db
```

**Con frameworks:**
```python
# PyTorch hace todo autom√°ticamente
output = model(x)
loss = criterion(output, y)
loss.backward()  # ¬°Gradientes autom√°ticos!
optimizer.step()  # ¬°Actualizaci√≥n autom√°tica!
```

### Ventajas Principales

**1. Autograd (Diferenciaci√≥n Autom√°tica)**
- No m√°s backpropagation manual
- Sin errores en derivadas
- Soporta operaciones complejas

**2. Optimizaci√≥n de Performance**
- Operaciones optimizadas en C++/CUDA
- Paralelizaci√≥n autom√°tica
- 10-100x m√°s r√°pido que NumPy

**3. GPU Acceleration**
```python
# Mover a GPU (una l√≠nea!)
model = model.to('cuda')
```

**4. Ecosistema Rico**
- Modelos pre-entrenados (ResNet, BERT, GPT)
- Data loaders optimizados
- Herramientas de visualizaci√≥n
- Comunidad masiva

**5. Productizaci√≥n**
- Guardar/cargar modelos f√°cilmente
- Desplegar en servidores
- Exportar a m√≥viles (TF Lite)
- Optimizar para inferencia

### PyTorch vs TensorFlow

| Caracter√≠stica | PyTorch | TensorFlow/Keras |
|---------------|---------|------------------|
| **Filosof√≠a** | Investigaci√≥n, Pyth√≥nico | Producci√≥n, Escalable |
| **Curva de aprendizaje** | M√°s f√°cil | Media (Keras f√°cil) |
| **Debugging** | Excelente | Bueno |
| **Popularidad investigaci√≥n** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Popularidad industria** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Documentaci√≥n** | Excelente | Excelente |
| **Despliegue** | Mejorando | Excelente |

**Recomendaci√≥n:** Aprende ambos! Son las herramientas est√°ndar de la industria.

### Aplicaciones en el Mundo Real

**Todos los modelos modernos usan frameworks:**
- **GPT-3/4**: Entrenado con frameworks
- **Stable Diffusion**: PyTorch
- **BERT, T5**: TensorFlow
- **AlphaGo**: TensorFlow
- **DALL-E**: PyTorch
- **99% de papers de investigaci√≥n**: PyTorch o TensorFlow

## ü§î Preguntas de Reflexi√≥n Iniciales

1. ¬øPor qu√© no seguir implementando todo manualmente?
2. ¬øQu√© significa "diferenciaci√≥n autom√°tica"?
3. ¬øC√≥mo puede un framework ser 100x m√°s r√°pido?
4. ¬øCu√°l es el trade-off entre control y conveniencia?
5. ¬øPyTorch o TensorFlow para tu proyecto?

## üî¨ Parte 1: PyTorch Fundamentals (60 min)

### 1.1 Tensores: Los Bloques B√°sicos

Los tensores son la estructura de datos fundamental de PyTorch y de todo el deep learning moderno. Antes de construir redes neuronales, es imprescindible dominar este bloque esencial.

**¬øQu√© es un tensor?**

Un tensor es una generalizaci√≥n matem√°tica de escalares, vectores y matrices a dimensiones arbitrarias:

| Dimensiones | Nombre matem√°tico | Ejemplo en deep learning |
|---|---|---|
| 0D | Escalar | Un valor de p√©rdida: `loss = 0.42` |
| 1D | Vector | Un ejemplo: `[x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]` |
| 2D | Matriz | Un batch de datos: `(batch_size √ó features)` |
| 3D | Tensor orden 3 | Imagen en escala de grises por batch: `(batch √ó alto √ó ancho)` |
| 4D | Tensor orden 4 | Batch de im√°genes a color: `(batch √ó canales √ó alto √ó ancho)` |

**¬øPor qu√© tensores y no arrays NumPy?**

Los tensores de PyTorch son casi id√©nticos a los `ndarray` de NumPy, pero a√±aden dos capacidades cr√≠ticas que NumPy no tiene:

1. **Ejecuci√≥n en GPU**: Las operaciones con tensores pueden ejecutarse en NVIDIA GPUs con CUDA, logrando aceleraciones de 10√ó‚Äì100√ó para modelos grandes.
2. **Grafo computacional autom√°tico**: Cuando `requires_grad=True`, PyTorch registra cada operaci√≥n sobre el tensor y construye din√°micamente un **grafo de c√≥mputo**. Este grafo es la base de `autograd` (secci√≥n siguiente), que calcula gradientes autom√°ticamente.

**¬øC√≥mo funciona la interoperabilidad con NumPy?**

PyTorch y NumPy pueden **compartir memoria** mediante `torch.from_numpy()`: modificar el tensor modifica el array original y viceversa. Para evitar esto, usa `.clone()` para obtener una copia independiente.

**¬øQu√© resultados debes esperar?**

El c√≥digo a continuaci√≥n crea tensores de distintas formas, realiza operaciones matem√°ticas b√°sicas (suma, producto elemento a elemento, producto punto, multiplicaci√≥n matricial) y demuestra la conversi√≥n fluida entre NumPy y PyTorch.

```python
import torch
import numpy as np

print("=== TENSORES EN PYTORCH ===\n")

# Crear tensores
x = torch.tensor([1, 2, 3, 4])
print(f"Tensor 1D: {x}")
print(f"Shape: {x.shape}, dtype: {x.dtype}\n")

# Tensor 2D
matrix = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float32)
print(f"Tensor 2D:\n{matrix}")
print(f"Shape: {matrix.shape}\n")

# Tensores especiales
zeros = torch.zeros(3, 4)
ones = torch.ones(2, 3)
randn = torch.randn(3, 3)  # Distribuci√≥n normal
rand = torch.rand(2, 2)     # Distribuci√≥n uniforme [0, 1]

print(f"Zeros:\n{zeros}\n")
print(f"Random normal:\n{randn}\n")

# Conversi√≥n NumPy ‚Üî PyTorch
np_array = np.array([1, 2, 3])
torch_tensor = torch.from_numpy(np_array)
back_to_numpy = torch_tensor.numpy()

print(f"NumPy ‚Üí Torch ‚Üí NumPy: {back_to_numpy}\n")

# Operaciones b√°sicas
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

print(f"Suma: {a + b}")
print(f"Producto elemento-wise: {a * b}")
print(f"Producto punto: {torch.dot(a, b)}")
print(f"Matriz @ vector: {torch.randn(3, 4) @ torch.randn(4)}")
```

**Salida esperada:**
```
Tensor 1D: tensor([1, 2, 3, 4])
Shape: torch.Size([4]), dtype: torch.int64

Tensor 2D:
tensor([[1., 2.],
        [3., 4.],
        [5., 6.]])
Shape: torch.Size([3, 2])

Zeros:
tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])

NumPy ‚Üí Torch ‚Üí NumPy: [1 2 3]

Suma: tensor([5., 7., 9.])
Producto elemento-wise: tensor([ 4., 10., 18.])
Producto punto: tensor(32.)
```

> üí° **Tip:** Un tensor creado con `torch.from_numpy()` comparte memoria con el array NumPy original. Si modificas uno, el otro cambia. Usa `tensor.clone()` o `tensor.detach().clone()` para obtener una copia completamente independiente.

### 1.2 Autograd: El Coraz√≥n de PyTorch

Autograd es el sistema de **diferenciaci√≥n autom√°tica** de PyTorch y es la raz√≥n por la que los frameworks eliminan la necesidad de implementar backpropagation manualmente.

**¬øQu√© es la diferenciaci√≥n autom√°tica?**

La diferenciaci√≥n autom√°tica (AD) es una t√©cnica computacional que calcula derivadas exactas de funciones definidas como programas. No es diferenciaci√≥n simb√≥lica (como SymPy) ni diferenciaci√≥n num√©rica (como diferencias finitas): es algo m√°s eficiente y preciso que ambas.

**¬øC√≥mo funciona el grafo computacional?**

Cada vez que realizas una operaci√≥n sobre tensores con `requires_grad=True`, PyTorch construye din√°micamente un **grafo computacional dirigido ac√≠clico (DAG)**:

```
x ‚îÄ‚îÄ‚Üí (operaci√≥n) ‚îÄ‚îÄ‚Üí y ‚îÄ‚îÄ‚Üí (operaci√≥n) ‚îÄ‚îÄ‚Üí loss
        ‚Üë                       ‚Üë
   registra                registra
   gradiente               gradiente
```

Cuando llamas a `loss.backward()`, PyTorch recorre el grafo **en sentido inverso** aplicando la **regla de la cadena** (chain rule) autom√°ticamente para calcular el gradiente de `loss` respecto a cada par√°metro con `requires_grad=True`.

**La regla de la cadena en PyTorch:**

Para una composici√≥n de funciones `L = f(g(h(x)))`:

```
‚àÇL/‚àÇx = (‚àÇL/‚àÇf) ¬∑ (‚àÇf/‚àÇg) ¬∑ (‚àÇg/‚àÇh) ¬∑ (‚àÇh/‚àÇx)
```

PyTorch calcula y acumula esto autom√°ticamente al llamar `.backward()`.

**¬øPor qu√© elimina el backpropagation manual?**

En los laboratorios anteriores (Lab 05), implementaste backpropagation calculando manualmente cada derivada parcial. Con autograd:
- **No necesitas derivar f√≥rmulas** para cada nueva arquitectura
- **No hay errores de derivaci√≥n** (gradientes siempre correctos)
- **Soporta operaciones complejas**: convoluciones, atenci√≥n, operaciones personalizadas

**¬øQu√© resultados debes esperar?**

El c√≥digo calcula gradientes de funciones simples y verifica que coincidan con los valores anal√≠ticos: para `y = x¬≤` con `x = 3`, el gradiente es `dy/dx = 2x = 6`.

```python
print("\n=== AUTOGRAD: DIFERENCIACI√ìN AUTOM√ÅTICA ===\n")

# Ejemplo 1: Derivada simple
x = torch.tensor(3.0, requires_grad=True)  # Activar tracking de gradientes
print(f"x = {x}")

# Forward: y = x¬≤
y = x ** 2
print(f"y = x¬≤ = {y}")

# Backward: calcular dy/dx
y.backward()
print(f"dy/dx = 2x = {x.grad}")  # Deber√≠a ser 2*3 = 6

print("\n--- Ejemplo 2: Funci√≥n m√°s compleja ---")

# Reset
x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

# z = w*x + b
z = w * x + b
print(f"z = w*x + b = {z}")

# Backward
z.backward()

print(f"‚àÇz/‚àÇx = w = {x.grad}")  # = w = 3
print(f"‚àÇz/‚àÇw = x = {w.grad}")  # = x = 2
print(f"‚àÇz/‚àÇb = 1 = {b.grad}")  # = 1

print("\n--- Ejemplo 3: Red neuronal simple ---")

# Input
x = torch.randn(1, 10, requires_grad=True)
print(f"Input shape: {x.shape}")

# Par√°metros
W1 = torch.randn(10, 5, requires_grad=True)
b1 = torch.randn(1, 5, requires_grad=True)

W2 = torch.randn(5, 1, requires_grad=True)
b2 = torch.randn(1, 1, requires_grad=True)

# Forward
h = torch.relu(x @ W1 + b1)  # Capa oculta
y = h @ W2 + b2               # Salida

# Simular p√©rdida
target = torch.tensor([[1.0]])
loss = (y - target) ** 2

print(f"Loss: {loss.item():.4f}")

# Backward: ¬°calcula TODOS los gradientes autom√°ticamente!
loss.backward()

print(f"Gradiente de W1: {W1.grad.shape}")  # (10, 5)
print(f"Gradiente de W2: {W2.grad.shape}")  # (5, 1)
print("‚úì Gradientes calculados autom√°ticamente!")
```

**Salida esperada:**
```
=== AUTOGRAD: DIFERENCIACI√ìN AUTOM√ÅTICA ===

x = 3.0
y = x¬≤ = 9.0
dy/dx = 2x = 6.0

--- Ejemplo 2: Funci√≥n m√°s compleja ---
z = w*x + b = 8.0
‚àÇz/‚àÇx = w = 3.0
‚àÇz/‚àÇw = x = 2.0
‚àÇz/‚àÇb = 1 = 1.0

--- Ejemplo 3: Red neuronal simple ---
Input shape: torch.Size([1, 10])
Loss: (valor variable)
Gradiente de W1: torch.Size([10, 5])
Gradiente de W2: torch.Size([5, 1])
‚úì Gradientes calculados autom√°ticamente!
```

> üí° **Tip:** Recuerda llamar `optimizer.zero_grad()` antes de cada `backward()`. PyTorch **acumula** gradientes por defecto (en lugar de reemplazarlos). Si no limpias los gradientes, obtendr√°s sumas de gradientes de iteraciones anteriores, lo que corrompe el entrenamiento.

### 1.3 Primera Red Neuronal en PyTorch

PyTorch define redes neuronales mediante el paradigma de **programaci√≥n orientada a objetos (POO)**. La clase `nn.Module` es la clase base de todos los modelos en PyTorch.

**¬øQu√© es `nn.Module`?**

`nn.Module` es la clase base que provee toda la infraestructura necesaria para una red neuronal:
- Registro autom√°tico de par√°metros entrenables (`nn.Parameter`)
- M√©todo `.parameters()` para iterar sobre todos los pesos
- M√©todos `.train()` y `.eval()` para cambiar el comportamiento de capas como Dropout y BatchNorm
- Serializaci√≥n para guardar y cargar modelos

**El paradigma PyTorch: `__init__` + `forward`**

```python
class MiRed(nn.Module):
    def __init__(self):          # 1. Define las CAPAS (estructura)
        super().__init__()
        self.capa1 = nn.Linear(...)
    
    def forward(self, x):        # 2. Define el FLUJO de datos
        return self.capa1(x)
```

- **`__init__`**: Se llama **una vez** al crear el modelo. Aqu√≠ defines las capas y sus par√°metros.
- **`forward`**: Se llama **cada vez** que pasas datos por el modelo. Define c√≥mo fluyen los datos de entrada a salida.

> El m√©todo `backward()` **no** se define manualmente: autograd lo construye autom√°ticamente a partir del grafo generado por `forward()`.

**El optimizador SGD:**

El **Descenso de Gradiente Estoc√°stico (SGD)** actualiza cada par√°metro Œ∏ seg√∫n:

```
Œ∏ ‚Üê Œ∏ - lr ¬∑ ‚àÇL/‚àÇŒ∏
```

Donde `lr` es la tasa de aprendizaje. En cada iteraci√≥n del entrenamiento el ciclo es:
1. `optimizer.zero_grad()` ‚Üí Limpiar gradientes acumulados
2. `loss.backward()` ‚Üí Calcular gradientes (autograd)
3. `optimizer.step()` ‚Üí Aplicar la actualizaci√≥n

**¬øQu√© resultados debes esperar?**

Ver√°s la arquitectura del modelo impresa por PyTorch, el conteo de par√°metros por capa, y c√≥mo la p√©rdida (MSE) decrece progresivamente durante 100 √©pocas de entrenamiento con datos sint√©ticos.

```python
import torch.nn as nn
import torch.optim as optim

print("\n=== PRIMERA RED NEURONAL EN PYTORCH ===\n")

# Definir arquitectura
class SimpleNet(nn.Module):
    """Red neuronal simple: 10 ‚Üí 20 ‚Üí 1"""
    
    def __init__(self):
        super(SimpleNet, self).__init__()
        # Definir capas
        self.fc1 = nn.Linear(10, 20)  # Capa 1: 10 ‚Üí 20
        self.fc2 = nn.Linear(20, 1)   # Capa 2: 20 ‚Üí 1
    
    def forward(self, x):
        """Forward pass"""
        x = torch.relu(self.fc1(x))  # Activaci√≥n ReLU
        x = self.fc2(x)               # Sin activaci√≥n en salida
        return x

# Instanciar modelo
model = SimpleNet()
print(model)
print()

# Ver par√°metros
total_params = sum(p.numel() for p in model.parameters())
print(f"Par√°metros totales: {total_params}")

# Listar par√°metros
for name, param in model.named_parameters():
    print(f"{name:10s}: {param.shape}")

print("\n--- Entrenar el modelo ---")

# Generar datos sint√©ticos
X_train = torch.randn(100, 10)
y_train = torch.randn(100, 1)

# Funci√≥n de p√©rdida y optimizador
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Loop de entrenamiento
for epoch in range(100):
    # Forward pass
    predictions = model(X_train)
    loss = criterion(predictions, y_train)
    
    # Backward pass
    optimizer.zero_grad()  # Limpiar gradientes
    loss.backward()        # Calcular gradientes
    optimizer.step()       # Actualizar par√°metros
    
    if epoch % 20 == 0:
        print(f"Epoch {epoch:3d}, Loss: {loss.item():.4f}")

print("\n‚úì Modelo entrenado!")
```

**Salida esperada:**
```
=== PRIMERA RED NEURONAL EN PYTORCH ===

SimpleNet(
  (fc1): Linear(in_features=10, out_features=20, bias=True)
  (fc2): Linear(in_features=20, out_features=1, bias=True)
)

Par√°metros totales: 241
fc1.weight: torch.Size([20, 10])
fc1.bias  : torch.Size([20])
fc2.weight: torch.Size([1, 20])
fc2.bias  : torch.Size([1])

--- Entrenar el modelo ---
Epoch   0, Loss: 1.2345
Epoch  20, Loss: 0.9876
Epoch  40, Loss: 0.8123
Epoch  60, Loss: 0.7654
Epoch  80, Loss: 0.7321

‚úì Modelo entrenado!
```

> üí° **Tip:** El n√∫mero `241` de par√°metros se calcula as√≠: capa `fc1` tiene `10√ó20 + 20 = 220` (pesos + biases), y `fc2` tiene `20√ó1 + 1 = 21`. Total: `220 + 21 = 241`. Saber contar par√°metros te ayuda a estimar la complejidad del modelo y el riesgo de overfitting.

**Actividad 1.1:** Crea una red 20 ‚Üí 50 ‚Üí 30 ‚Üí 10 con ReLU y entr√©nala en un problema de regresi√≥n.

### 1.4 Clasificaci√≥n con PyTorch

Ahora construiremos un clasificador binario completo: desde la creaci√≥n del dataset hasta la evaluaci√≥n del modelo entrenado. Este flujo de trabajo es el est√°ndar en PyTorch para tareas de clasificaci√≥n.

**Clasificaci√≥n binaria y BCELoss:**

En clasificaci√≥n binaria, la salida del modelo es una probabilidad `≈∑ ‚àà [0, 1]` (usando `Sigmoid`). La funci√≥n de p√©rdida **Binary Cross-Entropy (BCE)** mide qu√© tan bien calibradas est√°n esas probabilidades:

```
BCE(≈∑, y) = -[y¬∑log(≈∑) + (1-y)¬∑log(1-≈∑)]
```

- Si `y=1` y `≈∑‚âà1`: p√©rdida ‚âà 0 ‚úì
- Si `y=1` y `≈∑‚âà0`: p√©rdida ‚Üí ‚àû ‚úó (penaliza fuertemente el error)

**El optimizador Adam:**

**Adam (Adaptive Moment Estimation)** es una mejora sobre SGD que mantiene tasas de aprendizaje adaptativas para cada par√°metro:

```
m‚Çú = Œ≤‚ÇÅ¬∑m‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÅ)¬∑g‚Çú       ‚Üê Media m√≥vil del gradiente (1er momento)
v‚Çú = Œ≤‚ÇÇ¬∑v‚Çú‚Çã‚ÇÅ + (1-Œ≤‚ÇÇ)¬∑g‚Çú¬≤      ‚Üê Media m√≥vil del gradiente¬≤ (2do momento)
Œ∏ ‚Üê Œ∏ - lr ¬∑ mÃÇ‚Çú / (‚àövÃÇ‚Çú + Œµ)    ‚Üê Actualizaci√≥n adaptativa
```

Adam suele converger m√°s r√°pido que SGD porque:
1. **Momentum**: recuerda el historial de gradientes (evita oscilaciones)
2. **Adaptativo**: par√°metros con gradientes grandes reciben actualizaciones m√°s peque√±as

**`model.train()` vs `model.eval()`:**

| Modo | Efecto en Dropout | Efecto en BatchNorm |
|---|---|---|
| `model.train()` | Activo (desactiva aleatoriamente neuronas) | Usa estad√≠sticas del batch actual |
| `model.eval()` | Desactivado (todas las neuronas activas) | Usa estad√≠sticas acumuladas |

**`torch.no_grad()`:**

Durante la evaluaci√≥n no necesitamos calcular gradientes. `torch.no_grad()` le dice a PyTorch que no construya el grafo computacional, reduciendo el consumo de memoria y acelerando la inferencia.

**¬øQu√© resultados debes esperar?**

Con 100 √©pocas de entrenamiento en un dataset sint√©tico balanceado de 1000 ejemplos y 20 features, deber√≠as alcanzar una **accuracy de test ‚â• 85%**. La p√©rdida de entrenamiento y test deben decrecer de forma estable.

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

print("\n=== CLASIFICACI√ìN BINARIA CON PYTORCH ===\n")

# Generar datos
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convertir a tensores
X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train).reshape(-1, 1)
X_test = torch.FloatTensor(X_test)
y_test = torch.FloatTensor(y_test).reshape(-1, 1)

print(f"Train: {X_train.shape}, Test: {X_test.shape}\n")

# Definir modelo
class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),  # Regularizaci√≥n
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
            nn.Sigmoid()  # Salida [0, 1]
        )
    
    def forward(self, x):
        return self.network(x)

# Crear modelo
model = BinaryClassifier(input_dim=20)
print(model)
print()

# Setup entrenamiento
criterion = nn.BCELoss()  # Binary Cross-Entropy
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Entrenar
epochs = 100
for epoch in range(epochs):
    # Training mode
    model.train()
    
    # Forward
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Evaluar cada 10 √©pocas
    if epoch % 10 == 0:
        model.eval()  # Evaluation mode
        with torch.no_grad():  # No calcular gradientes
            test_outputs = model(X_test)
            test_loss = criterion(test_outputs, y_test)
            
            # Accuracy
            predictions = (test_outputs > 0.5).float()
            accuracy = (predictions == y_test).float().mean()
        
        print(f"Epoch {epoch:3d} | Train Loss: {loss.item():.4f} | "
              f"Test Loss: {test_loss.item():.4f} | Acc: {accuracy.item():.4f}")

print("\n‚úì Clasificador entrenado!")
```

**Salida esperada:**
```
Train: torch.Size([800, 20]), Test: torch.Size([200, 20])

BinaryClassifier(
  (network): Sequential(
    (0): Linear(in_features=20, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.3, inplace=False)
    ...
  )
)

Epoch   0 | Train Loss: 0.7023 | Test Loss: 0.6891 | Acc: 0.5450
Epoch  10 | Train Loss: 0.5832 | Test Loss: 0.5721 | Acc: 0.7250
Epoch  20 | Train Loss: 0.4901 | Test Loss: 0.4843 | Acc: 0.8050
...
Epoch  90 | Train Loss: 0.3241 | Test Loss: 0.3312 | Acc: 0.8750

‚úì Clasificador entrenado!
```

> üí° **Tip:** Si observas que `Train Loss` es mucho menor que `Test Loss`, es se√±al de **overfitting**: el modelo memoriza los datos de entrenamiento en lugar de generalizar. El `Dropout(0.3)` en este modelo act√∫a como regularizador para mitigar este problema.

**Actividad 1.2:** Modifica el modelo para clasificaci√≥n multiclase (3+ clases) usando Softmax.

## üî¨ Parte 2: TensorFlow/Keras Fundamentals (60 min)

### 2.1 Introducci√≥n a TensorFlow

TensorFlow es el framework de deep learning desarrollado por Google Brain. Junto con PyTorch, es el est√°ndar de la industria. Keras, integrado en TensorFlow desde la versi√≥n 2.0, proporciona una API de alto nivel que simplifica enormemente la construcci√≥n y entrenamiento de modelos.

**TensorFlow 2.x: Eager Execution por defecto**

En TensorFlow 1.x, era necesario construir un grafo est√°tico y luego ejecutarlo en una "sesi√≥n". TensorFlow 2.x elimina esta complejidad con **Eager Execution**: las operaciones se ejecutan inmediatamente, igual que en Python y PyTorch, haciendo el c√≥digo mucho m√°s intuitivo y f√°cil de depurar.

**TensorFlow vs PyTorch a nivel de tensores:**

| Caracter√≠stica | PyTorch | TensorFlow |
|---|---|---|
| Creaci√≥n | `torch.tensor([1,2,3])` | `tf.constant([1,2,3])` |
| Mutabilidad | Mutable (`x[0] = 1` funciona) | Inmutable (usa `tf.Variable`) |
| Autograd | `requires_grad=True` + `.backward()` | `tf.GradientTape()` |
| NumPy | `.numpy()` | `.numpy()` (igual) |
| GPU | `.to('cuda')` | autom√°tico o `/device:GPU:0` |

**`tf.constant` vs `tf.Variable`:**

- `tf.constant`: tensor **inmutable**, para datos de entrada y constantes.
- `tf.Variable`: tensor **mutable**, para par√°metros del modelo (pesos y biases). Keras gestiona las Variables autom√°ticamente.

**¬øQu√© resultados debes esperar?**

Ver√°s la versi√≥n de TensorFlow instalada, la creaci√≥n de tensores b√°sicos y operaciones matem√°ticas equivalentes a las de PyTorch, confirmando que ambas APIs son muy similares a nivel de operaciones.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

print("=== TENSORFLOW/KERAS BASICS ===\n")
print(f"TensorFlow version: {tf.__version__}\n")

# Tensores en TensorFlow
t1 = tf.constant([1, 2, 3, 4])
t2 = tf.constant([[1, 2], [3, 4]])

print(f"Tensor 1D: {t1}")
print(f"Tensor 2D:\n{t2}\n")

# Operaciones
a = tf.constant([1.0, 2.0, 3.0])
b = tf.constant([4.0, 5.0, 6.0])

print(f"Suma: {a + b}")
print(f"Producto: {a * b}")
print(f"Matmul: {tf.linalg.matmul(tf.reshape(a, (3, 1)), tf.reshape(b, (1, 3)))}")
```

**Salida esperada:**
```
=== TENSORFLOW/KERAS BASICS ===

TensorFlow version: 2.x.x

Tensor 1D: tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)
Tensor 2D:
tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32)

Suma: tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)
Producto: tf.Tensor([ 4. 10. 18.], shape=(3,), dtype=float32)
```

> üí° **Tip:** En TensorFlow 2.x puedes acceder al valor num√©rico de un tensor con `.numpy()`, igual que en PyTorch. Internamente, tanto TensorFlow como PyTorch delegan las operaciones matem√°ticas a librer√≠as optimizadas en C++ (Eigen, cuBLAS) que explotan las capacidades del hardware.

### 2.2 Primera Red con Keras Sequential API

La **Sequential API** de Keras es la forma m√°s simple de construir modelos de deep learning. Est√° dise√±ada para arquitecturas donde los datos fluyen linealmente de una capa a la siguiente, sin bifurcaciones.

**¬øQu√© hace `model.compile()`?**

`compile()` configura el proceso de entrenamiento especificando tres elementos:

```python
model.compile(
    optimizer='adam',           # Algoritmo de optimizaci√≥n
    loss='binary_crossentropy', # Funci√≥n de p√©rdida a minimizar
    metrics=['accuracy']        # M√©tricas a monitorear (no se optimizan)
)
```

Internamente, Keras construye el grafo de TensorFlow para el entrenamiento, incluyendo el c√°lculo de gradientes con `tf.GradientTape`.

**¬øQu√© hace `model.fit()`?**

`model.fit()` abstrae completamente el loop de entrenamiento:

```
Para cada √©poca:
  Para cada batch:
    1. Forward pass: ≈∑ = model(X_batch)
    2. Calcular p√©rdida: L = loss(≈∑, y_batch)
    3. Backward pass: gradientes = tape.gradient(L, params)
    4. Actualizar: optimizer.apply_gradients(...)
  Calcular m√©tricas de validaci√≥n
  Imprimir progreso
```

Esto es equivalente al loop manual de PyTorch, pero completamente encapsulado.

**¬øQu√© hace `model.evaluate()`?**

`evaluate()` ejecuta solo el **forward pass** (sin actualizar pesos) sobre los datos proporcionados y devuelve la p√©rdida y m√©tricas configuradas en `compile()`.

**Ventajas y desventajas de la abstracci√≥n de Keras:**

| Aspecto | Keras Sequential | PyTorch manual |
|---|---|---|
| **C√≥digo necesario** | Muy poco (~5 l√≠neas) | M√°s verboso (~15 l√≠neas) |
| **Flexibilidad** | Limitada (solo flujo lineal) | Total |
| **Debugging** | M√°s dif√≠cil (caja negra) | M√°s f√°cil (control total) |
| **Curva de aprendizaje** | Baja | Media |

**¬øQu√© resultados debes esperar?**

`model.summary()` mostrar√° la arquitectura con el n√∫mero de par√°metros por capa. Despu√©s de 50 √©pocas, deber√≠as obtener una **accuracy de validaci√≥n ‚â• 85%** en el dataset de clasificaci√≥n binaria.

```python
print("\n=== RED NEURONAL CON KERAS SEQUENTIAL ===\n")

# Definir modelo (API secuencial - m√°s simple)
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

# Ver arquitectura
model.summary()

# Compilar
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Generar datos (NumPy)
X_train_np, y_train_np = make_classification(
    n_samples=1000, n_features=20, n_classes=2, random_state=42
)
X_test_np = X_train_np[:200]
y_test_np = y_train_np[:200]
X_train_np = X_train_np[200:]
y_train_np = y_train_np[200:]

# Entrenar
print("\nEntrenando...")
history = model.fit(
    X_train_np, y_train_np,
    epochs=50,
    batch_size=32,
    validation_data=(X_test_np, y_test_np),
    verbose=0  # Silencioso
)

# Evaluar
test_loss, test_acc = model.evaluate(X_test_np, y_test_np, verbose=0)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# Predicciones
predictions = model.predict(X_test_np[:5])
print(f"\nPrimeras 5 predicciones:\n{predictions.flatten()}")
```

**Salida esperada:**
```
=== RED NEURONAL CON KERAS SEQUENTIAL ===

Model: "sequential"
_________________________________________________________________
 Layer (type)          Output Shape         Param #
=================================================================
 dense (Dense)         (None, 64)           1344
 dropout (Dropout)     (None, 64)           0
 dense_1 (Dense)       (None, 32)           2080
 dropout_1 (Dropout)   (None, 32)           0
 dense_2 (Dense)       (None, 1)            33
=================================================================
Total params: 3,457
Trainable params: 3,457
Non-trainable params: 0

Entrenando...

Test Accuracy: 0.8800
Test Loss: 0.2943

Primeras 5 predicciones:
[0.923 0.041 0.876 0.134 0.791]
```

> üí° **Tip:** El objeto `history` retornado por `model.fit()` contiene un diccionario con el historial de p√©rdida y m√©tricas por √©poca: `history.history['loss']`, `history.history['val_accuracy']`, etc. √ösalo para graficar curvas de aprendizaje y diagnosticar overfitting o underfitting.

### 2.3 Keras Functional API (M√°s Flexible)

La **Functional API** de Keras permite construir modelos m√°s complejos que la Sequential API. Mientras que Sequential obliga a un flujo lineal (una capa tras otra), la Functional API trata las capas como **funciones** que pueden conectarse arbitrariamente.

**¬øPor qu√© existe la Functional API?**

La Sequential API no puede manejar:
- **M√∫ltiples entradas**: e.g., un modelo que recibe imagen + texto
- **M√∫ltiples salidas**: e.g., clasificador + regresor simult√°neo
- **Skip connections** (conexiones residuales): e.g., ResNet
- **Ramas paralelas**: e.g., Inception modules
- **Grafos ac√≠clicos dirigidos (DAG)** en general

**¬øC√≥mo funciona?**

En la Functional API, cada capa es literalmente una funci√≥n Python:

```python
x = layers.Dense(64)(input_tensor)   # La capa se "llama" sobre el tensor
x = layers.ReLU()(x)                  # Se pueden encadenar
```

El modelo se define especificando **qu√© tensor entra** y **qu√© tensor sale**:
```python
model = keras.Model(inputs=inputs, outputs=outputs)
```

Keras infiere autom√°ticamente toda la topolog√≠a del grafo entre `inputs` y `outputs`.

**Ventaja pr√°ctica:** La Functional API mantiene la comodidad de `compile()`/`fit()` pero con la flexibilidad para arquitecturas complejas. Es el **punto medio** entre Sequential (simple pero r√≠gido) y Subclassing (flexible pero m√°s c√≥digo).

**¬øCu√°ndo usarla?**

- Cuando necesitas skip connections
- Cuando tienes m√∫ltiples entradas o salidas
- Cuando quieres visualizar el grafo con `keras.utils.plot_model()`
- Para arquitecturas estilo ResNet, U-Net, Siamese Networks

**¬øQu√© resultados debes esperar?**

El modelo Functional tendr√° exactamente la misma arquitectura y n√∫mero de par√°metros que el Sequential equivalente, demostrando que la API es solo una forma diferente de definir el mismo modelo.

```python
print("\n=== KERAS FUNCTIONAL API ===\n")

# Input layer
inputs = keras.Input(shape=(20,))

# Hidden layers
x = layers.Dense(64, activation='relu')(inputs)
x = layers.Dropout(0.3)(x)
x = layers.Dense(32, activation='relu')(x)
x = layers.Dropout(0.3)(x)

# Output layer
outputs = layers.Dense(1, activation='sigmoid')(x)

# Crear modelo
model_functional = keras.Model(inputs=inputs, outputs=outputs)

model_functional.summary()

# Compilar y entrenar igual que antes
model_functional.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model_functional.fit(
    X_train_np, y_train_np,
    epochs=50,
    batch_size=32,
    validation_data=(X_test_np, y_test_np),
    verbose=0
)

print(f"Final val accuracy: {history.history['val_accuracy'][-1]:.4f}")
```

**Salida esperada:**
```
=== KERAS FUNCTIONAL API ===

Model: "model"
_________________________________________________________________
 Layer (type)          Output Shape         Param #
=================================================================
 input_1 (InputLayer)  [(None, 20)]         0
 dense (Dense)         (None, 64)           1344
 dropout (Dropout)     (None, 64)           0
 dense_1 (Dense)       (None, 32)           2080
 dropout_1 (Dropout)   (None, 32)           0
 dense_2 (Dense)       (None, 1)            33
=================================================================
Total params: 3,457

Final val accuracy: 0.8750
```

> üí° **Tip:** Una ventaja √∫nica de la Functional API es que puedes acceder a las salidas **intermedias** del modelo: `intermediate_model = keras.Model(inputs=model.input, outputs=model.layers[2].output)`. Esto es muy √∫til para visualizar feature maps o construir modelos de extracci√≥n de caracter√≠sticas.

### 2.4 Subclassing (M√°ximo Control)

El **Model Subclassing** en Keras (equivalente a `nn.Module` en PyTorch) es la API m√°s flexible de TensorFlow. Permite implementar cualquier arquitectura, incluidas aquellas con comportamiento din√°mico que no pueden expresarse como un grafo est√°tico.

**¬øQu√© hace el m√©todo `call()`?**

`call()` es el equivalente de `forward()` en PyTorch: define c√≥mo fluyen los datos de entrada a salida. Se invoca autom√°ticamente cuando "llamas" al modelo como funci√≥n:

```python
modelo = CustomModel()
salida = modelo(entrada)  # Invoca model.call(entrada)
```

**El par√°metro `training` en `call()`:**

```python
def call(self, inputs, training=False):
    x = self.dropout1(x, training=training)  # Solo activo si training=True
```

El par√°metro `training` es cr√≠tico para capas con comportamiento diferente en entrenamiento vs inferencia:
- **Dropout**: aplica m√°scara aleatoria solo durante `training=True`
- **BatchNormalization**: usa estad√≠sticas del batch en training, estad√≠sticas acumuladas en inferencia

**¬øCu√°ndo usar cada API?**

| Situaci√≥n | API recomendada |
|---|---|
| Arquitectura simple, prototipo r√°pido | **Sequential** |
| M√∫ltiples I/O, skip connections, grafo est√°tico | **Functional** |
| L√≥gica din√°mica, loops en forward, investigaci√≥n | **Subclassing** |
| M√°ximo control, equivalente a PyTorch | **Subclassing** |

**Tradeoffs del Subclassing:**

- ‚úÖ **M√°xima flexibilidad**: cualquier l√≥gica Python en `call()`
- ‚úÖ **Familiar para usuarios de PyTorch**: mismo paradigma
- ‚ùå **No serializable directamente**: `model.save()` requiere llamar al modelo primero
- ‚ùå **No puedes inspeccionar el grafo** con `plot_model()` sin ejecutarlo antes

**¬øQu√© resultados debes esperar?**

El modelo subclassed entrenar√° igual que los anteriores, alcanzando accuracy similar (~85‚Äì88%), demostrando que las tres APIs producen modelos funcionalmente equivalentes para arquitecturas simples.

```python
print("\n=== MODEL SUBCLASSING ===\n")

class CustomModel(keras.Model):
    """Modelo personalizado con subclassing"""
    
    def __init__(self):
        super(CustomModel, self).__init__()
        self.dense1 = layers.Dense(64, activation='relu')
        self.dropout1 = layers.Dropout(0.3)
        self.dense2 = layers.Dense(32, activation='relu')
        self.dropout2 = layers.Dropout(0.3)
        self.output_layer = layers.Dense(1, activation='sigmoid')
    
    def call(self, inputs, training=False):
        """Forward pass"""
        x = self.dense1(inputs)
        x = self.dropout1(x, training=training)  # Dropout solo en training
        x = self.dense2(x)
        x = self.dropout2(x, training=training)
        return self.output_layer(x)

# Crear y entrenar
model_custom = CustomModel()

model_custom.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Necesita build con una llamada o especificando input_shape
model_custom.build(input_shape=(None, 20))
model_custom.summary()

history = model_custom.fit(
    X_train_np, y_train_np,
    epochs=50,
    batch_size=32,
    validation_data=(X_test_np, y_test_np),
    verbose=0
)

print(f"Final accuracy: {history.history['val_accuracy'][-1]:.4f}")
```

**Salida esperada:**
```
=== MODEL SUBCLASSING ===

Model: "custom_model"
_________________________________________________________________
 Layer (type)          Output Shape         Param #
=================================================================
 dense (Dense)         multiple             1344
 dropout (Dropout)     multiple             0
 dense_1 (Dense)       multiple             2080
 dropout_1 (Dropout)   multiple             0
 dense_2 (Dense)       multiple             33
=================================================================
Total params: 3,457

Final accuracy: 0.8650
```

> üí° **Tip:** Con Subclassing, el `model.summary()` muestra "multiple" en Output Shape porque el grafo no se construye hasta que el modelo se ejecuta. Para ver las formas correctas, construye el modelo con `model.build(input_shape=(None, 20))` antes de llamar a `summary()`.

**Actividad 2.1:** Crea un modelo con arquitectura residual (skip connections) usando Functional API.

## üî¨ Parte 3: Comparaci√≥n PyTorch vs TensorFlow (40 min)

### 3.1 Mismo Modelo en Ambos Frameworks

Una de las mejores formas de consolidar el aprendizaje de ambos frameworks es implementar exactamente el mismo modelo en PyTorch y TensorFlow y comparar sus caracter√≠sticas. Esta secci√≥n revela las diferencias fundamentales en filosof√≠a de dise√±o.

**¬øQu√© revela esta comparaci√≥n?**

Implementando el mismo modelo en ambos frameworks con los mismos hiperpar√°metros, podemos comparar:

1. **Cantidad de c√≥digo**: PyTorch requiere un loop de entrenamiento expl√≠cito; Keras lo encapsula en `fit()`.
2. **Velocidad**: Ambos deber√≠an ser comparables en CPU; TensorFlow puede tener ventaja en GPU con XLA compilation.
3. **Accuracy**: Con los mismos datos y arquitectura, los resultados deben ser similares (las diferencias son por inicializaci√≥n aleatoria).

**El loop de entrenamiento expl√≠cito (PyTorch) vs impl√≠cito (TensorFlow):**

```
PyTorch (expl√≠cito):              TensorFlow/Keras (impl√≠cito):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ             ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
for epoch in range(50):           model.fit(X_train, y_train,
    outputs = model(X)                epochs=50,
    loss = criterion(out, y)          batch_size=32)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

**PyTorch da m√°s control**, pero requiere m√°s c√≥digo. **Keras abstrae el loop**, lo que es m√°s conveniente pero oculta los detalles.

**Consideraciones de rendimiento:**

- En datasets peque√±os (como el de este ejemplo), las diferencias de tiempo son m√≠nimas y dependen de la inicializaci√≥n de los frameworks.
- Para datasets grandes, TensorFlow puede compilar el grafo con XLA para mayor velocidad, mientras que PyTorch tiene `torch.compile()` (desde PyTorch 2.0).
- En producci√≥n, ambos frameworks ofrecen herramientas de optimizaci√≥n similares.

**¬øQu√© resultados debes esperar?**

Los tiempos de entrenamiento ser√°n del orden de segundos para 50 √©pocas. Las diferencias pueden ser de 20-50%, pero var√≠an seg√∫n hardware. La accuracy final ser√° similar en ambos.

```python
print("\n=== COMPARACI√ìN LADO A LADO ===\n")

# ----- PYTORCH -----
print("1. PYTORCH\n")

class PyTorchModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(20, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.layers(x)

pytorch_model = PyTorchModel()
print(f"Par√°metros PyTorch: {sum(p.numel() for p in pytorch_model.parameters())}")

# ----- TENSORFLOW -----
print("\n2. TENSORFLOW/KERAS\n")

tensorflow_model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

tensorflow_model.build(input_shape=(None, 20))
print(f"Par√°metros TensorFlow: {tensorflow_model.count_params()}")

# ----- ENTRENAR AMBOS -----
import time

# Datos
X_train_torch = torch.FloatTensor(X_train_np)
y_train_torch = torch.FloatTensor(y_train_np).reshape(-1, 1)

# PyTorch
print("\n--- Entrenando PyTorch ---")
criterion = nn.BCELoss()
optimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)

start = time.time()
for epoch in range(50):
    outputs = pytorch_model(X_train_torch)
    loss = criterion(outputs, y_train_torch)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

pytorch_time = time.time() - start
print(f"Tiempo PyTorch: {pytorch_time:.2f}s")

# TensorFlow
print("\n--- Entrenando TensorFlow ---")
tensorflow_model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

start = time.time()
tensorflow_model.fit(
    X_train_np, y_train_np,
    epochs=50,
    batch_size=32,
    verbose=0
)
tensorflow_time = time.time() - start
print(f"Tiempo TensorFlow: {tensorflow_time:.2f}s")

print(f"\n--- Comparaci√≥n ---")
print(f"PyTorch:    {pytorch_time:.2f}s")
print(f"TensorFlow: {tensorflow_time:.2f}s")
```

**Salida esperada:**
```
=== COMPARACI√ìN LADO A LADO ===

1. PYTORCH
Par√°metros PyTorch: 3457

2. TENSORFLOW/KERAS
Par√°metros TensorFlow: 3457

--- Entrenando PyTorch ---
Tiempo PyTorch: 0.85s

--- Entrenando TensorFlow ---
Tiempo TensorFlow: 1.23s

--- Comparaci√≥n ---
PyTorch:    0.85s
TensorFlow: 1.23s
```

> üí° **Tip:** Los tiempos variar√°n significativamente seg√∫n tu hardware y si usas GPU. TensorFlow tiene un overhead de inicializaci√≥n mayor al primer run (compilaci√≥n JIT), pero puede ser m√°s r√°pido en ejecuciones subsecuentes. Para benchmarks precisos, ignora la primera ejecuci√≥n y promedia m√∫ltiples corridas (como hace la secci√≥n de Benchmark Completo m√°s adelante).

### 3.2 DataLoaders y Pipelines

El pipeline de carga de datos es uno de los cuellos de botella m√°s comunes en el entrenamiento de modelos de deep learning. Si los datos no se cargan suficientemente r√°pido, la GPU (o CPU) se queda esperando, desperdiciando capacidad de c√≥mputo.

**¬øPor qu√© son cr√≠ticos los pipelines de datos?**

En el entrenamiento moderno, el modelo puede procesar un batch en milisegundos, pero cargar im√°genes de disco, aplicar transformaciones y preprocesarlas puede ser mucho m√°s lento. Sin un pipeline eficiente:

```
[Cargar datos] ‚Üí [Procesar] ‚Üí [Entrenar]
   500ms              200ms       10ms       ‚Üê GPU inactiva 700ms por batch!
```

Con pipeline as√≠ncrono:
```
[Cargar N+1] ‚Üí paralelo con ‚Üí [Entrenar N]
   500ms                          10ms       ‚Üê GPU siempre ocupada
```

**PyTorch: Dataset + DataLoader**

El patr√≥n de PyTorch separa dos responsabilidades:
- **`Dataset`**: Define c√≥mo obtener un elemento individual por √≠ndice (`__getitem__`) y el tama√±o total (`__len__`).
- **`DataLoader`**: Orquesta el batching, shuffling y carga paralela en m√∫ltiples procesos (`num_workers`).

```python
# Dataset personalizado
class MyDataset(Dataset):
    def __getitem__(self, idx): ...  # Un elemento
    def __len__(self): ...           # Tama√±o total

# DataLoader gestiona el batching
loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)
```

**TensorFlow: tf.data.Dataset**

`tf.data` es el sistema de pipelines de TensorFlow, dise√±ado para alta eficiencia con operaciones encadenables:

```python
dataset = tf.data.Dataset.from_tensor_slices((X, y))
dataset = dataset.shuffle(buffer_size=1000)  # Aleatorizaci√≥n
dataset = dataset.batch(32)                  # Agrupar en batches
dataset = dataset.prefetch(tf.data.AUTOTUNE) # Prefetch autom√°tico
```

**¬øPor qu√© importa el shuffling?**

Si el modelo ve siempre los ejemplos en el mismo orden, puede aprender patrones espurios. El shuffling garantiza que cada √©poca el modelo vea los datos en orden diferente, mejorando la generalizaci√≥n.

**¬øQu√© resultados debes esperar?**

Ambos DataLoaders mostrar√°n un batch de shape `(32, 20)` para X y `(32, 1)` para y, confirmando que el batching funciona correctamente.

```python
print("\n=== DATA LOADING ===\n")

# ----- PYTORCH DATALOADER -----
from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    """Dataset personalizado para PyTorch"""
    
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y).reshape(-1, 1)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Crear dataset y dataloader
train_dataset = CustomDataset(X_train_np, y_train_np)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

print("PyTorch DataLoader:")
for batch_X, batch_y in train_loader:
    print(f"  Batch shape: X={batch_X.shape}, y={batch_y.shape}")
    break

# ----- TENSORFLOW DATASET -----
train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train_np, y_train_np))
train_dataset_tf = train_dataset_tf.shuffle(1000).batch(32)

print("\nTensorFlow Dataset:")
for batch_X, batch_y in train_dataset_tf.take(1):
    print(f"  Batch shape: X={batch_X.shape}, y={batch_y.shape}")
```

**Salida esperada:**
```
=== DATA LOADING ===

PyTorch DataLoader:
  Batch shape: X=torch.Size([32, 20]), y=torch.Size([32, 1])

TensorFlow Dataset:
  Batch shape: X=(32, 20), y=(32,)
```

> üí° **Tip:** Para datasets grandes que no caben en RAM, usa `DataLoader` con `num_workers > 0` en PyTorch (carga paralela en m√∫ltiples CPUs) o `.prefetch(tf.data.AUTOTUNE)` en TensorFlow. Para im√°genes, considera `torchvision.datasets` (PyTorch) o `tf.keras.preprocessing.image_dataset_from_directory` (TF) que cargan desde disco eficientemente.

**Actividad 3.1:** Implementa el mismo modelo en ambos frameworks y compara resultados.

## üî¨ Parte 4: Funcionalidades Avanzadas (50 min)

### 4.1 Guardar y Cargar Modelos

Guardar modelos es esencial en cualquier flujo de trabajo real de machine learning. Permite reanudar entrenamientos interrumpidos, compartir modelos entrenados, versionar experimentos y desplegar modelos en producci√≥n.

**¬øCu√°ndo y por qu√© guardar modelos?**

| Situaci√≥n | Estrategia |
|---|---|
| Entrenamiento largo (horas/d√≠as) | Checkpoints peri√≥dicos para no perder progreso |
| Mejor modelo durante validaci√≥n | Guardar cuando la m√©trica mejora (ModelCheckpoint) |
| Despliegue en producci√≥n | Guardar el modelo final optimizado |
| Reproducibilidad cient√≠fica | Guardar modelos de experimentos publicados |

**PyTorch: Dos enfoques**

**Enfoque 1 ‚Äî `state_dict` (recomendado):**
```python
torch.save(model.state_dict(), 'model.pth')  # Solo los pesos
model.load_state_dict(torch.load('model.pth'))
```
- ‚úÖ M√°s flexible (puedes cargar en modelos con arquitectura modificada)
- ‚úÖ Tama√±o de archivo m√°s peque√±o
- ‚ùå Debes tener el c√≥digo de la clase del modelo para cargar

**Enfoque 2 ‚Äî Modelo completo:**
```python
torch.save(model, 'model_complete.pth')  # Arquitectura + pesos
model = torch.load('model_complete.pth')
```
- ‚úÖ No necesitas el c√≥digo de la clase
- ‚ùå Fr√°gil si cambias la estructura del c√≥digo

**TensorFlow: M√∫ltiples formatos**

- **HDF5 (`.h5`)**: Formato legacy, compacto, soportado por Keras.
- **SavedModel (directorio)**: Formato moderno de TF, incluye el grafo computacional, compatible con TensorFlow Serving para producci√≥n.

```python
model.save('model.h5')              # HDF5
model.save('model_dir/')            # SavedModel (recomendado)
loaded = keras.models.load_model('model.h5')
```

**¬øQu√© resultados debes esperar?**

Los modelos guardados y cargados deben producir exactamente las mismas predicciones que el modelo original, verificando que los pesos se guardaron y cargaron correctamente.

```python
print("\n=== GUARDAR Y CARGAR MODELOS ===\n")

# ----- PYTORCH -----
print("1. PyTorch\n")

# Guardar
torch.save(pytorch_model.state_dict(), '/tmp/pytorch_model.pth')
print("‚úì Modelo guardado: pytorch_model.pth")

# Cargar
loaded_pytorch_model = PyTorchModel()
loaded_pytorch_model.load_state_dict(torch.load('/tmp/pytorch_model.pth'))
loaded_pytorch_model.eval()
print("‚úì Modelo cargado")

# Verificar que funciona
test_input = torch.randn(1, 20)
output = loaded_pytorch_model(test_input)
print(f"Predicci√≥n de prueba: {output.item():.4f}\n")

# ----- TENSORFLOW -----
print("2. TensorFlow\n")

# Guardar (varios formatos)
tensorflow_model.save('/tmp/tf_model.h5')  # HDF5
print("‚úì Modelo guardado: tf_model.h5")

# Cargar
loaded_tf_model = keras.models.load_model('/tmp/tf_model.h5')
print("‚úì Modelo cargado")

# Verificar
test_input_tf = np.random.randn(1, 20)
output_tf = loaded_tf_model.predict(test_input_tf, verbose=0)
print(f"Predicci√≥n de prueba: {output_tf[0][0]:.4f}")
```

**Salida esperada:**
```
=== GUARDAR Y CARGAR MODELOS ===

1. PyTorch

‚úì Modelo guardado: pytorch_model.pth
‚úì Modelo cargado
Predicci√≥n de prueba: 0.6234

2. TensorFlow

‚úì Modelo guardado: tf_model.h5
‚úì Modelo cargado
Predicci√≥n de prueba: 0.7891
```

> üí° **Tip:** Para PyTorch, siempre llama `model.eval()` despu√©s de cargar el modelo con `load_state_dict()`. Esto asegura que las capas como Dropout y BatchNorm est√©n en modo inferencia. Si planeas continuar entrenando, llama `model.train()` en su lugar. En TensorFlow, el modelo cargado recuerda autom√°ticamente su estado de compilaci√≥n.

### 4.2 Callbacks y Early Stopping

Los **callbacks** son funciones que Keras llama autom√°ticamente en puntos espec√≠ficos del entrenamiento (al inicio/fin de cada √©poca, al inicio/fin de cada batch, etc.). Permiten a√±adir comportamiento personalizado sin modificar el loop de entrenamiento.

**¬øQu√© es Early Stopping y por qu√© previene overfitting?**

El **Early Stopping** detiene el entrenamiento cuando la m√©trica de validaci√≥n deja de mejorar. Sin √©l, un modelo puede seguir bajando la p√©rdida de entrenamiento mientras la p√©rdida de validaci√≥n sube (overfitting):

```
          Zona ideal
             ‚Üì
P√©rdida ‚îÇ  ‚ï≤  train
        ‚îÇ   ‚ï≤___________
        ‚îÇ         ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ val (empieza a subir ‚Üí overfitting)
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ √©pocas
                  ‚Üë
           Early Stopping aqu√≠
```

El par√°metro **`patience`** define cu√°ntas √©pocas consecutivas sin mejora tolerar antes de detener. `patience=10` significa "detente si no mejora en 10 √©pocas seguidas". `restore_best_weights=True` recupera los pesos del mejor modelo encontrado.

**¬øQu√© hace `ReduceLROnPlateau`?**

Reduce la tasa de aprendizaje cuando la p√©rdida de validaci√≥n se estanca. Si no mejora en `patience` √©pocas, multiplica `lr` por `factor`:

```
lr_nueva = lr_actual √ó factor    (e.g., 0.001 √ó 0.5 = 0.0005)
```

Esto permite que el modelo "refine" su posici√≥n en el espacio de par√°metros con pasos m√°s peque√±os cuando est√° cerca del √≥ptimo.

**¬øQu√© hace `ModelCheckpoint`?**

Guarda el modelo autom√°ticamente cada vez que la m√©trica monitoreada mejora. Con `save_best_only=True`, solo guarda cuando supera el mejor resultado anterior, actuando como un "versionado autom√°tico" del mejor modelo.

**Callbacks disponibles en Keras:**

| Callback | Funci√≥n |
|---|---|
| `EarlyStopping` | Detener cuando la m√©trica se estanca |
| `ReduceLROnPlateau` | Reducir lr cuando la m√©trica se estanca |
| `ModelCheckpoint` | Guardar el mejor modelo autom√°ticamente |
| `TensorBoard` | Registrar m√©tricas para visualizaci√≥n |
| `LearningRateScheduler` | Programar cambios de lr manualmente |
| `CSVLogger` | Guardar historial en CSV |

**¬øQu√© resultados debes esperar?**

Con `patience=10` sobre 100 √©pocas programadas, el entrenamiento t√≠picamente se detiene antes (entre 20 y 50 √©pocas) cuando la validaci√≥n converge, ahorrando tiempo de c√≥mputo sin sacrificar accuracy.

```python
print("\n=== CALLBACKS (TENSORFLOW) ===\n")

# Crear modelo fresco
model_with_callbacks = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

model_with_callbacks.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Definir callbacks
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        verbose=1
    ),
    keras.callbacks.ModelCheckpoint(
        '/tmp/best_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
]

# Entrenar con callbacks
history = model_with_callbacks.fit(
    X_train_np, y_train_np,
    epochs=100,  # Muchas √©pocas, pero early stopping lo detendr√°
    batch_size=32,
    validation_data=(X_test_np, y_test_np),
    callbacks=callbacks,
    verbose=0
)

print(f"\n√âpocas entrenadas: {len(history.history['loss'])}")
print(f"Mejor val_accuracy: {max(history.history['val_accuracy']):.4f}")
```

**Salida esperada:**
```
=== CALLBACKS (TENSORFLOW) ===

Epoch 00032: early stopping
Restoring model weights from the end of the best epoch.

Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0005.

Epoch 00032: val_accuracy improved from 0.8700 to 0.8750, saving model to /tmp/best_model.h5

√âpocas entrenadas: 32
Mejor val_accuracy: 0.8750
```

> üí° **Tip:** El n√∫mero de √©pocas que Early Stopping necesita depende del dataset y la arquitectura. Un `patience` muy peque√±o puede detener el entrenamiento prematuramente (antes de que converja), mientras que uno muy grande pierde el beneficio. Un valor de `patience = epochs * 0.1` (10% del total) suele ser un buen punto de partida.

### 4.3 Visualizaci√≥n con TensorBoard

**TensorBoard** es la herramienta de visualizaci√≥n oficial de TensorFlow (tambi√©n disponible para PyTorch). Proporciona un dashboard interactivo en el navegador para monitorear y depurar el entrenamiento de modelos.

**¬øPor qu√© es cr√≠tico monitorear el entrenamiento?**

Entrenar un modelo sin visualizaci√≥n es como conducir con los ojos cerrados. TensorBoard permite:
- **Detectar problemas temprano**: overfitting, gradientes que se desvanecen, learning rate inadecuado
- **Comparar experimentos**: diferentes arquitecturas o hiperpar√°metros en la misma gr√°fica
- **Entender el modelo**: visualizar pesos, activaciones y gradientes por capa

**¬øQu√© m√©tricas registra TensorBoard?**

| Panel | Informaci√≥n |
|---|---|
| **Scalars** | P√©rdida y m√©tricas por √©poca (loss, accuracy, lr) |
| **Histograms** | Distribuci√≥n de pesos y gradientes por capa |
| **Graphs** | Grafo computacional del modelo |
| **Images** | Im√°genes de entrada, activaciones, filtros |
| **Projector** | Visualizaci√≥n de embeddings en 2D/3D (t-SNE) |
| **HParams** | B√∫squeda de hiperpar√°metros |

**¬øC√≥mo se usa en la pr√°ctica?**

```python
# 1. Crear callback con directorio de logs
tensorboard_cb = keras.callbacks.TensorBoard(
    log_dir='./logs/experimento_01',
    histogram_freq=1  # Guardar histogramas cada √©poca
)

# 2. Pasar callback a fit()
model.fit(..., callbacks=[tensorboard_cb])

# 3. Lanzar TensorBoard desde terminal
# tensorboard --logdir=./logs --port=6006
# Abrir en navegador: http://localhost:6006
```

**Interpretando las curvas de aprendizaje en TensorBoard:**

```
Escenario ideal:           Overfitting:          Underfitting:
train ‚ï≤                    train  ‚ï≤__             train ‚ï≤
val   ‚ï≤__                  val    ‚ï±               val   ‚ï≤  (ambas altas)
                                (divergen)
```

**¬øQu√© resultados debes esperar?**

Despu√©s de ejecutar el c√≥digo, los logs se guardar√°n en `/tmp/logs`. Al lanzar TensorBoard, ver√°s gr√°ficas de p√©rdida y accuracy que bajan progresivamente durante 20 √©pocas.

```python
print("\n=== TENSORBOARD ===\n")

# Crear callback de TensorBoard
tensorboard_callback = keras.callbacks.TensorBoard(
    log_dir='/tmp/logs',
    histogram_freq=1
)

# Entrenar con logging
model_tb = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

model_tb.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model_tb.fit(
    X_train_np, y_train_np,
    epochs=20,
    validation_data=(X_test_np, y_test_np),
    callbacks=[tensorboard_callback],
    verbose=0
)

print("‚úì Logs guardados en /tmp/logs")
print("Para visualizar: tensorboard --logdir=/tmp/logs")
```

**Salida esperada:**
```
=== TENSORBOARD ===

‚úì Logs guardados en /tmp/logs
Para visualizar: tensorboard --logdir=/tmp/logs
```

Para visualizar en un entorno local, abre una terminal y ejecuta:
```bash
tensorboard --logdir=/tmp/logs --port=6006
```
Luego abre `http://localhost:6006` en tu navegador.

> üí° **Tip:** Organiza tus experimentos con subdirectorios con nombre descriptivo: `logs/experimento_lr001/`, `logs/experimento_lr0001/`. As√≠ TensorBoard mostrar√° ambas curvas superpuestas y podr√°s comparar directamente el efecto de diferentes hiperpar√°metros.

### 4.4 Transfer Learning B√°sico

El **Transfer Learning** (aprendizaje por transferencia) es una de las t√©cnicas m√°s poderosas del deep learning moderno. Permite reutilizar conocimiento aprendido en una tarea para resolver una tarea diferente pero relacionada.

**¬øPor qu√© funciona el Transfer Learning?**

Las redes profundas aprenden representaciones **jer√°rquicas**:

```
Capa 1-3:   Detectan bordes, colores, texturas simples (universales)
Capa 4-7:   Detectan formas, partes de objetos (semiespec√≠ficas)
Capa 8-10:  Detectan objetos espec√≠ficos del dataset (espec√≠ficas)
```

Las capas tempranas aprenden caracter√≠sticas gen√©ricas √∫tiles para cualquier tarea visual. Una red entrenada en ImageNet (1.2M im√°genes, 1000 clases) ha aprendido detectores de bordes, texturas y formas que son transferibles a cualquier tarea de visi√≥n.

**Feature Extraction vs Fine-tuning:**

| Enfoque | ¬øQu√© se entrena? | ¬øCu√°ndo usarlo? |
|---|---|---|
| **Feature Extraction** | Solo las capas nuevas (clasificador) | Dataset peque√±o (<1000 im√°genes) |
| **Fine-tuning parcial** | Capas nuevas + √∫ltimas capas del base | Dataset mediano |
| **Fine-tuning total** | Todo el modelo | Dataset grande y similar |

**¬øPor qu√© congelamos capas (`trainable = False`)?**

Al congelar la red base:
1. **Evitamos destruir features aprendidas**: Si entrenamos toda la red con pocas muestras, podr√≠amos sobreescribir representaciones valiosas con ruido.
2. **Reducimos par√°metros entrenables**: Entrenamos solo el clasificador nuevo, que es mucho m√°s peque√±o y r√°pido de entrenar.
3. **Necesitamos menos datos**: El clasificador es simple y no necesita millones de ejemplos.

**MobileNetV2:**

MobileNetV2 es una arquitectura eficiente dise√±ada para dispositivos m√≥viles. Con `include_top=False` y `weights='imagenet'`, cargamos la red base sin la capa de clasificaci√≥n final, listos para a√±adir nuestro propio clasificador.

**¬øQu√© resultados debes esperar?**

El modelo mostrar√° el contraste dram√°tico entre par√°metros totales (~3.5M de MobileNetV2) y par√°metros entrenables (solo los del clasificador personalizado, ~100K). Este es el poder del Transfer Learning: entrenar solo el 3% de los par√°metros para obtener un clasificador poderoso.

```python
print("\n=== TRANSFER LEARNING (EJEMPLO) ===\n")

# Usando modelo pre-entrenado de Keras
from tensorflow.keras.applications import MobileNetV2

# Cargar modelo pre-entrenado (sin top/clasificador)
base_model = MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)

# Congelar pesos del modelo base
base_model.trainable = False

# A√±adir clasificador personalizado
inputs = keras.Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(10, activation='softmax')(x)  # 10 clases

transfer_model = keras.Model(inputs, outputs)

print("Modelo con transfer learning:")
print(f"  Par√°metros totales: {transfer_model.count_params():,}")
print(f"  Par√°metros entrenables: {sum(tf.size(w).numpy() for w in transfer_model.trainable_weights):,}")
print(f"  Par√°metros congelados: {sum(tf.size(w).numpy() for w in transfer_model.non_trainable_weights):,}")
```

**Salida esperada:**
```
=== TRANSFER LEARNING (EJEMPLO) ===

Modelo con transfer learning:
  Par√°metros totales:      3,638,538
  Par√°metros entrenables:    131,082
  Par√°metros congelados:   3,507,456
```

La proporci√≥n t√≠pica es: **~3.6% de par√°metros entrenables**. Esto es lo que hace que el Transfer Learning sea tan eficiente: con solo 131K par√°metros a entrenar (en lugar de 3.6M), necesitas muchos menos datos y tiempo de entrenamiento.

> üí° **Tip:** Para hacer **fine-tuning** despu√©s de feature extraction, descongela las √∫ltimas capas del modelo base con `base_model.trainable = True` y entrena con una tasa de aprendizaje muy baja (`lr=1e-5`). Esto "afina" las representaciones espec√≠ficas para tu tarea sin destruir el conocimiento previo.

**Actividad 4.1:** Implementa un sistema de checkpointing en PyTorch similar a los callbacks de Keras.

## üìä An√°lisis Final de Rendimiento

### Benchmark Completo

El benchmark final proporciona una comparaci√≥n estad√≠stica rigurosa entre PyTorch y TensorFlow. Al ejecutar m√∫ltiples corridas, obtenemos no solo el tiempo y accuracy promedio, sino tambi√©n la **variabilidad** (desviaci√≥n est√°ndar), que es un indicador de la estabilidad de cada framework.

**Metodolog√≠a del benchmark:**

1. **M√∫ltiples corridas (`n_runs=3-5`)**: Promediar m√∫ltiples ejecuciones elimina la variabilidad por calentamiento del sistema, JIT compilation, y otras fuentes de ruido.
2. **Mismo dataset y arquitectura**: Control de variables ‚Äî la √∫nica diferencia es el framework.
3. **Mismos hiperpar√°metros**: `lr=0.001`, `epochs=20`, `batch_size=32` en ambos.

**¬øQu√© m√©tricas se comparan?**

| M√©trica | Qu√© indica | Cu√°l es mejor |
|---|---|---|
| `avg_time` | Velocidad de entrenamiento | Menor es mejor |
| `std_time` | Estabilidad de velocidad | Menor es mejor |
| `avg_accuracy` | Calidad del modelo | Mayor es mejor |
| `std_accuracy` | Consistencia de resultados | Menor es mejor |

**¬øC√≥mo interpretar los resultados?**

- Si los tiempos son similares (< 20% de diferencia): ambos frameworks son igualmente apropiados para esta tarea.
- Si la accuracy var√≠a mucho entre corridas: considera revisar la inicializaci√≥n de pesos o el learning rate.
- Las barras de error en la gr√°fica representan ¬±1 desviaci√≥n est√°ndar.

**Insight clave:** Los benchmarks en CPU con datasets peque√±os no reflejan el rendimiento real en producci√≥n con GPU y datasets grandes. Las diferencias se magnifican con modelos m√°s grandes y datos m√°s complejos.

```python
import matplotlib.pyplot as plt

print("\n=== BENCHMARK FINAL ===\n")

def benchmark_framework(framework_name, train_fn, n_runs=5):
    """Benchmark de un framework"""
    times = []
    accuracies = []
    
    for run in range(n_runs):
        start = time.time()
        accuracy = train_fn()
        elapsed = time.time() - start
        
        times.append(elapsed)
        accuracies.append(accuracy)
    
    return {
        'framework': framework_name,
        'avg_time': np.mean(times),
        'std_time': np.std(times),
        'avg_accuracy': np.mean(accuracies),
        'std_accuracy': np.std(accuracies)
    }

# Definir funciones de entrenamiento
def train_pytorch():
    model = PyTorchModel()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.BCELoss()
    
    for epoch in range(20):
        outputs = model(X_train_torch)
        loss = criterion(outputs, y_train_torch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # Evaluar
    model.eval()
    with torch.no_grad():
        preds = model(torch.FloatTensor(X_test_np))
        preds = (preds > 0.5).float()
        accuracy = (preds.numpy().flatten() == y_test_np).mean()
    
    return accuracy

def train_tensorflow():
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(20,)),
        layers.Dropout(0.3),
        layers.Dense(32, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(1, activation='sigmoid')
    ])
    
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    history = model.fit(X_train_np, y_train_np, epochs=20, verbose=0)
    
    _, accuracy = model.evaluate(X_test_np, y_test_np, verbose=0)
    return accuracy

# Ejecutar benchmarks
results_pytorch = benchmark_framework('PyTorch', train_pytorch, n_runs=3)
results_tf = benchmark_framework('TensorFlow', train_tensorflow, n_runs=3)

# Mostrar resultados
print("RESULTADOS:\n")
print(f"PyTorch:")
print(f"  Tiempo: {results_pytorch['avg_time']:.2f}s (+/- {results_pytorch['std_time']:.2f}s)")
print(f"  Accuracy: {results_pytorch['avg_accuracy']:.4f} (+/- {results_pytorch['std_accuracy']:.4f})")

print(f"\nTensorFlow:")
print(f"  Tiempo: {results_tf['avg_time']:.2f}s (+/- {results_tf['std_time']:.2f}s)")
print(f"  Accuracy: {results_tf['avg_accuracy']:.4f} (+/- {results_tf['std_accuracy']:.4f})")

# Visualizar
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

frameworks = ['PyTorch', 'TensorFlow']
times = [results_pytorch['avg_time'], results_tf['avg_time']]
time_errs = [results_pytorch['std_time'], results_tf['std_time']]

ax1.bar(frameworks, times, yerr=time_errs, capsize=10, color=['#EE4C2C', '#FF6F00'])
ax1.set_ylabel('Tiempo (s)')
ax1.set_title('Tiempo de Entrenamiento')
ax1.grid(True, alpha=0.3, axis='y')

accuracies = [results_pytorch['avg_accuracy'], results_tf['avg_accuracy']]
acc_errs = [results_pytorch['std_accuracy'], results_tf['std_accuracy']]

ax2.bar(frameworks, accuracies, yerr=acc_errs, capsize=10, color=['#EE4C2C', '#FF6F00'])
ax2.set_ylabel('Accuracy')
ax2.set_title('Accuracy Final')
ax2.set_ylim(0, 1)
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('/tmp/framework_comparison.png')
print("\n‚úì Gr√°fica guardada: framework_comparison.png")
```

**Resultados esperados:**
```
=== BENCHMARK FINAL ===

RESULTADOS:

PyTorch:
  Tiempo: 0.78s (+/- 0.12s)
  Accuracy: 0.8767 (+/- 0.0152)

TensorFlow:
  Tiempo: 1.15s (+/- 0.23s)
  Accuracy: 0.8833 (+/- 0.0208)

‚úì Gr√°fica guardada: framework_comparison.png
```

Los valores exactos variar√°n seg√∫n tu hardware. Lo importante es el **orden de magnitud** y la **variabilidad relativa**.

> üí° **Conclusi√≥n del benchmark:** Para datasets peque√±os en CPU, las diferencias entre frameworks son marginales. La elecci√≥n entre PyTorch y TensorFlow debe basarse en factores como: tu equipo, el ecosistema de herramientas, los requisitos de despliegue, y las arquitecturas que necesitas implementar ‚Äî no en benchmarks de velocidad en datasets de juguete.

### Nivel B√°sico

**Ejercicio 1:** Primera Red en PyTorch
```
Implementa una red 784 ‚Üí 128 ‚Üí 64 ‚Üí 10 para MNIST:
- Usa ReLU en capas ocultas
- Softmax en salida
- CrossEntropyLoss
- Entrena 10 √©pocas
```

**Ejercicio 2:** Primera Red en TensorFlow
```
Implementa la misma arquitectura en Keras:
- Usa Sequential API
- Compila con Adam optimizer
- Entrena con validation split
- Grafica historia de entrenamiento
```

**Ejercicio 3:** Comparaci√≥n Directa
```
Implementa el mismo modelo en ambos frameworks:
- Misma arquitectura
- Mismos hiperpar√°metros
- Compara tiempos y resultados
```

### Nivel Intermedio

**Ejercicio 4:** DataLoaders Personalizados
```
Crea un dataset personalizado:
- PyTorch: Implementa Dataset y DataLoader
- TensorFlow: Usa tf.data.Dataset
- Incluye augmentation de datos
- Batch processing eficiente
```

**Ejercicio 5:** Early Stopping
```
Implementa early stopping:
- PyTorch: Manualmente o con biblioteca
- TensorFlow: Usa Callbacks
- Compara implementaciones
- Guarda mejor modelo
```

**Ejercicio 6:** Transfer Learning
```
Usa modelo pre-entrenado:
- Carga ResNet o MobileNet
- Congela capas base
- A√±ade clasificador personalizado
- Fine-tuning gradual
```

### Nivel Avanzado

**Ejercicio 7:** Modelo Personalizado Complejo
```
Implementa arquitectura compleja:
- Skip connections (ResNet-style)
- Multiple inputs/outputs
- Custom training loop
- En ambos frameworks
```

**Ejercicio 8:** Optimizaci√≥n y Despliegue
```
Optimiza modelo para producci√≥n:
- Pruning/Quantization
- ONNX export (PyTorch)
- TF Lite conversion
- Benchmarks de inferencia
```

**Ejercicio 9:** Proyecto Completo
```
Pipeline end-to-end:
- Carga y preprocesamiento de datos
- Entrenamiento con validaci√≥n
- Evaluaci√≥n completa
- Guardado y deployment
- API de inferencia
```

## üìù Entregables

### 1. C√≥digo Fuente
- `pytorch_basics.py`: Fundamentos de PyTorch
- `tensorflow_basics.py`: Fundamentos de TensorFlow
- `comparison.py`: Comparaci√≥n de frameworks
- `advanced_features.py`: Funcionalidades avanzadas
- `experiments.ipynb`: Notebook comparativo

### 2. Modelos Entrenados
- Modelos guardados en ambos formatos
- Checkpoints de entrenamiento
- M√©tricas de evaluaci√≥n

### 3. Documentaci√≥n
- Gu√≠a de uso de cada framework
- Comparaci√≥n detallada
- Mejores pr√°cticas
- Troubleshooting common

### 4. Reporte Final (4-5 p√°ginas)
- Experiencia con cada framework
- Ventajas y desventajas
- Casos de uso recomendados
- Conclusiones y recomendaciones

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Conceive (Concebir) - 25%
- [ ] Comprensi√≥n de ventajas de frameworks
- [ ] Selecci√≥n apropiada de herramientas
- [ ] Dise√±o de experimentos comparativos
- [ ] Planificaci√≥n de arquitecturas

### Design (Dise√±ar) - 25%
- [ ] Implementaci√≥n correcta en PyTorch
- [ ] Implementaci√≥n correcta en TensorFlow
- [ ] Uso apropiado de APIs
- [ ] C√≥digo limpio y modular

### Implement (Implementar) - 30%
- [ ] Modelos entrenan correctamente
- [ ] Uso efectivo de autograd
- [ ] Aprovechamiento de utilidades
- [ ] Resultados reproducibles

### Operate (Operar) - 20%
- [ ] Comparaciones significativas
- [ ] An√°lisis cr√≠tico de resultados
- [ ] Optimizaci√≥n de performance
- [ ] Documentaci√≥n completa

## üìã R√∫brica de Evaluaci√≥n

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|--------------|---------------------|------------------|
| **PyTorch** | Dominio completo | Buen manejo | Uso b√°sico | Dificultades |
| **TensorFlow** | Dominio completo | Buen manejo | Uso b√°sico | Dificultades |
| **Comparaci√≥n** | An√°lisis profundo | Buena comparaci√≥n | Comparaci√≥n b√°sica | Comparaci√≥n pobre |
| **C√≥digo** | Excelente, modular | Bien estructurado | Funcional | Desorganizado |
| **Optimizaci√≥n** | Altamente optimizado | Bien optimizado | Optimizaci√≥n b√°sica | Sin optimizaci√≥n |

## üìö Referencias Adicionales

### Documentaci√≥n Oficial
- **PyTorch**: https://pytorch.org/docs/
- **TensorFlow**: https://www.tensorflow.org/api_docs
- **Keras**: https://keras.io/

### Tutoriales
- PyTorch Tutorials: https://pytorch.org/tutorials/
- TensorFlow Tutorials: https://www.tensorflow.org/tutorials
- Fast.ai: https://www.fast.ai/

### Libros
- "Deep Learning with PyTorch" (Stevens et al.)
- "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" (G√©ron)
- "Programming PyTorch for Deep Learning" (Rao)

### Comunidad
- PyTorch Forums
- TensorFlow Community
- Stack Overflow
- GitHub repositories

## üéì Notas Finales

### ¬øPyTorch o TensorFlow?

**Usa PyTorch si:**
- Trabajas en investigaci√≥n
- Necesitas m√°xima flexibilidad
- Prefieres c√≥digo pyth√≥nico
- Quieres debugging f√°cil

**Usa TensorFlow si:**
- Despliegas a producci√≥n
- Necesitas exportar a m√≥viles/web
- Trabajas en industria
- Quieres pipelines completos

**La verdad: Aprende ambos.** Son las herramientas est√°ndar.

### Del NumPy Manual a los Frameworks

Has recorrido un camino incre√≠ble:
1. ‚úÖ Implementaste todo desde cero (Labs 1-7)
2. ‚úÖ Entiendes profundamente c√≥mo funcionan las redes
3. ‚úÖ Ahora usas herramientas profesionales

**Este conocimiento profundo te hace un mejor practicante de deep learning.**

### Checklist de Frameworks

- [ ] Entiendo tensores y operaciones b√°sicas
- [ ] Puedo crear modelos en PyTorch
- [ ] Puedo crear modelos en TensorFlow/Keras
- [ ] S√© usar autograd
- [ ] Entiendo DataLoaders y Datasets
- [ ] Puedo guardar/cargar modelos
- [ ] S√© usar callbacks y early stopping
- [ ] Puedo optimizar para producci√≥n

### Pr√≥ximos Pasos

**Contin√∫a aprendiendo:**
- Arquitecturas avanzadas (ResNet, Transformers)
- Visi√≥n por computadora (CNNs)
- PLN (RNNs, Transformers)
- IA Generativa (GANs, VAEs, Diffusion)

**Proyectos recomendados:**
- Clasificador de im√°genes personalizado
- Chatbot con RNNs
- Detector de objetos
- Sistema de recomendaci√≥n

### Reflexi√≥n Final

**Los frameworks no reemplazan el conocimiento profundo - lo amplifican.**

Ahora que entiendes los fundamentos, los frameworks te permiten:
- Iterar m√°s r√°pido
- Experimentar con arquitecturas complejas
- Deployar modelos en producci√≥n
- Competir con estado del arte

¬°Usa este poder sabiamente! üöÄ

---

**"The best way to learn deep learning is to do deep learning." - Andrew Ng**

**¬°Los frameworks hacen el deep learning accesible! üöÄ**
