# Gu√≠a de Laboratorio: Frameworks de Deep Learning

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Fundamentos de Deep Learning - Frameworks Modernos  
**C√≥digo:** Lab 08  
**Duraci√≥n:** 3-4 horas  
**Nivel:** Intermedio-Avanzado  

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender las ventajas de usar frameworks de deep learning
2. Implementar redes neuronales en PyTorch desde cero
3. Implementar redes neuronales en TensorFlow/Keras
4. Utilizar diferenciaci√≥n autom√°tica (Autograd)
5. Aprovechar aceleraci√≥n con GPU
6. Comparar PyTorch y TensorFlow en casos pr√°cticos
7. Migrar c√≥digo desde implementaciones NumPy a frameworks
8. Usar utilidades modernas (DataLoaders, Optimizers, etc.)
9. Entrenar modelos de manera eficiente
10. Guardar, cargar y desplegar modelos

## üìö Prerrequisitos

### Conocimientos

- Python avanzado (POO, decoradores)
- NumPy s√≥lido (todos los labs anteriores)
- Redes neuronales, backpropagation, entrenamiento
- Conceptos de GPU computing (b√°sicos)

### Software

- Python 3.8+
- PyTorch 1.9+ (`pip install torch torchvision`)
- TensorFlow 2.6+ (`pip install tensorflow`)
- NumPy, Matplotlib
- CUDA (opcional, para GPU)

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` - Comparaci√≥n de frameworks
- `README.md` - Recursos y estructura
- Documentaci√≥n oficial de PyTorch y TensorFlow

## üìñ Introducci√≥n

### Del C√≥digo Manual a los Frameworks

Felicidades! Has llegado lejos implementando todo desde cero:
- ‚úì Neuronas y capas (Lab 01)
- ‚úì Funciones de activaci√≥n (Lab 03)
- ‚úì Funciones de p√©rdida (Lab 04)
- ‚úì Backpropagation (Lab 05)
- ‚úì Entrenamiento completo (Lab 06)
- ‚úì Evaluaci√≥n rigurosa (Lab 07)

**Ahora es tiempo de usar las herramientas profesionales.**

### ¬øPor Qu√© Usar Frameworks?

**Sin frameworks (lo que has hecho):**
```python
# Implementar forward pass
z = np.dot(W, x) + b
a = sigmoid(z)

# Implementar backward pass
dz = a - y
dW = np.dot(dz, x.T)
db = np.sum(dz)

# Actualizar manualmente
W -= learning_rate * dW
b -= learning_rate * db
```

**Con frameworks:**
```python
# PyTorch hace todo autom√°ticamente
output = model(x)
loss = criterion(output, y)
loss.backward()  # ¬°Gradientes autom√°ticos!
optimizer.step()  # ¬°Actualizaci√≥n autom√°tica!
```

### Ventajas Principales

**1. Autograd (Diferenciaci√≥n Autom√°tica)**
- No m√°s backpropagation manual
- Sin errores en derivadas
- Soporta operaciones complejas

**2. Optimizaci√≥n de Performance**
- Operaciones optimizadas en C++/CUDA
- Paralelizaci√≥n autom√°tica
- 10-100x m√°s r√°pido que NumPy

**3. GPU Acceleration**
```python
# Mover a GPU (una l√≠nea!)
model = model.to('cuda')
```

**4. Ecosistema Rico**
- Modelos pre-entrenados (ResNet, BERT, GPT)
- Data loaders optimizados
- Herramientas de visualizaci√≥n
- Comunidad masiva

**5. Productizaci√≥n**
- Guardar/cargar modelos f√°cilmente
- Desplegar en servidores
- Exportar a m√≥viles (TF Lite)
- Optimizar para inferencia

### PyTorch vs TensorFlow

| Caracter√≠stica | PyTorch | TensorFlow/Keras |
|---------------|---------|------------------|
| **Filosof√≠a** | Investigaci√≥n, Pyth√≥nico | Producci√≥n, Escalable |
| **Curva de aprendizaje** | M√°s f√°cil | Media (Keras f√°cil) |
| **Debugging** | Excelente | Bueno |
| **Popularidad investigaci√≥n** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Popularidad industria** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Documentaci√≥n** | Excelente | Excelente |
| **Despliegue** | Mejorando | Excelente |

**Recomendaci√≥n:** Aprende ambos! Son las herramientas est√°ndar de la industria.

### Aplicaciones en el Mundo Real

**Todos los modelos modernos usan frameworks:**
- **GPT-3/4**: Entrenado con frameworks
- **Stable Diffusion**: PyTorch
- **BERT, T5**: TensorFlow
- **AlphaGo**: TensorFlow
- **DALL-E**: PyTorch
- **99% de papers de investigaci√≥n**: PyTorch o TensorFlow

## ü§î Preguntas de Reflexi√≥n Iniciales

1. ¬øPor qu√© no seguir implementando todo manualmente?
2. ¬øQu√© significa "diferenciaci√≥n autom√°tica"?
3. ¬øC√≥mo puede un framework ser 100x m√°s r√°pido?
4. ¬øCu√°l es el trade-off entre control y conveniencia?
5. ¬øPyTorch o TensorFlow para tu proyecto?

## üî¨ Parte 1: PyTorch Fundamentals (60 min)

### 1.1 Tensores: Los Bloques B√°sicos

```python
import torch
import numpy as np

print("=== TENSORES EN PYTORCH ===\n")

# Crear tensores
x = torch.tensor([1, 2, 3, 4])
print(f"Tensor 1D: {x}")
print(f"Shape: {x.shape}, dtype: {x.dtype}\n")

# Tensor 2D
matrix = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float32)
print(f"Tensor 2D:\n{matrix}")
print(f"Shape: {matrix.shape}\n")

# Tensores especiales
zeros = torch.zeros(3, 4)
ones = torch.ones(2, 3)
randn = torch.randn(3, 3)  # Distribuci√≥n normal
rand = torch.rand(2, 2)     # Distribuci√≥n uniforme [0, 1]

print(f"Zeros:\n{zeros}\n")
print(f"Random normal:\n{randn}\n")

# Conversi√≥n NumPy ‚Üî PyTorch
np_array = np.array([1, 2, 3])
torch_tensor = torch.from_numpy(np_array)
back_to_numpy = torch_tensor.numpy()

print(f"NumPy ‚Üí Torch ‚Üí NumPy: {back_to_numpy}\n")

# Operaciones b√°sicas
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

print(f"Suma: {a + b}")
print(f"Producto elemento-wise: {a * b}")
print(f"Producto punto: {torch.dot(a, b)}")
print(f"Matriz @ vector: {torch.randn(3, 4) @ torch.randn(4)}")
```

### 1.2 Autograd: El Coraz√≥n de PyTorch

```python
print("\n=== AUTOGRAD: DIFERENCIACI√ìN AUTOM√ÅTICA ===\n")

# Ejemplo 1: Derivada simple
x = torch.tensor(3.0, requires_grad=True)  # Activar tracking de gradientes
print(f"x = {x}")

# Forward: y = x¬≤
y = x ** 2
print(f"y = x¬≤ = {y}")

# Backward: calcular dy/dx
y.backward()
print(f"dy/dx = 2x = {x.grad}")  # Deber√≠a ser 2*3 = 6

print("\n--- Ejemplo 2: Funci√≥n m√°s compleja ---")

# Reset
x = torch.tensor(2.0, requires_grad=True)
w = torch.tensor(3.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)

# z = w*x + b
z = w * x + b
print(f"z = w*x + b = {z}")

# Backward
z.backward()

print(f"‚àÇz/‚àÇx = w = {x.grad}")  # = w = 3
print(f"‚àÇz/‚àÇw = x = {w.grad}")  # = x = 2
print(f"‚àÇz/‚àÇb = 1 = {b.grad}")  # = 1

print("\n--- Ejemplo 3: Red neuronal simple ---")

# Input
x = torch.randn(1, 10, requires_grad=True)
print(f"Input shape: {x.shape}")

# Par√°metros
W1 = torch.randn(10, 5, requires_grad=True)
b1 = torch.randn(1, 5, requires_grad=True)

W2 = torch.randn(5, 1, requires_grad=True)
b2 = torch.randn(1, 1, requires_grad=True)

# Forward
h = torch.relu(x @ W1 + b1)  # Capa oculta
y = h @ W2 + b2               # Salida

# Simular p√©rdida
target = torch.tensor([[1.0]])
loss = (y - target) ** 2

print(f"Loss: {loss.item():.4f}")

# Backward: ¬°calcula TODOS los gradientes autom√°ticamente!
loss.backward()

print(f"Gradiente de W1: {W1.grad.shape}")  # (10, 5)
print(f"Gradiente de W2: {W2.grad.shape}")  # (5, 1)
print("‚úì Gradientes calculados autom√°ticamente!")
```

### 1.3 Primera Red Neuronal en PyTorch

```python
import torch.nn as nn
import torch.optim as optim

print("\n=== PRIMERA RED NEURONAL EN PYTORCH ===\n")

# Definir arquitectura
class SimpleNet(nn.Module):
    """Red neuronal simple: 10 ‚Üí 20 ‚Üí 1"""
    
    def __init__(self):
        super(SimpleNet, self).__init__()
        # Definir capas
        self.fc1 = nn.Linear(10, 20)  # Capa 1: 10 ‚Üí 20
        self.fc2 = nn.Linear(20, 1)   # Capa 2: 20 ‚Üí 1
    
    def forward(self, x):
        """Forward pass"""
        x = torch.relu(self.fc1(x))  # Activaci√≥n ReLU
        x = self.fc2(x)               # Sin activaci√≥n en salida
        return x

# Instanciar modelo
model = SimpleNet()
print(model)
print()

# Ver par√°metros
total_params = sum(p.numel() for p in model.parameters())
print(f"Par√°metros totales: {total_params}")

# Listar par√°metros
for name, param in model.named_parameters():
    print(f"{name:10s}: {param.shape}")

print("\n--- Entrenar el modelo ---")

# Generar datos sint√©ticos
X_train = torch.randn(100, 10)
y_train = torch.randn(100, 1)

# Funci√≥n de p√©rdida y optimizador
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Loop de entrenamiento
for epoch in range(100):
    # Forward pass
    predictions = model(X_train)
    loss = criterion(predictions, y_train)
    
    # Backward pass
    optimizer.zero_grad()  # Limpiar gradientes
    loss.backward()        # Calcular gradientes
    optimizer.step()       # Actualizar par√°metros
    
    if epoch % 20 == 0:
        print(f"Epoch {epoch:3d}, Loss: {loss.item():.4f}")

print("\n‚úì Modelo entrenado!")
```

**Actividad 1.1:** Crea una red 20 ‚Üí 50 ‚Üí 30 ‚Üí 10 con ReLU y entr√©nala en un problema de regresi√≥n.

### 1.4 Clasificaci√≥n con PyTorch

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

print("\n=== CLASIFICACI√ìN BINARIA CON PYTORCH ===\n")

# Generar datos
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convertir a tensores
X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train).reshape(-1, 1)
X_test = torch.FloatTensor(X_test)
y_test = torch.FloatTensor(y_test).reshape(-1, 1)

print(f"Train: {X_train.shape}, Test: {X_test.shape}\n")

# Definir modelo
class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.3),  # Regularizaci√≥n
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
            nn.Sigmoid()  # Salida [0, 1]
        )
    
    def forward(self, x):
        return self.network(x)

# Crear modelo
model = BinaryClassifier(input_dim=20)
print(model)
print()

# Setup entrenamiento
criterion = nn.BCELoss()  # Binary Cross-Entropy
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Entrenar
epochs = 100
for epoch in range(epochs):
    # Training mode
    model.train()
    
    # Forward
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Evaluar cada 10 √©pocas
    if epoch % 10 == 0:
        model.eval()  # Evaluation mode
        with torch.no_grad():  # No calcular gradientes
            test_outputs = model(X_test)
            test_loss = criterion(test_outputs, y_test)
            
            # Accuracy
            predictions = (test_outputs > 0.5).float()
            accuracy = (predictions == y_test).float().mean()
        
        print(f"Epoch {epoch:3d} | Train Loss: {loss.item():.4f} | "
              f"Test Loss: {test_loss.item():.4f} | Acc: {accuracy.item():.4f}")

print("\n‚úì Clasificador entrenado!")
```

**Actividad 1.2:** Modifica el modelo para clasificaci√≥n multiclase (3+ clases) usando Softmax.

## üî¨ Parte 2: TensorFlow/Keras Fundamentals (60 min)

### 2.1 Introducci√≥n a TensorFlow

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

print("=== TENSORFLOW/KERAS BASICS ===\n")
print(f"TensorFlow version: {tf.__version__}\n")

# Tensores en TensorFlow
t1 = tf.constant([1, 2, 3, 4])
t2 = tf.constant([[1, 2], [3, 4]])

print(f"Tensor 1D: {t1}")
print(f"Tensor 2D:\n{t2}\n")

# Operaciones
a = tf.constant([1.0, 2.0, 3.0])
b = tf.constant([4.0, 5.0, 6.0])

print(f"Suma: {a + b}")
print(f"Producto: {a * b}")
print(f"Matmul: {tf.linalg.matmul(tf.reshape(a, (3, 1)), tf.reshape(b, (1, 3)))}")
```

### 2.2 Primera Red con Keras Sequential API

```python
print("\n=== RED NEURONAL CON KERAS SEQUENTIAL ===\n")

# Definir modelo (API secuencial - m√°s simple)
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

# Ver arquitectura
model.summary()

# Compilar
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Generar datos (NumPy)
X_train_np, y_train_np = make_classification(
    n_samples=1000, n_features=20, n_classes=2, random_state=42
)
X_test_np = X_train_np[:200]
y_test_np = y_train_np[:200]
X_train_np = X_train_np[200:]
y_train_np = y_train_np[200:]

# Entrenar
print("\nEntrenando...")
history = model.fit(
    X_train_np, y_train_np,
    epochs=50,
    batch_size=32,
    validation_data=(X_test_np, y_test_np),
    verbose=0  # Silencioso
)

# Evaluar
test_loss, test_acc = model.evaluate(X_test_np, y_test_np, verbose=0)
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# Predicciones
predictions = model.predict(X_test_np[:5])
print(f"\nPrimeras 5 predicciones:\n{predictions.flatten()}")
```

### 2.3 Keras Functional API (M√°s Flexible)

```python
print("\n=== KERAS FUNCTIONAL API ===\n")

# Input layer
inputs = keras.Input(shape=(20,))

# Hidden layers
x = layers.Dense(64, activation='relu')(inputs)
x = layers.Dropout(0.3)(x)
x = layers.Dense(32, activation='relu')(x)
x = layers.Dropout(0.3)(x)

# Output layer
outputs = layers.Dense(1, activation='sigmoid')(x)

# Crear modelo
model_functional = keras.Model(inputs=inputs, outputs=outputs)

model_functional.summary()

# Compilar y entrenar igual que antes
model_functional.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model_functional.fit(
    X_train_np, y_train_np,
    epochs=50,
    batch_size=32,
    validation_data=(X_test_np, y_test_np),
    verbose=0
)

print(f"Final val accuracy: {history.history['val_accuracy'][-1]:.4f}")
```

### 2.4 Subclassing (M√°ximo Control)

```python
print("\n=== MODEL SUBCLASSING ===\n")

class CustomModel(keras.Model):
    """Modelo personalizado con subclassing"""
    
    def __init__(self):
        super(CustomModel, self).__init__()
        self.dense1 = layers.Dense(64, activation='relu')
        self.dropout1 = layers.Dropout(0.3)
        self.dense2 = layers.Dense(32, activation='relu')
        self.dropout2 = layers.Dropout(0.3)
        self.output_layer = layers.Dense(1, activation='sigmoid')
    
    def call(self, inputs, training=False):
        """Forward pass"""
        x = self.dense1(inputs)
        x = self.dropout1(x, training=training)  # Dropout solo en training
        x = self.dense2(x)
        x = self.dropout2(x, training=training)
        return self.output_layer(x)

# Crear y entrenar
model_custom = CustomModel()

model_custom.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Necesita build con una llamada o especificando input_shape
model_custom.build(input_shape=(None, 20))
model_custom.summary()

history = model_custom.fit(
    X_train_np, y_train_np,
    epochs=50,
    batch_size=32,
    validation_data=(X_test_np, y_test_np),
    verbose=0
)

print(f"Final accuracy: {history.history['val_accuracy'][-1]:.4f}")
```

**Actividad 2.1:** Crea un modelo con arquitectura residual (skip connections) usando Functional API.

## üî¨ Parte 3: Comparaci√≥n PyTorch vs TensorFlow (40 min)

### 3.1 Mismo Modelo en Ambos Frameworks

```python
print("\n=== COMPARACI√ìN LADO A LADO ===\n")

# ----- PYTORCH -----
print("1. PYTORCH\n")

class PyTorchModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(20, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.layers(x)

pytorch_model = PyTorchModel()
print(f"Par√°metros PyTorch: {sum(p.numel() for p in pytorch_model.parameters())}")

# ----- TENSORFLOW -----
print("\n2. TENSORFLOW/KERAS\n")

tensorflow_model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

tensorflow_model.build(input_shape=(None, 20))
print(f"Par√°metros TensorFlow: {tensorflow_model.count_params()}")

# ----- ENTRENAR AMBOS -----
import time

# Datos
X_train_torch = torch.FloatTensor(X_train_np)
y_train_torch = torch.FloatTensor(y_train_np).reshape(-1, 1)

# PyTorch
print("\n--- Entrenando PyTorch ---")
criterion = nn.BCELoss()
optimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)

start = time.time()
for epoch in range(50):
    outputs = pytorch_model(X_train_torch)
    loss = criterion(outputs, y_train_torch)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

pytorch_time = time.time() - start
print(f"Tiempo PyTorch: {pytorch_time:.2f}s")

# TensorFlow
print("\n--- Entrenando TensorFlow ---")
tensorflow_model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

start = time.time()
tensorflow_model.fit(
    X_train_np, y_train_np,
    epochs=50,
    batch_size=32,
    verbose=0
)
tensorflow_time = time.time() - start
print(f"Tiempo TensorFlow: {tensorflow_time:.2f}s")

print(f"\n--- Comparaci√≥n ---")
print(f"PyTorch:    {pytorch_time:.2f}s")
print(f"TensorFlow: {tensorflow_time:.2f}s")
```

### 3.2 DataLoaders y Pipelines

```python
print("\n=== DATA LOADING ===\n")

# ----- PYTORCH DATALOADER -----
from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    """Dataset personalizado para PyTorch"""
    
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y).reshape(-1, 1)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Crear dataset y dataloader
train_dataset = CustomDataset(X_train_np, y_train_np)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

print("PyTorch DataLoader:")
for batch_X, batch_y in train_loader:
    print(f"  Batch shape: X={batch_X.shape}, y={batch_y.shape}")
    break

# ----- TENSORFLOW DATASET -----
train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train_np, y_train_np))
train_dataset_tf = train_dataset_tf.shuffle(1000).batch(32)

print("\nTensorFlow Dataset:")
for batch_X, batch_y in train_dataset_tf.take(1):
    print(f"  Batch shape: X={batch_X.shape}, y={batch_y.shape}")
```

**Actividad 3.1:** Implementa el mismo modelo en ambos frameworks y compara resultados.

## üî¨ Parte 4: Funcionalidades Avanzadas (50 min)

### 4.1 Guardar y Cargar Modelos

```python
print("\n=== GUARDAR Y CARGAR MODELOS ===\n")

# ----- PYTORCH -----
print("1. PyTorch\n")

# Guardar
torch.save(pytorch_model.state_dict(), '/tmp/pytorch_model.pth')
print("‚úì Modelo guardado: pytorch_model.pth")

# Cargar
loaded_pytorch_model = PyTorchModel()
loaded_pytorch_model.load_state_dict(torch.load('/tmp/pytorch_model.pth'))
loaded_pytorch_model.eval()
print("‚úì Modelo cargado")

# Verificar que funciona
test_input = torch.randn(1, 20)
output = loaded_pytorch_model(test_input)
print(f"Predicci√≥n de prueba: {output.item():.4f}\n")

# ----- TENSORFLOW -----
print("2. TensorFlow\n")

# Guardar (varios formatos)
tensorflow_model.save('/tmp/tf_model.h5')  # HDF5
print("‚úì Modelo guardado: tf_model.h5")

# Cargar
loaded_tf_model = keras.models.load_model('/tmp/tf_model.h5')
print("‚úì Modelo cargado")

# Verificar
test_input_tf = np.random.randn(1, 20)
output_tf = loaded_tf_model.predict(test_input_tf, verbose=0)
print(f"Predicci√≥n de prueba: {output_tf[0][0]:.4f}")
```

### 4.2 Callbacks y Early Stopping

```python
print("\n=== CALLBACKS (TENSORFLOW) ===\n")

# Crear modelo fresco
model_with_callbacks = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

model_with_callbacks.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Definir callbacks
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        verbose=1
    ),
    keras.callbacks.ModelCheckpoint(
        '/tmp/best_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
]

# Entrenar con callbacks
history = model_with_callbacks.fit(
    X_train_np, y_train_np,
    epochs=100,  # Muchas √©pocas, pero early stopping lo detendr√°
    batch_size=32,
    validation_data=(X_test_np, y_test_np),
    callbacks=callbacks,
    verbose=0
)

print(f"\n√âpocas entrenadas: {len(history.history['loss'])}")
print(f"Mejor val_accuracy: {max(history.history['val_accuracy']):.4f}")
```

### 4.3 Visualizaci√≥n con TensorBoard

```python
print("\n=== TENSORBOARD ===\n")

# Crear callback de TensorBoard
tensorboard_callback = keras.callbacks.TensorBoard(
    log_dir='/tmp/logs',
    histogram_freq=1
)

# Entrenar con logging
model_tb = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

model_tb.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model_tb.fit(
    X_train_np, y_train_np,
    epochs=20,
    validation_data=(X_test_np, y_test_np),
    callbacks=[tensorboard_callback],
    verbose=0
)

print("‚úì Logs guardados en /tmp/logs")
print("Para visualizar: tensorboard --logdir=/tmp/logs")
```

### 4.4 Transfer Learning B√°sico

```python
print("\n=== TRANSFER LEARNING (EJEMPLO) ===\n")

# Usando modelo pre-entrenado de Keras
from tensorflow.keras.applications import MobileNetV2

# Cargar modelo pre-entrenado (sin top/clasificador)
base_model = MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)

# Congelar pesos del modelo base
base_model.trainable = False

# A√±adir clasificador personalizado
inputs = keras.Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(10, activation='softmax')(x)  # 10 clases

transfer_model = keras.Model(inputs, outputs)

print("Modelo con transfer learning:")
print(f"  Par√°metros totales: {transfer_model.count_params():,}")
print(f"  Par√°metros entrenables: {sum(tf.size(w).numpy() for w in transfer_model.trainable_weights):,}")
print(f"  Par√°metros congelados: {sum(tf.size(w).numpy() for w in transfer_model.non_trainable_weights):,}")
```

**Actividad 4.1:** Implementa un sistema de checkpointing en PyTorch similar a los callbacks de Keras.

## üìä An√°lisis Final de Rendimiento

### Benchmark Completo

```python
import matplotlib.pyplot as plt

print("\n=== BENCHMARK FINAL ===\n")

def benchmark_framework(framework_name, train_fn, n_runs=5):
    """Benchmark de un framework"""
    times = []
    accuracies = []
    
    for run in range(n_runs):
        start = time.time()
        accuracy = train_fn()
        elapsed = time.time() - start
        
        times.append(elapsed)
        accuracies.append(accuracy)
    
    return {
        'framework': framework_name,
        'avg_time': np.mean(times),
        'std_time': np.std(times),
        'avg_accuracy': np.mean(accuracies),
        'std_accuracy': np.std(accuracies)
    }

# Definir funciones de entrenamiento
def train_pytorch():
    model = PyTorchModel()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.BCELoss()
    
    for epoch in range(20):
        outputs = model(X_train_torch)
        loss = criterion(outputs, y_train_torch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # Evaluar
    model.eval()
    with torch.no_grad():
        preds = model(torch.FloatTensor(X_test_np))
        preds = (preds > 0.5).float()
        accuracy = (preds.numpy().flatten() == y_test_np).mean()
    
    return accuracy

def train_tensorflow():
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(20,)),
        layers.Dropout(0.3),
        layers.Dense(32, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(1, activation='sigmoid')
    ])
    
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    history = model.fit(X_train_np, y_train_np, epochs=20, verbose=0)
    
    _, accuracy = model.evaluate(X_test_np, y_test_np, verbose=0)
    return accuracy

# Ejecutar benchmarks
results_pytorch = benchmark_framework('PyTorch', train_pytorch, n_runs=3)
results_tf = benchmark_framework('TensorFlow', train_tensorflow, n_runs=3)

# Mostrar resultados
print("RESULTADOS:\n")
print(f"PyTorch:")
print(f"  Tiempo: {results_pytorch['avg_time']:.2f}s (+/- {results_pytorch['std_time']:.2f}s)")
print(f"  Accuracy: {results_pytorch['avg_accuracy']:.4f} (+/- {results_pytorch['std_accuracy']:.4f})")

print(f"\nTensorFlow:")
print(f"  Tiempo: {results_tf['avg_time']:.2f}s (+/- {results_tf['std_time']:.2f}s)")
print(f"  Accuracy: {results_tf['avg_accuracy']:.4f} (+/- {results_tf['std_accuracy']:.4f})")

# Visualizar
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

frameworks = ['PyTorch', 'TensorFlow']
times = [results_pytorch['avg_time'], results_tf['avg_time']]
time_errs = [results_pytorch['std_time'], results_tf['std_time']]

ax1.bar(frameworks, times, yerr=time_errs, capsize=10, color=['#EE4C2C', '#FF6F00'])
ax1.set_ylabel('Tiempo (s)')
ax1.set_title('Tiempo de Entrenamiento')
ax1.grid(True, alpha=0.3, axis='y')

accuracies = [results_pytorch['avg_accuracy'], results_tf['avg_accuracy']]
acc_errs = [results_pytorch['std_accuracy'], results_tf['std_accuracy']]

ax2.bar(frameworks, accuracies, yerr=acc_errs, capsize=10, color=['#EE4C2C', '#FF6F00'])
ax2.set_ylabel('Accuracy')
ax2.set_title('Accuracy Final')
ax2.set_ylim(0, 1)
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('/tmp/framework_comparison.png')
print("\n‚úì Gr√°fica guardada: framework_comparison.png")
```

## üéØ EJERCICIOS PROPUESTOS

### Nivel B√°sico

**Ejercicio 1:** Primera Red en PyTorch
```
Implementa una red 784 ‚Üí 128 ‚Üí 64 ‚Üí 10 para MNIST:
- Usa ReLU en capas ocultas
- Softmax en salida
- CrossEntropyLoss
- Entrena 10 √©pocas
```

**Ejercicio 2:** Primera Red en TensorFlow
```
Implementa la misma arquitectura en Keras:
- Usa Sequential API
- Compila con Adam optimizer
- Entrena con validation split
- Grafica historia de entrenamiento
```

**Ejercicio 3:** Comparaci√≥n Directa
```
Implementa el mismo modelo en ambos frameworks:
- Misma arquitectura
- Mismos hiperpar√°metros
- Compara tiempos y resultados
```

### Nivel Intermedio

**Ejercicio 4:** DataLoaders Personalizados
```
Crea un dataset personalizado:
- PyTorch: Implementa Dataset y DataLoader
- TensorFlow: Usa tf.data.Dataset
- Incluye augmentation de datos
- Batch processing eficiente
```

**Ejercicio 5:** Early Stopping
```
Implementa early stopping:
- PyTorch: Manualmente o con biblioteca
- TensorFlow: Usa Callbacks
- Compara implementaciones
- Guarda mejor modelo
```

**Ejercicio 6:** Transfer Learning
```
Usa modelo pre-entrenado:
- Carga ResNet o MobileNet
- Congela capas base
- A√±ade clasificador personalizado
- Fine-tuning gradual
```

### Nivel Avanzado

**Ejercicio 7:** Modelo Personalizado Complejo
```
Implementa arquitectura compleja:
- Skip connections (ResNet-style)
- Multiple inputs/outputs
- Custom training loop
- En ambos frameworks
```

**Ejercicio 8:** Optimizaci√≥n y Despliegue
```
Optimiza modelo para producci√≥n:
- Pruning/Quantization
- ONNX export (PyTorch)
- TF Lite conversion
- Benchmarks de inferencia
```

**Ejercicio 9:** Proyecto Completo
```
Pipeline end-to-end:
- Carga y preprocesamiento de datos
- Entrenamiento con validaci√≥n
- Evaluaci√≥n completa
- Guardado y deployment
- API de inferencia
```

## üìù Entregables

### 1. C√≥digo Fuente
- `pytorch_basics.py`: Fundamentos de PyTorch
- `tensorflow_basics.py`: Fundamentos de TensorFlow
- `comparison.py`: Comparaci√≥n de frameworks
- `advanced_features.py`: Funcionalidades avanzadas
- `experiments.ipynb`: Notebook comparativo

### 2. Modelos Entrenados
- Modelos guardados en ambos formatos
- Checkpoints de entrenamiento
- M√©tricas de evaluaci√≥n

### 3. Documentaci√≥n
- Gu√≠a de uso de cada framework
- Comparaci√≥n detallada
- Mejores pr√°cticas
- Troubleshooting common

### 4. Reporte Final (4-5 p√°ginas)
- Experiencia con cada framework
- Ventajas y desventajas
- Casos de uso recomendados
- Conclusiones y recomendaciones

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Conceive (Concebir) - 25%
- [ ] Comprensi√≥n de ventajas de frameworks
- [ ] Selecci√≥n apropiada de herramientas
- [ ] Dise√±o de experimentos comparativos
- [ ] Planificaci√≥n de arquitecturas

### Design (Dise√±ar) - 25%
- [ ] Implementaci√≥n correcta en PyTorch
- [ ] Implementaci√≥n correcta en TensorFlow
- [ ] Uso apropiado de APIs
- [ ] C√≥digo limpio y modular

### Implement (Implementar) - 30%
- [ ] Modelos entrenan correctamente
- [ ] Uso efectivo de autograd
- [ ] Aprovechamiento de utilidades
- [ ] Resultados reproducibles

### Operate (Operar) - 20%
- [ ] Comparaciones significativas
- [ ] An√°lisis cr√≠tico de resultados
- [ ] Optimizaci√≥n de performance
- [ ] Documentaci√≥n completa

## üìã R√∫brica de Evaluaci√≥n

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|--------------|---------------------|------------------|
| **PyTorch** | Dominio completo | Buen manejo | Uso b√°sico | Dificultades |
| **TensorFlow** | Dominio completo | Buen manejo | Uso b√°sico | Dificultades |
| **Comparaci√≥n** | An√°lisis profundo | Buena comparaci√≥n | Comparaci√≥n b√°sica | Comparaci√≥n pobre |
| **C√≥digo** | Excelente, modular | Bien estructurado | Funcional | Desorganizado |
| **Optimizaci√≥n** | Altamente optimizado | Bien optimizado | Optimizaci√≥n b√°sica | Sin optimizaci√≥n |

## üìö Referencias Adicionales

### Documentaci√≥n Oficial
- **PyTorch**: https://pytorch.org/docs/
- **TensorFlow**: https://www.tensorflow.org/api_docs
- **Keras**: https://keras.io/

### Tutoriales
- PyTorch Tutorials: https://pytorch.org/tutorials/
- TensorFlow Tutorials: https://www.tensorflow.org/tutorials
- Fast.ai: https://www.fast.ai/

### Libros
- "Deep Learning with PyTorch" (Stevens et al.)
- "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" (G√©ron)
- "Programming PyTorch for Deep Learning" (Rao)

### Comunidad
- PyTorch Forums
- TensorFlow Community
- Stack Overflow
- GitHub repositories

## üéì Notas Finales

### ¬øPyTorch o TensorFlow?

**Usa PyTorch si:**
- Trabajas en investigaci√≥n
- Necesitas m√°xima flexibilidad
- Prefieres c√≥digo pyth√≥nico
- Quieres debugging f√°cil

**Usa TensorFlow si:**
- Despliegas a producci√≥n
- Necesitas exportar a m√≥viles/web
- Trabajas en industria
- Quieres pipelines completos

**La verdad: Aprende ambos.** Son las herramientas est√°ndar.

### Del NumPy Manual a los Frameworks

Has recorrido un camino incre√≠ble:
1. ‚úÖ Implementaste todo desde cero (Labs 1-7)
2. ‚úÖ Entiendes profundamente c√≥mo funcionan las redes
3. ‚úÖ Ahora usas herramientas profesionales

**Este conocimiento profundo te hace un mejor practicante de deep learning.**

### Checklist de Frameworks

- [ ] Entiendo tensores y operaciones b√°sicas
- [ ] Puedo crear modelos en PyTorch
- [ ] Puedo crear modelos en TensorFlow/Keras
- [ ] S√© usar autograd
- [ ] Entiendo DataLoaders y Datasets
- [ ] Puedo guardar/cargar modelos
- [ ] S√© usar callbacks y early stopping
- [ ] Puedo optimizar para producci√≥n

### Pr√≥ximos Pasos

**Contin√∫a aprendiendo:**
- Arquitecturas avanzadas (ResNet, Transformers)
- Visi√≥n por computadora (CNNs)
- PLN (RNNs, Transformers)
- IA Generativa (GANs, VAEs, Diffusion)

**Proyectos recomendados:**
- Clasificador de im√°genes personalizado
- Chatbot con RNNs
- Detector de objetos
- Sistema de recomendaci√≥n

### Reflexi√≥n Final

**Los frameworks no reemplazan el conocimiento profundo - lo amplifican.**

Ahora que entiendes los fundamentos, los frameworks te permiten:
- Iterar m√°s r√°pido
- Experimentar con arquitecturas complejas
- Deployar modelos en producci√≥n
- Competir con estado del arte

¬°Usa este poder sabiamente! üöÄ

---

**"The best way to learn deep learning is to do deep learning." - Andrew Ng**

**¬°Los frameworks hacen el deep learning accesible! üöÄ**
