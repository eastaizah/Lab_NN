# Resumen de Laboratorios 03-08: Redes Neuronales desde Cero

## ğŸ“š Contenido Creado

Este documento resume todos los laboratorios creados para el curso de Redes Neuronales.

---

## Lab 03: Funciones de ActivaciÃ³n

### Contenido
- **teoria.md**: ExplicaciÃ³n detallada de ReLU, Sigmoid, Tanh, Softmax, Leaky ReLU
- **codigo/activaciones.py**: ImplementaciÃ³n completa con clases modulares
- **practica.ipynb**: Notebook interactivo con ejercicios
- **README.md**: GuÃ­a del laboratorio

### Conceptos Clave
- Por quÃ© necesitamos funciones de activaciÃ³n (no linealidad)
- ComparaciÃ³n de funciones: ventajas y desventajas
- Problema del gradiente que desaparece
- ReLU como estÃ¡ndar moderno
- Softmax para clasificaciÃ³n multiclase

### Ejercicios Incluidos
1. Implementar funciones de activaciÃ³n
2. Visualizar funciones y derivadas
3. Comparar saturaciÃ³n de gradientes
4. Verificar gradientes numÃ©ricamente
5. Implementar Leaky ReLU

### Archivos
```
Lab03_Funciones_Activacion/
â”œâ”€â”€ README.md (6.5 KB)
â”œâ”€â”€ teoria.md (6.4 KB)
â”œâ”€â”€ practica.ipynb (interactivo)
â””â”€â”€ codigo/
    â””â”€â”€ activaciones.py (10.7 KB)
```

---

## Lab 04: Funciones de PÃ©rdida y OptimizaciÃ³n

### Contenido
- **teoria.md**: MSE, MAE, Cross-Entropy, Gradient Descent
- **codigo/perdida.py**: ImplementaciÃ³n de pÃ©rdidas y optimizaciÃ³n
- **practica.ipynb**: Experimentos con pÃ©rdidas
- **README.md**: GuÃ­a completa

### Conceptos Clave
- MSE para regresiÃ³n
- Binary Cross-Entropy para clasificaciÃ³n binaria
- Categorical Cross-Entropy para multiclase
- Gradient Descent y variantes (Batch, SGD, Mini-batch)
- Learning rate y su importancia
- Overfitting y cÃ³mo detectarlo

### Ejercicios Incluidos
1. Implementar MSE y MAE
2. Comparar sensibilidad a outliers
3. Visualizar Binary Cross-Entropy
4. Implementar Gradient Descent
5. Experimentos con learning rates
6. Detectar overfitting

### Archivos
```
Lab04_Funciones_Perdida/
â”œâ”€â”€ README.md (7.8 KB)
â”œâ”€â”€ teoria.md (9.3 KB)
â”œâ”€â”€ practica.ipynb (interactivo)
â””â”€â”€ codigo/
    â””â”€â”€ perdida.py (14.1 KB)
```

---

## Lab 05: Backpropagation

### Contenido
- **teoria.md**: Regla de la cadena, grafos computacionales, algoritmo completo
- **codigo/backprop.py**: ImplementaciÃ³n modular con clases
- **practica.ipynb**: Paso a paso del algoritmo
- **README.md**: GuÃ­a del laboratorio

### Conceptos Clave
- Regla de la cadena como fundamento
- Grafos computacionales
- Forward pass (guardar valores intermedios)
- Backward pass (calcular gradientes)
- VerificaciÃ³n con gradientes numÃ©ricos
- Eficiencia del algoritmo

### Ejercicios Incluidos
1. Visualizar grafos computacionales simples
2. Implementar backprop manualmente
3. Verificar con gradientes numÃ©ricos
4. Entrenar red en problema XOR
5. Implementar red de 3 capas

### Archivos
```
Lab05_Backpropagation/
â”œâ”€â”€ README.md (1.5 KB)
â”œâ”€â”€ teoria.md (8.5 KB)
â”œâ”€â”€ practica.ipynb (interactivo)
â””â”€â”€ codigo/
    â””â”€â”€ backprop.py (11.1 KB)
```

---

## Lab 06: Entrenamiento

### Contenido
- **teoria.md**: Loop de entrenamiento, Ã©pocas, batches, regularizaciÃ³n
- **codigo/entrenamiento.py**: ImplementaciÃ³n completa con validaciÃ³n
- **practica.ipynb**: Entrenamiento end-to-end
- **README.md**: GuÃ­a prÃ¡ctica

### Conceptos Clave
- Ã‰pocas vs Iteraciones vs Batches
- DivisiÃ³n de datos (Train/Val/Test)
- Learning rate scheduling
- Early stopping
- Dropout y regularizaciÃ³n
- InicializaciÃ³n de pesos (Xavier, He)
- Monitoreo de mÃ©tricas

### Ejercicios Incluidos
1. Entrenar red completa
2. Implementar early stopping
3. Comparar batch sizes
4. Experimentar con learning rates
5. Visualizar curvas de aprendizaje
6. Detectar overfitting

### Archivos
```
Lab06_Entrenamiento/
â”œâ”€â”€ README.md (1.8 KB)
â”œâ”€â”€ teoria.md (8.8 KB)
â”œâ”€â”€ practica.ipynb (interactivo)
â””â”€â”€ codigo/
    â””â”€â”€ entrenamiento.py (implementaciÃ³n completa)
```

---

## Lab 07: Frameworks de Deep Learning

### Contenido
- **teoria.md**: PyTorch vs TensorFlow, ventajas, comparaciones
- **codigo/pytorch_ejemplo.py**: Ejemplo completo en PyTorch
- **codigo/tensorflow_ejemplo.py**: Ejemplo completo en TensorFlow/Keras
- **practica.ipynb**: ComparaciÃ³n prÃ¡ctica
- **README.md**: GuÃ­a de frameworks

### Conceptos Clave
- Por quÃ© usar frameworks (Autograd, GPU, eficiencia)
- PyTorch: pythÃ³nico, dinÃ¡mico, investigaciÃ³n
- TensorFlow/Keras: producciÃ³n, escalabilidad
- DiferenciaciÃ³n automÃ¡tica
- Data loaders y pipelines
- Optimizadores avanzados
- Checkpoints y logging

### Ejercicios Incluidos
1. Mismo modelo en ambos frameworks
2. Comparar sintaxis
3. Usar autograd
4. Experimentar con optimizadores
5. Visualizar con TensorBoard

### Archivos
```
Lab07_Frameworks_DeepLearning/
â”œâ”€â”€ README.md (2.3 KB)
â”œâ”€â”€ teoria.md (10.0 KB)
â”œâ”€â”€ practica.ipynb (comparativo)
â””â”€â”€ codigo/
    â”œâ”€â”€ pytorch_ejemplo.py (completo)
    â””â”€â”€ tensorflow_ejemplo.py (completo)
```

---

## Lab 08: IA Generativa

### Contenido
- **teoria.md**: VAE, GAN, Diffusion Models, aplicaciones
- **codigo/generativo.py**: ImplementaciÃ³n de VAE y GAN simples
- **practica.ipynb**: Experimentos con modelos generativos
- **README.md**: GuÃ­a completa

### Conceptos Clave
- Modelos discriminativos vs generativos
- Autoencoders y VAE
- GANs (Generator vs Discriminator)
- Diffusion Models
- Espacio latente
- Reparameterization trick
- Entrenamiento adversarial
- Aplicaciones y Ã©tica

### Ejercicios Incluidos
1. Implementar VAE simple
2. Explorar espacio latente
3. Generar nuevas muestras
4. Entender arquitectura GAN
5. InterpolaciÃ³n en espacio latente

### Archivos
```
Lab08_IA_Generativa/
â”œâ”€â”€ README.md (3.5 KB)
â”œâ”€â”€ teoria.md (9.7 KB)
â”œâ”€â”€ practica.ipynb (generaciÃ³n)
â””â”€â”€ codigo/
    â””â”€â”€ generativo.py (10.1 KB)
```

---

## ğŸ“Š EstadÃ­sticas Totales

### Archivos Creados
- **6 Laboratorios** (Lab 03-08)
- **6 archivos teoria.md** (~53 KB total)
- **6 archivos README.md** (~23 KB total)
- **8 archivos .py** (~46 KB total)
- **6 archivos .ipynb** (notebooks interactivos)
- **Total: 26 archivos**

### LÃ­neas de CÃ³digo
- **TeorÃ­a**: ~1,500 lÃ­neas de teorÃ­a
- **CÃ³digo**: ~1,200 lÃ­neas de implementaciÃ³n
- **Total**: ~2,700 lÃ­neas

### Temas Cubiertos
1. âœ… Funciones de ActivaciÃ³n (ReLU, Sigmoid, Tanh, Softmax)
2. âœ… Funciones de PÃ©rdida (MSE, MAE, Cross-Entropy)
3. âœ… OptimizaciÃ³n (Gradient Descent, learning rate)
4. âœ… Backpropagation (algoritmo completo)
5. âœ… Entrenamiento (loop completo, validaciÃ³n)
6. âœ… Frameworks (PyTorch, TensorFlow)
7. âœ… IA Generativa (VAE, GAN)

---

## ğŸ¯ Objetivos de Aprendizaje Cumplidos

### Lab 03
- âœ… Comprender la importancia de la no linealidad
- âœ… Implementar funciones de activaciÃ³n desde cero
- âœ… Calcular derivadas para backpropagation
- âœ… Elegir activaciones apropiadas

### Lab 04
- âœ… Entender funciones de pÃ©rdida
- âœ… Implementar MSE, MAE, Cross-Entropy
- âœ… Comprender Gradient Descent
- âœ… Detectar overfitting

### Lab 05
- âœ… Dominar la regla de la cadena
- âœ… Implementar backpropagation completo
- âœ… Verificar gradientes numÃ©ricamente
- âœ… Entrenar redes desde cero

### Lab 06
- âœ… Implementar loop de entrenamiento completo
- âœ… Manejar Ã©pocas, batches, iteraciones
- âœ… Implementar early stopping
- âœ… Monitorear mÃ©tricas

### Lab 07
- âœ… Comprender ventajas de frameworks
- âœ… Usar PyTorch y TensorFlow
- âœ… Aprovechar diferenciaciÃ³n automÃ¡tica
- âœ… Acelerar con GPU

### Lab 08
- âœ… Entender modelos generativos
- âœ… Conocer arquitecturas VAE y GAN
- âœ… Explorar espacio latente
- âœ… Aplicaciones de IA generativa

---

## ğŸš€ CÃ³mo Usar Este Contenido

### Para Estudiantes

1. **Orden Recomendado**: Seguir Labs 03 â†’ 08 secuencialmente

2. **Por cada Lab**:
   ```bash
   # 1. Leer teorÃ­a
   cat LabXX/teoria.md
   
   # 2. Revisar README
   cat LabXX/README.md
   
   # 3. Ejecutar cÃ³digo
   python LabXX/codigo/*.py
   
   # 4. Practicar con notebook
   jupyter notebook LabXX/practica.ipynb
   ```

3. **VerificaciÃ³n**: Completar todos los ejercicios de cada lab

### Para Instructores

1. **Presentaciones**: Usar `teoria.md` como base
2. **Demostraciones**: Ejecutar archivos `.py` en vivo
3. **PrÃ¡ctica**: Asignar `practica.ipynb` como tarea
4. **EvaluaciÃ³n**: Usar ejercicios de cada README

### Para Autodidactas

1. **Estudiar teorÃ­a** primero
2. **Implementar** antes de ver el cÃ³digo
3. **Comparar** tu implementaciÃ³n con la provista
4. **Experimentar** con los notebooks
5. **Modificar** y extender el cÃ³digo

---

## ğŸ“‹ Prerrequisitos

### Conocimientos
- Python bÃ¡sico
- NumPy
- MatemÃ¡ticas: Ã¡lgebra lineal, cÃ¡lculo bÃ¡sico

### Software
```bash
# Instalar dependencias
pip install numpy matplotlib scikit-learn jupyter

# Para Lab 07 (opcional)
pip install torch tensorflow

# Para Lab 08 (opcional)
pip install torch torchvision
```

---

## ğŸ” CaracterÃ­sticas Destacadas

### PedagÃ³gicas
- âœ… **Progresivo**: De lo simple a lo complejo
- âœ… **Desde Cero**: Sin abstracciones ocultas
- âœ… **PrÃ¡ctico**: CÃ³digo ejecutable en cada lab
- âœ… **Visualizaciones**: GrÃ¡ficas en todos los labs
- âœ… **DidÃ¡ctico**: Explicaciones paso a paso

### TÃ©cnicas
- âœ… **CÃ³digo limpio**: Clases modulares, bien comentado
- âœ… **VerificaciÃ³n**: Gradient checking incluido
- âœ… **Ejemplos reales**: Problemas XOR, MNIST, etc.
- âœ… **Frameworks modernos**: PyTorch y TensorFlow
- âœ… **Estado del arte**: IA Generativa

---

## ğŸ’¡ Consejos de Estudio

1. **No saltar labs**: Cada uno construye sobre el anterior

2. **Implementar antes de ver**: Intenta implementar antes de mirar el cÃ³digo

3. **Debugging es aprendizaje**: Si algo no funciona, entiende por quÃ©

4. **Experimentar**: Cambia hiperparÃ¡metros, arquitecturas

5. **Visualizar**: Las grÃ¡ficas ayudan a entender

6. **Gradientes numÃ©ricos**: Siempre verifica tu backprop

7. **Comunidad**: Discute conceptos con otros

---

## ğŸ“ PrÃ³ximos Pasos DespuÃ©s del Curso

### Profundizar
1. **CNNs**: Redes Convolucionales para imÃ¡genes
2. **RNNs/LSTMs**: Redes Recurrentes para secuencias
3. **Transformers**: Arquitectura moderna (GPT, BERT)
4. **Reinforcement Learning**: Aprendizaje por refuerzo

### Practicar
1. **Kaggle**: Competencias de ML/DL
2. **Papers**: Implementar papers de investigaciÃ³n
3. **Proyectos**: Resolver problemas reales
4. **Contribuir**: Open source en frameworks

### Recursos
- Fast.ai (curso prÃ¡ctico)
- Stanford CS231n (Computer Vision)
- DeepLearning.ai (cursos de Andrew Ng)
- Papers with Code (implementaciones)

---

## ğŸ“ Notas Importantes

### FilosofÃ­a "Desde Cero"
Este curso implementa todo desde cero para **entender** los fundamentos. En la prÃ¡ctica:
- âœ… Usa frameworks (PyTorch, TensorFlow) para proyectos reales
- âœ… Pero conoce los fundamentos para debugging y arquitecturas custom
- âœ… "Desde cero" te da superpoderes

### CÃ³digo de ProducciÃ³n
El cÃ³digo aquÃ­ es **didÃ¡ctico**, no optimizado para producciÃ³n:
- Foco en claridad sobre eficiencia
- Algunos shortcuts tomados intencionalmente
- Para producciÃ³n: usar frameworks y mejores prÃ¡cticas

### Ã‰tica en IA
Lab 08 menciona consideraciones Ã©ticas. Recuerda:
- Los modelos pueden tener sesgos
- Deepfakes pueden ser mal usados
- Con gran poder viene gran responsabilidad

---

## ğŸ† Reconocimientos

Este contenido estÃ¡ diseÃ±ado para ser:
- **Accesible**: Para principiantes con Python bÃ¡sico
- **Completo**: Cubre fundamentos hasta generativa
- **PrÃ¡ctico**: Todo es ejecutable y verificable
- **Moderno**: Incluye frameworks y IA generativa
- **Gratuito**: Conocimiento abierto para todos

---

## ğŸ“ Soporte

### Si tienes problemas:

1. **Revisa los READMEs**: Cada lab tiene troubleshooting
2. **Gradient Checking**: Verifica tus implementaciones
3. **Visualiza**: Las grÃ¡ficas muestran si algo estÃ¡ mal
4. **Debugging**: Usa print statements, breakpoints
5. **Comunidad**: Busca ayuda en forums (Stack Overflow, Reddit)

---

## âœ… Checklist Final

DespuÃ©s de completar todos los labs, deberÃ­as poder:

- [ ] Explicar quÃ© es una red neuronal y cÃ³mo funciona
- [ ] Implementar forward pass manualmente
- [ ] Implementar backpropagation desde cero
- [ ] Entrenar una red en un problema real
- [ ] Elegir funciones de activaciÃ³n apropiadas
- [ ] Elegir funciones de pÃ©rdida para tu problema
- [ ] Detectar y prevenir overfitting
- [ ] Usar PyTorch o TensorFlow
- [ ] Entender modelos generativos (VAE, GAN)
- [ ] Leer papers de deep learning

---

## ğŸ‰ Â¡Felicitaciones!

Si completaste todos los labs, Â¡felicitaciones! ğŸ“

Ahora tienes una **base sÃ³lida** en Deep Learning. Has aprendido:
- âœ… CÃ³mo funcionan las redes neuronales internamente
- âœ… CÃ³mo implementar algoritmos desde cero
- âœ… CÃ³mo usar herramientas modernas
- âœ… Las fronteras de la IA (generativa)

**Â¡Sigue aprendiendo y construyendo cosas increÃ­bles! ğŸš€**

---

**Ãšltima actualizaciÃ³n**: Diciembre 2024  
**VersiÃ³n**: 1.0  
**Licencia**: Educativo - Uso libre
