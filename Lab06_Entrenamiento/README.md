# Lab 06: Entrenamiento de Redes Neuronales

## Objetivos
1. Implementar loop de entrenamiento completo
2. Dividir datos en train/val/test
3. Implementar early stopping
4. Monitorear m茅tricas durante entrenamiento
5. Detectar y prevenir overfitting

## Estructura
```
Lab06_Entrenamiento/
 README.md
 teoria.md
 practica.ipynb
 codigo/
     entrenamiento.py
```

## Conceptos Clave

### poca vs Iteraci贸n vs Batch
- **poca**: Pase completo por todos los datos
- **Batch**: Subconjunto de datos procesados juntos
- **Iteraci贸n**: Un paso de actualizaci贸n (procesar un batch)

### Divisi贸n de Datos
```
Train (70%): Entrenar el modelo
Validation (15%): Ajustar hiperpar谩metros
Test (15%): Evaluaci贸n final
```

### Early Stopping
Detener cuando validaci贸n deja de mejorar:
```python
if val_loss no mejora en 10 茅pocas:
    detener entrenamiento
```

## Pr谩ctica

### Ejecutar:
```bash
cd codigo/
python entrenamiento.py
```

### Notebook:
```bash
jupyter notebook practica.ipynb
```

## Hiperpar谩metros Recomendados

Para empezar:
```
Learning rate: 0.001 - 0.01
Batch size: 32
Epochs: 100
Optimizer: Adam (o SGD)
Hidden layers: 2
```

## Ejercicios

1. Entrenar en MNIST
2. Implementar learning rate decay
3. Comparar diferentes batch sizes
4. Experimentar con arquitecturas

## Debugging

**P茅rdida no baja**:
- Verificar learning rate
- Normalizar datos
- Verificar gradientes

**Overfitting**:
- A帽adir regularizaci贸n
- Early stopping
- M谩s datos

## Verificaci贸n
- [ ] Puedo entrenar una red completa
- [ ] Entiendo 茅pocas, batches e iteraciones
- [ ] S茅 detectar overfitting
- [ ] Puedo implementar early stopping

## Pr贸ximo Lab
**Lab 07**: M茅tricas de Evaluaci贸n y Matriz de Confusi贸n

---
**隆El entrenamiento es donde todo cobra vida! **
