# GuÃ­a de Laboratorio: Entrenamiento de Redes Neuronales

## ğŸ“‹ InformaciÃ³n del Laboratorio

**TÃ­tulo:** Fundamentos de Deep Learning - Entrenamiento de Redes Neuronales  
**CÃ³digo:** Lab 06  
**DuraciÃ³n:** 2-3 horas  
**Nivel:** Intermedio-Avanzado  

## ğŸ¯ Objetivos EspecÃ­ficos

Al completar este laboratorio, serÃ¡s capaz de:

1. Implementar un loop de entrenamiento completo end-to-end
2. Dividir datos correctamente en conjuntos train/validation/test
3. Comprender y aplicar conceptos de Ã©poca, batch e iteraciÃ³n
4. Implementar early stopping para prevenir overfitting
5. Monitorear mÃ©tricas de entrenamiento y validaciÃ³n en tiempo real
6. Detectar y diagnosticar overfitting y underfitting
7. Aplicar tÃ©cnicas de regularizaciÃ³n (L1, L2, Dropout)
8. Implementar learning rate scheduling y decay
9. Optimizar hiperparÃ¡metros mediante validaciÃ³n
10. Guardar y cargar modelos (checkpointing)

## ğŸ“š Prerrequisitos

### Conocimientos

- Python intermedio-avanzado (POO, manejo de datos)
- NumPy avanzado (operaciones matriciales, broadcasting)
- Backpropagation y cÃ¡lculo de gradientes (Lab 05)
- Funciones de pÃ©rdida y activaciÃ³n (Labs 03-04)
- Conceptos bÃ¡sicos de overfitting

### Software

- Python 3.8+
- NumPy 1.19+
- Matplotlib (visualizaciones)
- Scikit-learn (divisiÃ³n de datos, mÃ©tricas)
- Jupyter Notebook (recomendado)

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` - Marco teÃ³rico completo sobre entrenamiento
- `README.md` - Estructura del laboratorio y recursos
- Labs anteriores (especialmente Lab 05 sobre Backpropagation)

## ğŸ“– IntroducciÃ³n

### Del Gradiente a la Inteligencia

Has aprendido a calcular gradientes con backpropagation. Ahora viene la parte emocionante: **entrenar** una red neuronal para que realmente aprenda a resolver problemas.

El entrenamiento es el proceso iterativo mediante el cual:
1. La red hace predicciones
2. Medimos quÃ© tan incorrectas son (pÃ©rdida)
3. Calculamos cÃ³mo mejorar (gradientes)
4. Ajustamos los parÃ¡metros (optimizaciÃ³n)
5. Â¡Repetimos miles de veces!

**AnalogÃ­a del aprendizaje:**

Imagina aprender a tocar guitarra:
- **Ã‰poca**: Practicar la canciÃ³n completa una vez
- **Batch**: Practicar un fragmento especÃ­fico
- **IteraciÃ³n**: Un intento de tocar ese fragmento
- **Learning rate**: QuÃ© tan drÃ¡stico ajustas tu tÃ©cnica
- **Validation**: Tocar para un amigo que te da feedback
- **Early stopping**: Dejar de practicar cuando ya lo tocas bien

### El Loop de Entrenamiento

El corazÃ³n de todo entrenamiento es este loop simple pero poderoso:

```
PARA cada Ã©poca:
    PARA cada batch de datos:
        1. Forward pass: hacer predicciones
        2. Calcular pÃ©rdida
        3. Backward pass: calcular gradientes
        4. Actualizar parÃ¡metros
    
    Evaluar en validation set
    
    SI validation no mejora:
        Aplicar early stopping
```

### Conceptos Clave

**Ã‰poca (Epoch):**
Un pase completo a travÃ©s de todos los datos de entrenamiento.
```
1 Ã©poca = procesar 100% de los datos de entrenamiento
```

**Batch:**
Subconjunto de datos procesados simultÃ¡neamente.
```
Dataset de 1000 muestras, batch size 32
â†’ 32 batches por Ã©poca (1000 / 32 â‰ˆ 31.25)
```

**IteraciÃ³n:**
Un paso de actualizaciÃ³n de parÃ¡metros (procesar un batch).
```
Iteraciones por Ã©poca = total_muestras / batch_size
```

**Learning Rate:**
Controla el tamaÃ±o del paso de optimizaciÃ³n.
```
W_nuevo = W_viejo - learning_rate Ã— gradiente
```

### DivisiÃ³n de Datos

**Train (Entrenamiento)**: 70%
- Datos que el modelo ve durante entrenamiento
- Se usan para ajustar parÃ¡metros (W, b)

**Validation (ValidaciÃ³n)**: 15%
- Datos para evaluar durante entrenamiento
- Se usan para ajustar hiperparÃ¡metros
- Detectan overfitting

**Test (Prueba)**: 15%
- Datos que el modelo NUNCA ve durante entrenamiento
- EvaluaciÃ³n final del rendimiento real
- Simulan datos del mundo real

**Regla de oro:** Â¡NUNCA uses datos de test para tomar decisiones de entrenamiento!

### Problemas Comunes

**Underfitting (Subajuste):**
```
PÃ©rdida de entrenamiento: ALTA
PÃ©rdida de validaciÃ³n: ALTA
â†’ Modelo demasiado simple
```

**Overfitting (Sobreajuste):**
```
PÃ©rdida de entrenamiento: BAJA
PÃ©rdida de validaciÃ³n: ALTA
â†’ Modelo memorizÃ³ datos de entrenamiento
```

**Buen ajuste:**
```
PÃ©rdida de entrenamiento: BAJA
PÃ©rdida de validaciÃ³n: BAJA y cercana a train
â†’ Modelo generaliza bien
```

### Aplicaciones en el Mundo Real

El entrenamiento efectivo es crucial para:
- **Medicina**: Modelos que diagnostican enfermedades con precisiÃ³n
- **VehÃ­culos autÃ³nomos**: Redes que deben generalizar a cualquier carretera
- **Finanzas**: Prevenir overfitting en datos histÃ³ricos
- **PLN**: Modelos de lenguaje entrenados en billones de palabras
- **VisiÃ³n**: ImageNet (14M imÃ¡genes, semanas de entrenamiento)

## ğŸ¤” Preguntas de ReflexiÃ³n Iniciales

1. Â¿Por quÃ© necesitamos dividir datos en train/val/test?
2. Â¿QuÃ© pasarÃ­a si usamos todo el dataset para entrenar?
3. Â¿CÃ³mo sabemos cuÃ¡ndo detener el entrenamiento?
4. Â¿Por quÃ© procesar datos en batches en lugar de todos a la vez?
5. Â¿QuÃ© indica que un modelo estÃ¡ en overfitting?

## ğŸ”¬ Parte 1: Fundamentos del Entrenamiento (45 min)

### 1.1 Loop de Entrenamiento BÃ¡sico

Empecemos con la estructura mÃ¡s simple:

#### Fundamento TeÃ³rico: DivisiÃ³n de Datos y NormalizaciÃ³n

Antes de ejecutar cualquier entrenamiento, es imprescindible preparar los datos correctamente. La **divisiÃ³n en conjuntos train/validaciÃ³n/test** obedece a un principio estadÃ­stico fundamental: medir la capacidad de generalizaciÃ³n del modelo en datos que nunca ha visto. El conjunto de entrenamiento ajusta los parÃ¡metros internos (pesos y sesgos); el conjunto de validaciÃ³n nos guÃ­a para tomar decisiones de diseÃ±o (hiperparÃ¡metros, arquitectura, cuÃ¡ndo parar) sin contaminar la estimaciÃ³n final; y el conjunto de test proporciona una medida honesta e imparcial del rendimiento real del modelo sobre datos del mundo real. Usar datos de test durante el desarrollo equivale a "hacer trampa en el examen" y produce estimaciones de rendimiento optimistas que no se sostienen en producciÃ³n.

La distribuciÃ³n estÃ¡ndar **70% train / 15% val / 15% test** es un buen punto de partida para datasets de tamaÃ±o medio (miles de muestras). Para datasets muy grandes (millones de ejemplos) puede usarse una particiÃ³n 98/1/1 porque incluso el 1% de test representa decenas de miles de muestras suficientes para estimaciones estadÃ­sticamente robustas. En datasets muy pequeÃ±os (cientos de muestras), se recomienda la **validaciÃ³n cruzada K-fold** en lugar de una sola divisiÃ³n, porque maximiza el uso de los datos disponibles para entrenamiento y proporciona estimaciones mÃ¡s confiables del rendimiento.

```
DivisiÃ³n de datos:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dataset completo (N muestras)
         â”‚
         â”œâ”€â”€â–º Train set (70%)  â†’ Ajustar W, b por backpropagation
         â”‚
         â”œâ”€â”€â–º Validation set (15%) â†’ Monitorear, early stopping,
         â”‚                           selecciÃ³n de hiperparÃ¡metros
         â”‚
         â””â”€â”€â–º Test set (15%)   â†’ EvaluaciÃ³n FINAL (solo una vez)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

La **normalizaciÃ³n de caracterÃ­sticas** (restar la media y dividir por la desviaciÃ³n estÃ¡ndar) es igualmente crÃ­tica. Cuando las caracterÃ­sticas tienen escalas muy distintas â€”por ejemplo, una columna con valores en el rango [0, 1] y otra en [0, 10000]â€” los gradientes de los pesos asociados a la caracterÃ­stica grande dominan la actualizaciÃ³n, haciendo que el entrenamiento sea extremadamente lento o inestable. Con los datos normalizados, todas las caracterÃ­sticas contribuyen de forma equilibrada a la funciÃ³n de pÃ©rdida, la superficie de error se vuelve mÃ¡s esfÃ©rica y el descenso por gradiente converge con menos oscilaciones.

```
Sin normalizaciÃ³n:           Con normalizaciÃ³n (Z-score):
  PÃ©rdida                       PÃ©rdida
    â”‚  zig-zag                    â”‚  descenso suave
    â”‚ /\/\/\/\                    â”‚ â•²
    â”‚/        \___                â”‚  â•²___
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Ã©pocas           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Ã©pocas
```

**Importante:** la media y desviaciÃ³n estÃ¡ndar deben calcularse **sÃ³lo** sobre el conjunto de entrenamiento y luego aplicarse a validaciÃ³n y test; de lo contrario, estarÃ­amos filtrando informaciÃ³n futura al modelo (data leakage). La fÃ³rmula de normalizaciÃ³n es:

```text
X_normalizado = (X - Î¼_train) / (Ïƒ_train + Îµ)

donde:
  Î¼_train = media calculada en X_train
  Ïƒ_train = desviaciÃ³n estÃ¡ndar calculada en X_train
  Îµ = 1e-8  (evita divisiÃ³n por cero)
```

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification

# Generar datos sintÃ©ticos
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,
                          n_informative=15, n_redundant=5, random_state=42)

# Dividir datos
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(f"Train: {X_train.shape[0]} muestras")
print(f"Validation: {X_val.shape[0]} muestras")
print(f"Test: {X_test.shape[0]} muestras")

# Normalizar datos (importante!)
mean = X_train.mean(axis=0)
std = X_train.std(axis=0)

X_train = (X_train - mean) / (std + 1e-8)
X_val = (X_val - mean) / (std + 1e-8)
X_test = (X_test - mean) / (std + 1e-8)
```

**Simple Training Loop:**

#### Fundamento TeÃ³rico: La Clase SimpleTrainer

La clase `SimpleTrainer` encapsula el **loop de entrenamiento completo** siguiendo el ciclo de cuatro pasos que define el aprendizaje supervisado: *forward pass*, cÃ¡lculo de pÃ©rdida, *backward pass* y actualizaciÃ³n de parÃ¡metros. Comprender cada paso es fundamental antes de trabajar con frameworks de alto nivel como PyTorch o TensorFlow, que los abstraen automÃ¡ticamente.

```
Loop de entrenamiento (una Ã©poca):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  X_train â”€â”€â–º [Forward Pass] â”€â”€â–º Å· (predicciones)
                                   â”‚
                               [PÃ©rdida L]
                               L = -mean(yÂ·log(Å·) + (1-y)Â·log(1-Å·))
                                   â”‚
                           [Backward Pass]
                           âˆ‚L/âˆ‚Wâ‚‚, âˆ‚L/âˆ‚bâ‚‚, âˆ‚L/âˆ‚Wâ‚, âˆ‚L/âˆ‚bâ‚
                                   â”‚
                         [ActualizaciÃ³n GD]
                         W â† W - Î· Â· âˆ‚L/âˆ‚W
                         b â† b - Î· Â· âˆ‚L/âˆ‚b
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

En el **forward pass**, los datos de entrada se propagan capa por capa hasta producir una predicciÃ³n; la funciÃ³n de pÃ©rdida (en este caso *Binary Cross-Entropy*) cuantifica el error asignando un escalar positivo que crece cuanto mÃ¡s se equivoca el modelo. El *backward pass* aplica la regla de la cadena para propagar el gradiente de la pÃ©rdida hacia atrÃ¡s a travÃ©s de cada capa, obteniendo `âˆ‚L/âˆ‚W` y `âˆ‚L/âˆ‚b` para cada conjunto de parÃ¡metros. La **regla de actualizaciÃ³n** `W â† W âˆ’ Î·Â·âˆ‚L/âˆ‚W` mueve cada peso en la direcciÃ³n que reduce la pÃ©rdida, siendo `Î·` (learning rate) el hiperparÃ¡metro que controla el tamaÃ±o del paso.

El hecho de que `SimpleTrainer` ejecute el paso completo con todos los datos a la vez por Ã©poca se denomina **Batch Gradient Descent** puro. Es conceptualmente correcto pero ineficiente con datasets grandes â€”lo cual motiva la siguiente secciÃ³n sobre mini-batches. Para este dataset de 1000 muestras, el comportamiento esperado es:

- **Ã‰pocas 1-20:** Descenso rÃ¡pido de la pÃ©rdida (fase de aprendizaje principal)
- **Ã‰pocas 20-60:** Descenso mÃ¡s lento, convergencia gradual
- **Ã‰pocas 60+:** Plateau, pequeÃ±as oscilaciones alrededor del mÃ­nimo

Si la pÃ©rdida no desciende en las primeras 10 Ã©pocas, el learning rate probablemente es demasiado pequeÃ±o (< 0.001) o demasiado grande (> 1.0) y estÃ¡ causando divergencia.

```python
class SimpleTrainer:
    """Trainer bÃ¡sico para redes neuronales"""
    
    def __init__(self, model, learning_rate=0.01):
        self.model = model
        self.lr = learning_rate
        self.history = {
            'train_loss': [],
            'val_loss': []
        }
    
    def train_epoch(self, X, y):
        """Entrena una Ã©poca completa"""
        # Forward
        predictions = self.model.forward(X)
        
        # Loss
        loss = self.compute_loss(predictions, y)
        
        # Backward
        grad = predictions - y.reshape(-1, 1)
        self.model.backward(grad)
        
        # Update
        self.model.update(self.lr)
        
        return loss
    
    def compute_loss(self, predictions, targets):
        """Binary Cross-Entropy"""
        targets = targets.reshape(-1, 1)
        epsilon = 1e-8
        loss = -np.mean(
            targets * np.log(predictions + epsilon) +
            (1 - targets) * np.log(1 - predictions + epsilon)
        )
        return loss
    
    def evaluate(self, X, y):
        """EvalÃºa el modelo en un dataset"""
        predictions = self.model.forward(X)
        loss = self.compute_loss(predictions, y)
        
        # Accuracy
        pred_classes = (predictions > 0.5).astype(int)
        accuracy = np.mean(pred_classes.flatten() == y)
        
        return loss, accuracy
    
    def train(self, X_train, y_train, X_val, y_val, epochs=100):
        """Loop de entrenamiento completo"""
        print("Iniciando entrenamiento...")
        print("=" * 60)
        
        for epoch in range(epochs):
            # Entrenar
            train_loss = self.train_epoch(X_train, y_train)
            
            # Evaluar
            val_loss, val_acc = self.evaluate(X_val, y_val)
            
            # Guardar historia
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            
            # Mostrar progreso
            if epoch % 10 == 0:
                print(f"Epoch {epoch:3d} | "
                      f"Train Loss: {train_loss:.4f} | "
                      f"Val Loss: {val_loss:.4f} | "
                      f"Val Acc: {val_acc:.4f}")
        
        print("=" * 60)
        print("Entrenamiento completado!")

# Ejemplo de uso (asumiendo que tienes un modelo)
# trainer = SimpleTrainer(model, learning_rate=0.01)
# trainer.train(X_train, y_train, X_val, y_val, epochs=100)
```

### 1.2 Procesamiento en Batches

#### Fundamento TeÃ³rico: Tres Variantes de Descenso por Gradiente

El procesamiento por batches no es un mero truco de eficiencia: tiene profundas implicaciones teÃ³ricas sobre la calidad del entrenamiento. Existen tres variantes principales del descenso por gradiente que se diferencian en cuÃ¡ntos ejemplos se usan para calcular el gradiente en cada actualizaciÃ³n:

**1. Batch Gradient Descent (GD puro, batch_size = N):**
Usa el dataset completo en cada paso de actualizaciÃ³n. El gradiente calculado es exacto (sin ruido estadÃ­stico), produciendo actualizaciones suaves. Sin embargo, es computacionalmente prohibitivo en datasets grandes, no cabe en memoria GPU con millones de ejemplos, y puede quedar atrapado en mÃ­nimos locales al no tener ruido que le ayude a escapar.

```
GD puro:
IteraciÃ³n 1: gradiente con 1000 muestras â†’ W actualizado
IteraciÃ³n 2: gradiente con 1000 muestras â†’ W actualizado
...
1 Ã©poca = 1 actualizaciÃ³n de parÃ¡metros
```

**2. Stochastic Gradient Descent (SGD, batch_size = 1):**
Actualiza los parÃ¡metros tras procesar **un Ãºnico ejemplo**. El gradiente es muy ruidoso (alta varianza), lo que paradÃ³jicamente actÃºa como **regularizaciÃ³n implÃ­cita**: el ruido estocÃ¡stico permite al optimizador escapar de mÃ­nimos locales poco profundos. El inconveniente es que la convergencia es errÃ¡tica y no aprovecha el paralelismo hardware.

```
SGD (batch=1):
IteraciÃ³n 1: gradiente con muestra[0] â†’ W actualizado
IteraciÃ³n 2: gradiente con muestra[1] â†’ W actualizado
...
1 Ã©poca = 1000 actualizaciones de parÃ¡metros
```

**3. Mini-batch SGD (batch_size tÃ­pico: 16â€“256):**
Combina lo mejor de ambos mundos. Al calcular el gradiente sobre un subconjunto pequeÃ±o pero representativo, se reduce suficientemente el ruido para tener actualizaciones direccionalmente correctas, mientras se mantiene el beneficio regularizador del ruido estocÃ¡stico. Los mini-batches aprovechan al mÃ¡ximo las operaciones matriciales vectorizadas de las GPU/CPU modernas.

```
Mini-batch SGD (batch=32):
IteraciÃ³n 1: gradiente con muestras[0:32]   â†’ W actualizado
IteraciÃ³n 2: gradiente con muestras[32:64]  â†’ W actualizado
...
IteraciÃ³n 31: gradiente con muestras[992:1000] â†’ W actualizado
1 Ã©poca = 32 actualizaciones de parÃ¡metros
```

**ComparaciÃ³n de las tres variantes:**

| Propiedad | Batch GD | SGD (b=1) | Mini-batch SGD |
|-----------|----------|-----------|----------------|
| Varianza del gradiente | Nula (exacto) | Muy alta | Baja-moderada |
| Velocidad por Ã©poca | Lenta (1 update) | RÃ¡pida (N updates) | Balanceada |
| Uso de memoria GPU | Muy alto | MÃ­nimo | Configurable |
| RegularizaciÃ³n implÃ­cita | No | SÃ­ (mucho ruido) | SÃ­ (ruido moderado) |
| EstÃ¡ndar en industria | Raro | Raro | **SÃ­** |

**Â¿Por quÃ© batch_size=32 es tan comÃºn?** La elecciÃ³n de 32 tiene raÃ­ces empÃ­ricas y prÃ¡cticas: es suficientemente grande para aprovechar la paralelizaciÃ³n hardware (mÃºltiplo de potencias de 2), lo bastante pequeÃ±o para que el gradiente tenga varianza estocÃ¡stica beneficiosa, y produce actualizaciones frecuentes que aceleran la convergencia. Investigaciones como las de Keskar et al. (2017) muestran que los batch sizes muy grandes tienden a converger a **mÃ­nimos planos** (con mejor generalizaciÃ³n) mientras los muy pequeÃ±os pueden caer en **mÃ­nimos agudos** (menos robustos). Como regla prÃ¡ctica, empieza con 32 y ajusta segÃºn los recursos computacionales disponibles.

El **shuffle aleatorio** antes de cada Ã©poca es fundamental: asegura que cada mini-batch sea una muestra representativa del dataset completo, evitando que el modelo sobreajuste al orden de los datos.

El procesamiento por batches es esencial para eficiencia:

```python
class BatchTrainer:
    """Trainer con mini-batch SGD"""
    
    def __init__(self, model, learning_rate=0.01, batch_size=32):
        self.model = model
        self.lr = learning_rate
        self.batch_size = batch_size
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
    
    def create_batches(self, X, y):
        """Divide datos en batches"""
        n_samples = X.shape[0]
        indices = np.arange(n_samples)
        np.random.shuffle(indices)  # Importante: mezclar datos
        
        batches = []
        for start_idx in range(0, n_samples, self.batch_size):
            end_idx = min(start_idx + self.batch_size, n_samples)
            batch_indices = indices[start_idx:end_idx]
            batches.append((X[batch_indices], y[batch_indices]))
        
        return batches
    
    def train_epoch(self, X_train, y_train):
        """Entrena una Ã©poca con mini-batches"""
        batches = self.create_batches(X_train, y_train)
        epoch_loss = 0
        
        for batch_X, batch_y in batches:
            # Forward
            predictions = self.model.forward(batch_X)
            
            # Loss
            loss = self.compute_loss(predictions, batch_y)
            epoch_loss += loss
            
            # Backward
            grad = predictions - batch_y.reshape(-1, 1)
            self.model.backward(grad)
            
            # Update
            self.model.update(self.lr)
        
        # PÃ©rdida promedio de la Ã©poca
        return epoch_loss / len(batches)
    
    def compute_loss(self, predictions, targets):
        """Binary Cross-Entropy"""
        targets = targets.reshape(-1, 1)
        epsilon = 1e-8
        loss = -np.mean(
            targets * np.log(predictions + epsilon) +
            (1 - targets) * np.log(1 - predictions + epsilon)
        )
        return loss
    
    def evaluate(self, X, y):
        """EvalÃºa modelo"""
        predictions = self.model.forward(X)
        loss = self.compute_loss(predictions, y)
        
        pred_classes = (predictions > 0.5).astype(int)
        accuracy = np.mean(pred_classes.flatten() == y)
        
        return loss, accuracy
    
    def train(self, X_train, y_train, X_val, y_val, epochs=100, verbose=True):
        """Loop de entrenamiento con batches"""
        
        for epoch in range(epochs):
            # Entrenar con batches
            train_loss = self.train_epoch(X_train, y_train)
            train_loss_full, train_acc = self.evaluate(X_train, y_train)
            
            # Validar
            val_loss, val_acc = self.evaluate(X_val, y_val)
            
            # Guardar historia
            self.history['train_loss'].append(train_loss_full)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)
            
            # Mostrar progreso
            if verbose and epoch % 10 == 0:
                print(f"Epoch {epoch:3d}/{epochs} | "
                      f"Train: Loss={train_loss:.4f} Acc={train_acc:.4f} | "
                      f"Val: Loss={val_loss:.4f} Acc={val_acc:.4f}")
        
        return self.history
```

**Actividad 1.1:** Implementa el trainer y prueba con diferentes batch sizes (1, 16, 32, 128). Â¿QuÃ© observas?

> **Â¿QuÃ© debes observar y documentar?** Al variar el batch size notarÃ¡s diferencias claras en la *suavidad* de las curvas de pÃ©rdida: con batch_size=1 la pÃ©rdida oscilarÃ¡ fuertemente Ã©poca a Ã©poca; con batch_size grande las curvas serÃ¡n mÃ¡s suaves pero la convergencia inicial puede ser mÃ¡s lenta. Documenta el tiempo de entrenamiento por Ã©poca para cada configuraciÃ³n y observa si los modelos con batch size pequeÃ±o alcanzan menor pÃ©rdida final (efecto regularizador del ruido). Reflexiona sobre el compromiso velocidad-estabilidad-calidad del modelo final.

### 1.3 VisualizaciÃ³n del Entrenamiento

#### Fundamento TeÃ³rico: InterpretaciÃ³n de Curvas de Aprendizaje

Las **curvas de aprendizaje** son la herramienta de diagnÃ³stico mÃ¡s poderosa durante el entrenamiento de redes neuronales. Representan cÃ³mo evoluciona la pÃ©rdida (y la exactitud) en los conjuntos de entrenamiento y validaciÃ³n a lo largo de las Ã©pocas, y su forma nos da informaciÃ³n directa sobre el estado de salud del modelo.

**Patrones de diagnÃ³stico en las curvas de pÃ©rdida:**

```
BUEN AJUSTE:              OVERFITTING:              UNDERFITTING:
 PÃ©rdida                   PÃ©rdida                   PÃ©rdida
   â”‚ trainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®       â”‚ trainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®         â”‚ trainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚ valâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®  â”‚       â”‚              â•°â•¯ val       â”‚ valâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚              â•°â”€â”€â•¯       â”‚ valâ†— (diverge)           â”‚ (ambas altas)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã©pocas       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã©pocas        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã©pocas

  Gap pequeÃ±o y estable    Gap creciente con Ã©pocas    Ambas curvas altas
```

**Overfitting (Sobreajuste):** Se diagnostica cuando la pÃ©rdida de entrenamiento continÃºa bajando mientras la pÃ©rdida de validaciÃ³n deja de mejorar o comienza a subir. El **gap** `val_loss âˆ’ train_loss` es el indicador cuantitativo clave: un gap creciente con cada Ã©poca es la firma digital del overfitting. Visualmente, las dos curvas se separan en forma de tijera. El modelo ha aprendido los patrones especÃ­ficos del conjunto de entrenamiento (incluido el ruido) en lugar de las relaciones generalizables.

| Gap | DiagnÃ³stico | AcciÃ³n recomendada |
|-----|-------------|-------------------|
| < 0.05 | Buen ajuste | Continuar o aumentar capacidad |
| 0.05 â€“ 0.15 | Ligero overfitting | Monitorear, considerar regularizaciÃ³n |
| > 0.15 | Overfitting severo | Aplicar L2/Dropout, early stopping |
| Negativo | Underfitting | Aumentar capacidad o Ã©pocas |

**Underfitting (Subajuste):** Tanto la pÃ©rdida de entrenamiento como la de validaciÃ³n permanecen altas. Las curvas estÃ¡n cerca entre sÃ­ (gap pequeÃ±o) pero en un nivel de pÃ©rdida elevado. Esto indica que el modelo carece de capacidad suficiente para capturar la complejidad del problema.

**Buen ajuste:** Ambas curvas descienden juntas y se estabilizan en un nivel bajo, con un gap pequeÃ±o y estable. La curva de validaciÃ³n puede ser ligeramente superior a la de entrenamiento (es normal) pero no deberÃ­a separarse de ella significativamente.

**Â¿Por quÃ© monitorear tanto pÃ©rdida como exactitud?** La pÃ©rdida guÃ­a directamente la optimizaciÃ³n y detecta problemas sutiles que la exactitud puede ocultar: un modelo puede tener exactitud alta pero pÃ©rdida creciente si estÃ¡ sobreconfiado en sus predicciones incorrectas. La exactitud es mÃ¡s intuitiva para comunicar el rendimiento a no especialistas. Usar ambas mÃ©tricas juntas proporciona una imagen completa del comportamiento del modelo. Si ambas mÃ©tricas cuentan historias diferentes (alta exactitud pero pÃ©rdida creciente), la pÃ©rdida es el indicador mÃ¡s confiable del estado real del modelo.

```python
import matplotlib.pyplot as plt

def plot_training_history(history):
    """Visualiza curvas de aprendizaje"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # PÃ©rdida
    ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)
    ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)
    ax1.set_xlabel('Ã‰poca')
    ax1.set_ylabel('PÃ©rdida')
    ax1.set_title('Curva de PÃ©rdida')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Accuracy
    ax2.plot(history['train_acc'], label='Train Accuracy', linewidth=2)
    ax2.plot(history['val_acc'], label='Val Accuracy', linewidth=2)
    ax2.set_xlabel('Ã‰poca')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Curva de Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # DiagnÃ³stico
    final_train_loss = history['train_loss'][-1]
    final_val_loss = history['val_loss'][-1]
    gap = final_val_loss - final_train_loss
    
    print("\n=== DIAGNÃ“STICO ===")
    print(f"PÃ©rdida final - Train: {final_train_loss:.4f}, Val: {final_val_loss:.4f}")
    print(f"Gap (Val - Train): {gap:.4f}")
    
    if gap > 0.1:
        print("âš ï¸  OVERFITTING detectado!")
        print("Soluciones: RegularizaciÃ³n, Dropout, MÃ¡s datos, Early stopping")
    elif final_train_loss > 0.5:
        print("âš ï¸  UNDERFITTING detectado!")
        print("Soluciones: Modelo mÃ¡s complejo, MÃ¡s Ã©pocas, Ajustar learning rate")
    else:
        print("âœ“ Modelo bien ajustado")

# Usar
# plot_training_history(trainer.history)
```

## ğŸ”¬ Parte 2: Early Stopping (30 min)

### 2.1 ImplementaciÃ³n de Early Stopping

Early stopping previene overfitting deteniendo el entrenamiento cuando validation deja de mejorar:

#### Fundamento TeÃ³rico: Early Stopping, Patience y Checkpointing

El **early stopping** es quizÃ¡s la tÃ©cnica de regularizaciÃ³n mÃ¡s elegante porque no modifica la arquitectura del modelo ni la funciÃ³n de pÃ©rdida: simplemente detiene el entrenamiento en el momento Ã³ptimo antes de que el modelo comience a memorizar el ruido de los datos de entrenamiento. Desde una perspectiva teÃ³rica, el entrenamiento sigue una trayectoria en el espacio de parÃ¡metros: en las primeras Ã©pocas el modelo aprende patrones genuinos (mejora en validaciÃ³n), pero a partir de cierto punto comienza a sobreajustar los ejemplos de entrenamiento individuales (validaciÃ³n empeora). El early stopping identifica ese punto de inflexiÃ³n y "congela" el modelo en su mejor estado.

```
Comportamiento tÃ­pico del entrenamiento con early stopping:

  Val Loss
    â”‚
    â”‚\
    â”‚ \
    â”‚  \____
    â”‚       \___
    â”‚           \___
    â”‚               â•²___â•±â•²          â† punto de inflexiÃ³n
    â”‚                    â•²___â•±â•²___  â† overfitting inicia aquÃ­
    â”‚                â†‘
    â”‚         MEJOR CHECKPOINT
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã©pocas
    
    [â†â”€â”€â”€ patience â”€â”€â†’]
         Sin mejora     â†’ STOP y restaurar checkpoint
```

El parÃ¡metro **patience** define cuÃ¡ntas Ã©pocas consecutivas sin mejora en la validaciÃ³n se toleran antes de detener el entrenamiento. Un patience bajo (ej: 5) detiene el entrenamiento agresivamente y puede interrumpirlo en una meseta temporal antes de que el modelo retome su mejora; un patience alto (ej: 30) es mÃ¡s tolerante con la fluctuaciones pero puede resultar en mÃ¡s Ã©pocas de cÃ³mputo innecesarias. La elecciÃ³n depende de la suavidad esperada de las curvas: datasets ruidosos requieren patience mayor.

| Patience | Ventaja | Desventaja | CuÃ¡ndo usarlo |
|----------|---------|------------|---------------|
| 5-7 | Ahorra tiempo de cÃ³mputo | Puede detenerse en mesetas | Curvas muy suaves |
| 10-15 | Balance equilibrado | EstÃ¡ndar recomendado | **Caso general** |
| 20-30 | Explora mÃ¡s Ã©pocas | Mayor cÃ³mputo | Curvas con mesetas largas |

El concepto de **min_delta** (mejora mÃ­nima para considerarse progreso) complementa al patience: en lugar de considerar "mejora" cualquier reducciÃ³n por mÃ­nima que sea de la pÃ©rdida de validaciÃ³n, se exige que la reducciÃ³n supere un umbral `Î´`. Esto evita que pequeÃ±as fluctuaciones numÃ©ricas retrasen el early stopping indefinidamente. Por ejemplo, si `min_delta=0.001`, una reducciÃ³n de pÃ©rdida de 0.0001 no se contabiliza como mejora genuina.

```
LÃ³gica de early stopping con min_delta:

  nueva_val_loss < mejor_val_loss - min_delta?
        â”‚
        â”œâ”€â”€ SÃ â†’ Mejora genuina detectada
        â”‚         â€¢ Actualizar mejor_val_loss
        â”‚         â€¢ patience_counter = 0
        â”‚         â€¢ Guardar checkpoint
        â”‚
        â””â”€â”€ NO â†’ Sin mejora suficiente
                  â€¢ patience_counter += 1
                  â€¢ Si patience_counter >= patience: STOP y restaurar
```

El **checkpointing** (guardado del mejor modelo) es inseparable del early stopping: como el entrenamiento se detiene sÃ³lo despuÃ©s de `patience` Ã©pocas sin mejora, el Ãºltimo estado del modelo NO es el mejor. El checkpoint restaura los pesos correspondientes a la Ã©poca con menor pÃ©rdida de validaciÃ³n, garantizando que se usa el modelo en su punto Ã³ptimo de generalizaciÃ³n y no el modelo "degradado" por las Ãºltimas Ã©pocas de sobreajuste. En sistemas de producciÃ³n, los checkpoints tambiÃ©n protegen contra interrupciones inesperadas del entrenamiento (fallas de hardware, cortes de luz).

```python
class TrainerWithEarlyStopping:
    """Trainer con early stopping"""
    
    def __init__(self, model, learning_rate=0.01, batch_size=32):
        self.model = model
        self.lr = learning_rate
        self.batch_size = batch_size
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
        
        # Para early stopping
        self.best_val_loss = float('inf')
        self.best_weights = None
        self.patience_counter = 0
    
    def create_batches(self, X, y):
        """Divide datos en batches"""
        n_samples = X.shape[0]
        indices = np.arange(n_samples)
        np.random.shuffle(indices)
        
        batches = []
        for start_idx in range(0, n_samples, self.batch_size):
            end_idx = min(start_idx + self.batch_size, n_samples)
            batch_indices = indices[start_idx:end_idx]
            batches.append((X[batch_indices], y[batch_indices]))
        
        return batches
    
    def train_epoch(self, X_train, y_train):
        """Entrena una Ã©poca"""
        batches = self.create_batches(X_train, y_train)
        epoch_loss = 0
        
        for batch_X, batch_y in batches:
            predictions = self.model.forward(batch_X)
            loss = self.compute_loss(predictions, batch_y)
            epoch_loss += loss
            
            grad = predictions - batch_y.reshape(-1, 1)
            self.model.backward(grad)
            self.model.update(self.lr)
        
        return epoch_loss / len(batches)
    
    def compute_loss(self, predictions, targets):
        """Binary Cross-Entropy"""
        targets = targets.reshape(-1, 1)
        epsilon = 1e-8
        return -np.mean(
            targets * np.log(predictions + epsilon) +
            (1 - targets) * np.log(1 - predictions + epsilon)
        )
    
    def evaluate(self, X, y):
        """EvalÃºa modelo"""
        predictions = self.model.forward(X)
        loss = self.compute_loss(predictions, y)
        pred_classes = (predictions > 0.5).astype(int)
        accuracy = np.mean(pred_classes.flatten() == y)
        return loss, accuracy
    
    def save_checkpoint(self):
        """Guarda mejor modelo"""
        # En una implementaciÃ³n real, guardarÃ­as W y b de cada capa
        self.best_weights = {
            'layer1_W': self.model.layer1.W.copy(),
            'layer1_b': self.model.layer1.b.copy(),
            'layer2_W': self.model.layer2.W.copy(),
            'layer2_b': self.model.layer2.b.copy(),
        }
    
    def load_checkpoint(self):
        """Restaura mejor modelo"""
        if self.best_weights is not None:
            self.model.layer1.W = self.best_weights['layer1_W'].copy()
            self.model.layer1.b = self.best_weights['layer1_b'].copy()
            self.model.layer2.W = self.best_weights['layer2_W'].copy()
            self.model.layer2.b = self.best_weights['layer2_b'].copy()
    
    def train(self, X_train, y_train, X_val, y_val, 
              epochs=100, patience=10, verbose=True):
        """
        Entrenar con early stopping
        
        patience: nÃºmero de Ã©pocas sin mejora antes de detener
        """
        print(f"Entrenando con early stopping (patience={patience})...")
        print("=" * 70)
        
        for epoch in range(epochs):
            # Entrenar
            train_loss = self.train_epoch(X_train, y_train)
            train_loss_full, train_acc = self.evaluate(X_train, y_train)
            val_loss, val_acc = self.evaluate(X_val, y_val)
            
            # Guardar historia
            self.history['train_loss'].append(train_loss_full)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)
            
            # Early stopping logic
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self.save_checkpoint()
                improvement = "âœ“ Mejora!"
            else:
                self.patience_counter += 1
                improvement = f"No mejora ({self.patience_counter}/{patience})"
            
            # Mostrar progreso
            if verbose and epoch % 5 == 0:
                print(f"Epoch {epoch:3d}/{epochs} | "
                      f"Train: L={train_loss_full:.4f} A={train_acc:.4f} | "
                      f"Val: L={val_loss:.4f} A={val_acc:.4f} | {improvement}")
            
            # Detener si no hay mejora
            if self.patience_counter >= patience:
                print(f"\nâš ï¸  Early stopping en Ã©poca {epoch}")
                print(f"No mejora en {patience} Ã©pocas consecutivas")
                print(f"Mejor val_loss: {self.best_val_loss:.4f}")
                
                # Restaurar mejor modelo
                self.load_checkpoint()
                print("Modelo restaurado al mejor checkpoint")
                break
        
        else:
            print("\nEntrenamiento completado sin early stopping")
        
        print("=" * 70)
        return self.history

# Ejemplo de uso
# trainer = TrainerWithEarlyStopping(model, learning_rate=0.01, batch_size=32)
# history = trainer.train(X_train, y_train, X_val, y_val, 
#                         epochs=200, patience=15)
```

**Actividad 2.1:** Experimenta con diferentes valores de patience (5, 10, 20). Â¿CÃ³mo afecta al entrenamiento?

> **Â¿QuÃ© debes observar y documentar?** Registra en quÃ© Ã©poca se detiene el entrenamiento para cada valor de patience y cuÃ¡l es la pÃ©rdida de validaciÃ³n del checkpoint restaurado. Con patience=5 probablemente el entrenamiento se detenga prematuramente durante una meseta temporal; con patience=20 puede completar mÃ¡s Ã©pocas pero tambiÃ©n gastar mÃ¡s tiempo de cÃ³mputo. Compara las pÃ©rdidas finales en el conjunto de **test** (no validaciÃ³n) de los tres modelos para evaluar cuÃ¡l generaliza mejor. Esto ilustra el tradeoff entre detenciÃ³n temprana y exploraciÃ³n suficiente del espacio de soluciones.

## ğŸ”¬ Parte 3: RegularizaciÃ³n y TÃ©cnicas Avanzadas (50 min)

### 3.1 RegularizaciÃ³n L2 (Weight Decay)

#### Fundamento TeÃ³rico: RegularizaciÃ³n como PenalizaciÃ³n de Complejidad

La **regularizaciÃ³n** es el conjunto de tÃ©cnicas que previene el overfitting imponiendo restricciones sobre la complejidad del modelo. MatemÃ¡ticamente, modifica la funciÃ³n de pÃ©rdida aÃ±adiendo un **tÃ©rmino de penalizaciÃ³n** que crece cuando los pesos del modelo toman valores muy grandes:

```
L_total = L_datos + Î» Â· Î©(W)

donde:
  L_datos = pÃ©rdida original (ej: cross-entropy, MSE)
  Î©(W)    = penalizaciÃ³n sobre los pesos del modelo
  Î»       = hiperparÃ¡metro que balancea ambos tÃ©rminos
```

La **regularizaciÃ³n L2** (*Ridge* o *weight decay*) usa `Î©(W) = Â½ Â· Î£(Wáµ¢Â²)`, la suma de los cuadrados de todos los pesos. Su gradiente `âˆ‚Î©/âˆ‚W = W` modifica la regla de actualizaciÃ³n a:

```
W â† W - Î· Â· âˆ‚L_datos/âˆ‚W - Î· Â· Î» Â· W
W â† W Â· (1 - Î·Â·Î») - Î· Â· âˆ‚L_datos/âˆ‚W
         â†‘
     "weight decay": factor < 1 que reduce W en cada paso
```

Este factor `(1 âˆ’ Î·Â·Î») < 1` es exactamente el "decaimiento" del peso en cada paso, de ahÃ­ el nombre *weight decay*. El efecto es que los pesos tienen una presiÃ³n constante hacia cero, produciendo soluciones mÃ¡s **suaves y distribuidas** donde ningÃºn peso individual domina las predicciones.

La **regularizaciÃ³n L1** (*Lasso*) usa `Î©(W) = Î£|Wáµ¢|`. Su gradiente es `Î»Â·sign(W)`, que empuja los pesos exactamente a cero para los menos relevantes. Esto produce soluciones **dispersas (sparse)**: muchos pesos quedan en exactamente cero, equivalente a selecciÃ³n automÃ¡tica de caracterÃ­sticas.

**ComparaciÃ³n L1 vs L2:**

| Propiedad | L1 (Lasso) | L2 (Ridge / Weight Decay) |
|-----------|-----------|--------------------------|
| FÃ³rmula | Î»Â·Î£&#124;W&#124; | Î»/2Â·Î£(WÂ²) |
| Tipo de soluciÃ³n | Dispersa (muchos ceros) | Densa (pesos pequeÃ±os) |
| SelecciÃ³n de features | **SÃ­** (implÃ­cita) | No |
| Diferenciable en W=0 | No (problema numÃ©rico) | SÃ­ |
| Uso tÃ­pico | Feature selection | **RegularizaciÃ³n general** |

**CÃ³mo elegir lambda:** Un `Î»` muy pequeÃ±o no penaliza suficientemente y el overfitting persiste; un `Î»` muy grande fuerza todos los pesos a cero y el modelo pierde capacidad expresiva (underfitting). La prÃ¡ctica estÃ¡ndar es bÃºsqueda en escala logarÃ­tmica:

```text
Valores tÃ­picos a evaluar: Î» âˆˆ {0.1, 0.01, 0.001, 0.0001}

Î» = 0.1    â†’ RegularizaciÃ³n fuerte, riesgo de underfitting
Î» = 0.01   â†’ RegularizaciÃ³n moderada (buen punto de inicio)
Î» = 0.001  â†’ RegularizaciÃ³n suave
Î» = 0.0001 â†’ RegularizaciÃ³n muy suave
```

El valor Ã³ptimo se selecciona usando validaciÃ³n cruzada: el que maximiza el rendimiento en validaciÃ³n sin degradar el de entrenamiento de forma significativa.

```python
class TrainerWithL2:
    """Trainer con regularizaciÃ³n L2"""
    
    def __init__(self, model, learning_rate=0.01, batch_size=32, l2_lambda=0.01):
        self.model = model
        self.lr = learning_rate
        self.batch_size = batch_size
        self.l2_lambda = l2_lambda  # ParÃ¡metro de regularizaciÃ³n
        self.history = {'train_loss': [], 'val_loss': []}
    
    def compute_loss_with_l2(self, predictions, targets):
        """PÃ©rdida con regularizaciÃ³n L2"""
        # PÃ©rdida base (cross-entropy)
        targets = targets.reshape(-1, 1)
        epsilon = 1e-8
        data_loss = -np.mean(
            targets * np.log(predictions + epsilon) +
            (1 - targets) * np.log(1 - predictions + epsilon)
        )
        
        # TÃ©rmino de regularizaciÃ³n L2: Î» * Î£(WÂ²)
        l2_loss = 0
        l2_loss += np.sum(self.model.layer1.W ** 2)
        l2_loss += np.sum(self.model.layer2.W ** 2)
        l2_loss *= self.l2_lambda / 2
        
        total_loss = data_loss + l2_loss
        
        return total_loss, data_loss, l2_loss
    
    def train_epoch(self, X_train, y_train):
        """Entrena una Ã©poca con L2"""
        batches = self.create_batches(X_train, y_train)
        epoch_loss = 0
        
        for batch_X, batch_y in batches:
            # Forward
            predictions = self.model.forward(batch_X)
            
            # Loss con L2
            total_loss, data_loss, l2_loss = self.compute_loss_with_l2(
                predictions, batch_y
            )
            epoch_loss += total_loss
            
            # Backward (gradiente de data loss)
            grad = predictions - batch_y.reshape(-1, 1)
            self.model.backward(grad)
            
            # Agregar gradiente L2 a los pesos
            # âˆ‚(Î»||W||Â²)/âˆ‚W = Î»*W
            self.model.layer1.dW += self.l2_lambda * self.model.layer1.W
            self.model.layer2.dW += self.l2_lambda * self.model.layer2.W
            
            # Update
            self.model.update(self.lr)
        
        return epoch_loss / len(batches)
    
    def create_batches(self, X, y):
        """Crea batches"""
        n_samples = X.shape[0]
        indices = np.arange(n_samples)
        np.random.shuffle(indices)
        
        batches = []
        for start_idx in range(0, n_samples, self.batch_size):
            end_idx = min(start_idx + self.batch_size, n_samples)
            batch_indices = indices[start_idx:end_idx]
            batches.append((X[batch_indices], y[batch_indices]))
        
        return batches

# Comparar con y sin regularizaciÃ³n
# trainer_sin_reg = TrainerWithL2(model1, l2_lambda=0.0)
# trainer_con_reg = TrainerWithL2(model2, l2_lambda=0.01)
```

### 3.2 Learning Rate Scheduling

#### Fundamento TeÃ³rico: AdaptaciÃ³n DinÃ¡mica del Paso de Aprendizaje

Un **learning rate fijo** es subÃ³ptimo durante todo el proceso de entrenamiento por razones geomÃ©tricas claras: en las primeras Ã©pocas, el modelo estÃ¡ lejos del Ã³ptimo y un learning rate grande acelera la convergencia; pero en las Ã©pocas finales, cuando el modelo se acerca al Ã³ptimo, ese mismo learning rate grande hace que los parÃ¡metros "salten" alrededor del mÃ­nimo sin poder asentarse en Ã©l. Es el equivalente a intentar enroscar un tornillo con el destornillador a mÃ¡xima potencia: rÃ¡pido al principio pero impreciso al final. El **learning rate scheduling** resuelve esto reduciendo gradualmente la tasa de aprendizaje a medida que avanza el entrenamiento.

```
Problema del LR fijo:               SoluciÃ³n con LR scheduling:

  PÃ©rdida                             PÃ©rdida
    â”‚  \                                â”‚  \
    â”‚   \                               â”‚   \
    â”‚    \     LR grande                â”‚    \___
    â”‚     â•²â•±â•²â•±â•²â•±â•²â•±â”€â”€ oscilaciÃ³n         â”‚        â•²___ LR reducido
    â”‚                                   â”‚             â•²___
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã©pocas                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã©pocas
```

Las tres estrategias de scheduling mÃ¡s usadas tienen comportamientos distintos:

**Step Decay:**
```
lr(t) = lrâ‚€ Ã— factor^(Ã©poca // Ã©pocas_por_paso)

Ejemplo: lrâ‚€=0.1, factor=0.5, Ã©pocas_por_paso=10
  Ã‰poca 0-9:   lr = 0.1
  Ã‰poca 10-19: lr = 0.05
  Ã‰poca 20-29: lr = 0.025
```
Produce una curva de pÃ©rdida en escalones descendentes. Ideal cuando se sabe cuÃ¡ntas Ã©pocas necesita el modelo.

**Exponential Decay:** `lr(t) = lrâ‚€ Â· ráµ—` donde `r < 1` (ej: r=0.95). La reducciÃ³n es continua y suave. El LR decrece siempre, incluso si el modelo sigue mejorando, lo que puede ser una limitaciÃ³n.

**Reduce on Plateau:** SÃ³lo reduce el LR cuando la pÃ©rdida de validaciÃ³n deja de mejorar durante `patience` Ã©pocas. Es el mÃ¡s adaptativo y es el **estÃ¡ndar recomendado** para la mayorÃ­a de problemas.

| Estrategia | Tipo | Ventaja principal | LimitaciÃ³n |
|------------|------|-------------------|------------|
| Step Decay | Manual | Predecible, fÃ¡cil de depurar | Requiere configurar cuÃ¡ndo bajar |
| Exponential | AutomÃ¡tico | TransiciÃ³n suave continua | LR baja siempre, incluso si mejora |
| **Plateau** | Adaptativo | Se adapta al problema | **Recomendado en prÃ¡ctica** |

El **learning rate warmup** (calentamiento) es una tÃ©cnica complementaria usada en modelos grandes (Transformers, BERT): el LR empieza muy pequeÃ±o, aumenta linealmente durante las primeras Ã©pocas hasta el valor objetivo, y luego disminuye. El warmup estabiliza el entrenamiento en las primeras iteraciones cuando los pesos estÃ¡n aÃºn muy alejados del Ã³ptimo y los gradientes son grandes e inestables.

```
LR con warmup + cosine annealing (estÃ¡ndar en Transformers):

  LR
  â”‚        â•±â•²
  â”‚       â•±  â•²___
  â”‚      â•±       â•²___
  â”‚    â•±              â•²___
  â”‚  â•± (warmup)            â•²___ (decay)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã©pocas
```

Los **Cyclical Learning Rates** (CLR, Smith 2017) proponen una idea contraintuitiva: en lugar de sÃ³lo decrecer, el LR oscila entre un mÃ­nimo y un mÃ¡ximo en ciclos. La intuiciÃ³n es que los aumentos periÃ³dicos del LR ayudan al modelo a "saltar" de mÃ­nimos locales hacia mejores regiones del espacio de pÃ©rdida, logrando mejores soluciones finales que el scheduling monotÃ³nicamente decreciente.

```python
class TrainerWithLRSchedule:
    """Trainer con learning rate scheduling"""
    
    def __init__(self, model, initial_lr=0.1, batch_size=32):
        self.model = model
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.batch_size = batch_size
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': []
        }
    
    def step_decay(self, epoch, drop_factor=0.5, epochs_drop=10):
        """
        Step decay: reduce LR cada X Ã©pocas
        lr = initial_lr * drop_factor^(epoch // epochs_drop)
        """
        new_lr = self.initial_lr * (drop_factor ** (epoch // epochs_drop))
        return new_lr
    
    def exponential_decay(self, epoch, decay_rate=0.95):
        """
        Exponential decay: reducciÃ³n exponencial
        lr = initial_lr * decay_rate^epoch
        """
        new_lr = self.initial_lr * (decay_rate ** epoch)
        return new_lr
    
    def reduce_on_plateau(self, epoch, val_losses, patience=5, factor=0.5):
        """
        Reduce LR si validaciÃ³n no mejora
        """
        if len(val_losses) < patience + 1:
            return self.current_lr
        
        # Verificar si hubo mejora en las Ãºltimas 'patience' Ã©pocas
        recent_best = min(val_losses[-(patience+1):-1])
        current = val_losses[-1]
        
        if current >= recent_best:
            new_lr = self.current_lr * factor
            print(f"   â†’ Reducing LR: {self.current_lr:.6f} â†’ {new_lr:.6f}")
            return new_lr
        
        return self.current_lr
    
    def train(self, X_train, y_train, X_val, y_val, epochs=100, 
              schedule_type='step', verbose=True):
        """
        Entrenar con LR scheduling
        
        schedule_type: 'step', 'exponential', 'plateau'
        """
        print(f"Entrenando con {schedule_type} LR scheduling...")
        print("=" * 70)
        
        for epoch in range(epochs):
            # Actualizar learning rate
            if schedule_type == 'step':
                self.current_lr = self.step_decay(epoch)
            elif schedule_type == 'exponential':
                self.current_lr = self.exponential_decay(epoch)
            elif schedule_type == 'plateau':
                self.current_lr = self.reduce_on_plateau(
                    epoch, self.history['val_loss']
                )
            
            self.history['learning_rate'].append(self.current_lr)
            
            # Entrenar Ã©poca
            train_loss = self.train_epoch(X_train, y_train)
            val_loss, val_acc = self.evaluate(X_val, y_val)
            
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            
            if verbose and epoch % 10 == 0:
                print(f"Epoch {epoch:3d} | "
                      f"LR={self.current_lr:.6f} | "
                      f"Train Loss={train_loss:.4f} | "
                      f"Val Loss={val_loss:.4f}")
        
        return self.history
    
    def train_epoch(self, X_train, y_train):
        """Entrena una Ã©poca"""
        batches = self.create_batches(X_train, y_train)
        epoch_loss = 0
        
        for batch_X, batch_y in batches:
            predictions = self.model.forward(batch_X)
            loss = self.compute_loss(predictions, batch_y)
            epoch_loss += loss
            
            grad = predictions - batch_y.reshape(-1, 1)
            self.model.backward(grad)
            self.model.update(self.current_lr)  # Usar LR actual
        
        return epoch_loss / len(batches)
    
    def compute_loss(self, predictions, targets):
        """Binary Cross-Entropy"""
        targets = targets.reshape(-1, 1)
        epsilon = 1e-8
        return -np.mean(
            targets * np.log(predictions + epsilon) +
            (1 - targets) * np.log(1 - predictions + epsilon)
        )
    
    def evaluate(self, X, y):
        """EvalÃºa modelo"""
        predictions = self.model.forward(X)
        loss = self.compute_loss(predictions, y)
        pred_classes = (predictions > 0.5).astype(int)
        accuracy = np.mean(pred_classes.flatten() == y)
        return loss, accuracy
    
    def create_batches(self, X, y):
        """Crea batches"""
        n_samples = X.shape[0]
        indices = np.arange(n_samples)
        np.random.shuffle(indices)
        
        batches = []
        for start_idx in range(0, n_samples, self.batch_size):
            end_idx = min(start_idx + self.batch_size, n_samples)
            batch_indices = indices[start_idx:end_idx]
            batches.append((X[batch_indices], y[batch_indices]))
        
        return batches

def plot_lr_schedule(history):
    """Visualiza evoluciÃ³n del learning rate"""
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(history['learning_rate'])
    plt.xlabel('Ã‰poca')
    plt.ylabel('Learning Rate')
    plt.title('EvoluciÃ³n del Learning Rate')
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    plt.subplot(1, 2, 2)
    plt.plot(history['train_loss'], label='Train')
    plt.plot(history['val_loss'], label='Validation')
    plt.xlabel('Ã‰poca')
    plt.ylabel('PÃ©rdida')
    plt.title('Curvas de Aprendizaje')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
```

**Actividad 3.1:** Compara los tres tipos de scheduling. Â¿CuÃ¡l converge mÃ¡s rÃ¡pido?

> **Â¿QuÃ© debes observar y documentar?** Ejecuta el mismo modelo con los tres tipos de scheduling (step, exponential, plateau) usando el mismo learning rate inicial y el mismo nÃºmero mÃ¡ximo de Ã©pocas. Grafica la evoluciÃ³n del LR junto a las curvas de pÃ©rdida para visualizar la correlaciÃ³n entre los cambios de LR y las mejoras en la pÃ©rdida. Analiza: Â¿quÃ© estrategia alcanza la pÃ©rdida mÃ­nima primero? Â¿CuÃ¡l produce la menor pÃ©rdida de validaciÃ³n final? Â¿CuÃ¡l es mÃ¡s robusta a la elecciÃ³n inicial del LR? Documenta tus conclusiones con evidencia cuantitativa de los experimentos.

## ğŸ”¬ Parte 4: Monitoreo y Debugging (35 min)

### 4.1 Dashboard de Monitoreo

#### Fundamento TeÃ³rico: MÃ©tricas Clave y DiagnÃ³stico en Tiempo Real

El **monitoreo activo** durante el entrenamiento es lo que diferencia un experimento de ML bien conducido de un simple script que se ejecuta a ciegas. Un dashboard de mÃ©tricas permite detectar problemas a tiempo y tomar decisiones informadas: ajustar el LR, aumentar la regularizaciÃ³n, ampliar la capacidad del modelo o detener el experimento por completo.

**Â¿QuÃ© mÃ©tricas son mÃ¡s importantes?**

| MÃ©trica | Panel | QuÃ© indica |
|---------|-------|-----------|
| `val_loss` | Curva de pÃ©rdida | SeÃ±al de optimizaciÃ³n mÃ¡s sensible |
| `val_acc` | Curva de accuracy | Rendimiento interpretable |
| `val_loss - train_loss` | Gap de generalizaciÃ³n | Indicador directo de overfitting |
| `learning_rate` | LR schedule | Verificar que el scheduler funciona |
| `epoch_time` | Tiempo por Ã©poca | Detectar cuellos de botella |

**InterpretaciÃ³n del gap de generalizaciÃ³n a lo largo del tiempo:**

```
Gap = val_loss - train_loss

  Gap
  â”‚    /
  â”‚   /  â† creciente: overfitting progresivo
  â”‚  /
  â”‚ /
  â”‚â”€â”€â”€â”€ estable: equilibrio saludable
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ã©pocas

SeÃ±ales de alarma:
â€¢ Gap > 0.15 y creciente â†’ overfitting severo
â€¢ Gap oscilante fuertemente â†’ batch size muy pequeÃ±o
â€¢ Gap < 0 â†’ el modelo puede necesitar mÃ¡s capacidad
```

**SeÃ±ales de desvanecimiento de gradiente (Vanishing Gradient):** Si la pÃ©rdida de entrenamiento deja de disminuir desde las primeras Ã©pocas (se "congela" en un valor alto), puede indicar que los gradientes se vuelven cero o infinitesimalmente pequeÃ±os en las capas profundas. La soluciÃ³n es revisar las funciones de activaciÃ³n (ReLU en lugar de sigmoid/tanh en capas ocultas), la inicializaciÃ³n de pesos (Xavier/He), o aÃ±adir *batch normalization*.

```
SÃ­ntomas de problemas comunes durante el entrenamiento:

Problema               â”‚ SÃ­ntoma en dashboard             â”‚ AcciÃ³n
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LR muy alto            â”‚ PÃ©rdida explota o oscila mucho   â”‚ Reducir LR Ã·10
LR muy bajo            â”‚ PÃ©rdida no baja en 20+ Ã©pocas    â”‚ Aumentar LR Ã—10
Vanishing gradient     â”‚ PÃ©rdida se congela (no baja)     â”‚ Cambiar activaciÃ³n
Overfitting            â”‚ Gap > 0.15 y creciente           â”‚ L2, Dropout, Early stop
Underfitting           â”‚ Ambas pÃ©rdidas altas             â”‚ MÃ¡s Ã©pocas o modelo mayor
Data leakage           â”‚ Val < Train (val mejor que train)â”‚ Revisar preprocesamiento
```

**Â¿CuÃ¡ndo intervenir?** Interrumpe el entrenamiento si: (1) la pÃ©rdida de entrenamiento no disminuye en las primeras 20 Ã©pocas (posible problema de LR o inicializaciÃ³n); (2) el gap de generalizaciÃ³n supera 0.2 y sigue creciendo (overfitting severo); (3) la pÃ©rdida explota a NaN o infinito (LR demasiado grande o problema numÃ©rico). En todos estos casos, intervenir temprano ahorra tiempo de cÃ³mputo y permite corregir la configuraciÃ³n.

```python
class TrainingMonitor:
    """Monitor completo de entrenamiento"""
    
    def __init__(self):
        self.metrics = {
            'epoch': [],
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'learning_rate': [],
            'batch_time': [],
            'epoch_time': []
        }
    
    def update(self, epoch, train_loss, val_loss, train_acc, val_acc, lr, epoch_time):
        """Actualiza mÃ©tricas"""
        self.metrics['epoch'].append(epoch)
        self.metrics['train_loss'].append(train_loss)
        self.metrics['val_loss'].append(val_loss)
        self.metrics['train_acc'].append(train_acc)
        self.metrics['val_acc'].append(val_acc)
        self.metrics['learning_rate'].append(lr)
        self.metrics['epoch_time'].append(epoch_time)
    
    def print_summary(self):
        """Imprime resumen del entrenamiento"""
        print("\n" + "=" * 70)
        print("RESUMEN DEL ENTRENAMIENTO")
        print("=" * 70)
        
        best_val_idx = np.argmin(self.metrics['val_loss'])
        best_epoch = self.metrics['epoch'][best_val_idx]
        
        print(f"\nMejor Ã©poca: {best_epoch}")
        print(f"  Train Loss: {self.metrics['train_loss'][best_val_idx]:.4f}")
        print(f"  Val Loss: {self.metrics['val_loss'][best_val_idx]:.4f}")
        print(f"  Train Acc: {self.metrics['train_acc'][best_val_idx]:.4f}")
        print(f"  Val Acc: {self.metrics['val_acc'][best_val_idx]:.4f}")
        
        print(f"\nÃšltima Ã©poca: {self.metrics['epoch'][-1]}")
        print(f"  Train Loss: {self.metrics['train_loss'][-1]:.4f}")
        print(f"  Val Loss: {self.metrics['val_loss'][-1]:.4f}")
        print(f"  Train Acc: {self.metrics['train_acc'][-1]:.4f}")
        print(f"  Val Acc: {self.metrics['val_acc'][-1]:.4f}")
        
        total_time = sum(self.metrics['epoch_time'])
        avg_time = np.mean(self.metrics['epoch_time'])
        print(f"\nTiempo total: {total_time:.2f}s")
        print(f"Tiempo promedio por Ã©poca: {avg_time:.2f}s")
        
        # DiagnÃ³stico
        print("\n" + "-" * 70)
        self.diagnose()
        print("=" * 70)
    
    def diagnose(self):
        """Diagnostica problemas comunes"""
        train_loss = self.metrics['train_loss'][-1]
        val_loss = self.metrics['val_loss'][-1]
        gap = val_loss - train_loss
        
        print("DIAGNÃ“STICO:")
        
        if gap > 0.15:
            print("  âš ï¸  OVERFITTING detectado")
            print("      - Val loss >> Train loss")
            print("      - Recomendaciones:")
            print("        * Aumentar regularizaciÃ³n (L2, Dropout)")
            print("        * Early stopping con patience menor")
            print("        * MÃ¡s datos de entrenamiento")
            print("        * Reducir complejidad del modelo")
        
        elif train_loss > 0.5:
            print("  âš ï¸  UNDERFITTING detectado")
            print("      - Train loss alto")
            print("      - Recomendaciones:")
            print("        * Aumentar complejidad del modelo")
            print("        * Entrenar mÃ¡s Ã©pocas")
            print("        * Ajustar learning rate")
            print("        * Verificar preprocesamiento de datos")
        
        elif abs(gap) < 0.05:
            print("  âœ“ BUEN AJUSTE")
            print("      - Train y Val loss similares")
            print("      - Modelo generaliza bien")
        
        # Verificar convergencia
        if len(train_loss) > 10:
            recent_improvement = train_loss[-10] - train_loss[-1]
            if recent_improvement < 0.01:
                print("\n  â„¹ï¸  CONVERGENCIA alcanzada")
                print("      - PÃ©rdida estable en Ãºltimas 10 Ã©pocas")
    
    def plot_dashboard(self):
        """Visualiza dashboard completo"""
        fig = plt.figure(figsize=(16, 10))
        
        # 1. PÃ©rdidas
        ax1 = plt.subplot(2, 3, 1)
        ax1.plot(self.metrics['epoch'], self.metrics['train_loss'], 
                label='Train', linewidth=2)
        ax1.plot(self.metrics['epoch'], self.metrics['val_loss'], 
                label='Val', linewidth=2)
        ax1.set_xlabel('Ã‰poca')
        ax1.set_ylabel('PÃ©rdida')
        ax1.set_title('Curvas de PÃ©rdida')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Accuracy
        ax2 = plt.subplot(2, 3, 2)
        ax2.plot(self.metrics['epoch'], self.metrics['train_acc'], 
                label='Train', linewidth=2)
        ax2.plot(self.metrics['epoch'], self.metrics['val_acc'], 
                label='Val', linewidth=2)
        ax2.set_xlabel('Ã‰poca')
        ax2.set_ylabel('Accuracy')
        ax2.set_title('Curvas de Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Learning Rate
        ax3 = plt.subplot(2, 3, 3)
        ax3.plot(self.metrics['epoch'], self.metrics['learning_rate'], 
                linewidth=2, color='green')
        ax3.set_xlabel('Ã‰poca')
        ax3.set_ylabel('Learning Rate')
        ax3.set_title('Learning Rate Schedule')
        ax3.set_yscale('log')
        ax3.grid(True, alpha=0.3)
        
        # 4. Gap (Overfitting indicator)
        ax4 = plt.subplot(2, 3, 4)
        gap = np.array(self.metrics['val_loss']) - np.array(self.metrics['train_loss'])
        ax4.plot(self.metrics['epoch'], gap, linewidth=2, color='red')
        ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        ax4.fill_between(self.metrics['epoch'], 0, gap, 
                        where=(gap>0), alpha=0.3, color='red', label='Overfitting')
        ax4.set_xlabel('Ã‰poca')
        ax4.set_ylabel('Val Loss - Train Loss')
        ax4.set_title('Gap de GeneralizaciÃ³n')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. Tiempo por Ã©poca
        ax5 = plt.subplot(2, 3, 5)
        ax5.plot(self.metrics['epoch'], self.metrics['epoch_time'], 
                linewidth=2, color='purple')
        ax5.set_xlabel('Ã‰poca')
        ax5.set_ylabel('Tiempo (s)')
        ax5.set_title('Tiempo por Ã‰poca')
        ax5.grid(True, alpha=0.3)
        
        # 6. Resumen numÃ©rico
        ax6 = plt.subplot(2, 3, 6)
        ax6.axis('off')
        
        best_idx = np.argmin(self.metrics['val_loss'])
        summary_text = f"""
        RESUMEN
        
        Mejor Ã‰poca: {self.metrics['epoch'][best_idx]}
        Mejor Val Loss: {self.metrics['val_loss'][best_idx]:.4f}
        Mejor Val Acc: {self.metrics['val_acc'][best_idx]:.4f}
        
        Final:
        Train Loss: {self.metrics['train_loss'][-1]:.4f}
        Val Loss: {self.metrics['val_loss'][-1]:.4f}
        Gap: {gap[-1]:.4f}
        
        Total Ã‰pocas: {len(self.metrics['epoch'])}
        Tiempo Total: {sum(self.metrics['epoch_time']):.1f}s
        """
        
        ax6.text(0.1, 0.5, summary_text, fontsize=11, 
                family='monospace', verticalalignment='center')
        
        plt.tight_layout()
        plt.show()
```

**Actividad 4.1:** Usa el monitor para entrenar un modelo y analiza el dashboard completo.

> **Â¿QuÃ© debes observar y documentar?** Analiza los seis paneles del dashboard sistemÃ¡ticamente: (1) en las curvas de pÃ©rdida, identifica en quÃ© Ã©poca el modelo alcanza su mejor rendimiento de validaciÃ³n; (2) en las curvas de exactitud, verifica que la exactitud de validaciÃ³n no empieza a degradarse mientras la de entrenamiento sigue subiendo; (3) en el panel de LR, confirma que el scheduler opera como se diseÃ±Ã³; (4) en el gap de generalizaciÃ³n, observa si es creciente (overfitting), decreciente (el modelo aÃºn puede aprender) o estable (equilibrio); (5) en el tiempo por Ã©poca, comprueba que no hay variaciones inesperadas. Escribe un pÃ¡rrafo de diagnÃ³stico usando el vocabulario tÃ©cnico aprendido: overfitting, underfitting, convergencia, generalizaciÃ³n.

## ğŸ“Š AnÃ¡lisis Final de Rendimiento

### Experimento Completo: ComparaciÃ³n de TÃ©cnicas

#### Fundamento TeÃ³rico: ExperimentaciÃ³n Controlada en Machine Learning

Un **experimento controlado** en ML sigue los mismos principios del mÃ©todo cientÃ­fico: se varÃ­a **una sola variable independiente** a la vez (la tÃ©cnica de entrenamiento) manteniendo todo lo demÃ¡s constante (arquitectura del modelo, dataset, semilla aleatoria, nÃºmero de Ã©pocas). La funciÃ³n `run_experiment` implementa exactamente este diseÃ±o: crea un modelo fresco con la misma arquitectura e inicializaciÃ³n en cada experimento, garantizando que las diferencias en resultados se deben exclusivamente a la tÃ©cnica evaluada.

```
DiseÃ±o de experimento controlado:

Variable controlada: tÃ©cnica de entrenamiento
Variables fijas: arquitectura, datos, semilla aleatoria

  Experimento 1: Baseline (SGD simple)        â”€â”
  Experimento 2: + Mini-batches (batch=32)     â”œâ”€â”€ Misma arquitectura
  Experimento 3: + Early stopping              â”œâ”€â”€ Mismo dataset
  Experimento 4: + RegularizaciÃ³n L2           â”œâ”€â”€ Misma semilla aleatoria
  Experimento 5: + LR scheduling               â”€â”˜

  Comparar en: val_acc, val_loss, gap, tiempo
```

**Â¿Por quÃ© comparar mÃºltiples configuraciones?** Ninguna tÃ©cnica es universalmente superior: la efectividad del early stopping, la regularizaciÃ³n L2 y el LR scheduling depende del dataset especÃ­fico, la arquitectura, y el nivel de ruido de los datos. Comparar sistemÃ¡ticamente permite: (a) cuantificar el beneficio marginal de cada tÃ©cnica en el problema concreto, (b) identificar si tÃ©cnicas adicionales generan mejora o complejidad innecesaria, y (c) desarrollar intuiciÃ³n sobre quÃ© tÃ©cnicas funcionan mejor en quÃ© contextos.

**CÃ³mo extraer conclusiones vÃ¡lidas:**

| Principio | DescripciÃ³n |
|-----------|-------------|
| Una variable a la vez | Solo cambiar la tÃ©cnica, no la arquitectura |
| MÃºltiples semillas | Repetir 3-5 veces para estimar varianza |
| Evaluar en test set | Nunca en validaciÃ³n para comparar |
| Reportar media Â± std | No solo el mejor resultado obtenido |
| Contexto importa | Una tÃ©cnica puede ganar en un dataset y perder en otro |

Para que las comparaciones sean estadÃ­sticamente significativas, es buena prÃ¡ctica repetir cada experimento con mÃºltiples semillas aleatorias y reportar la media Â± desviaciÃ³n estÃ¡ndar del rendimiento. Un Ãºnico experimento puede dar resultados favorables o desfavorables por puro azar. AdemÃ¡s, la comparaciÃ³n debe hacerse siempre en el conjunto de **test** (nunca en validaciÃ³n), y todas las decisiones de diseÃ±o deben haberse tomado sin consultar el test set.

```python
import time

def run_experiment(model_fn, X_train, y_train, X_val, y_val, 
                   config_name, **kwargs):
    """Ejecuta un experimento completo de entrenamiento"""
    print(f"\n{'='*70}")
    print(f"Experimento: {config_name}")
    print(f"{'='*70}")
    
    # Crear modelo fresco
    model = model_fn()
    
    # Crear trainer
    trainer = kwargs.get('trainer_class', SimpleTrainer)(model, **kwargs.get('trainer_params', {}))
    
    # Entrenar
    start_time = time.time()
    history = trainer.train(X_train, y_train, X_val, y_val, 
                           epochs=kwargs.get('epochs', 100),
                           verbose=False)
    end_time = time.time()
    
    # Resultados
    final_train_loss = history['train_loss'][-1]
    final_val_loss = history['val_loss'][-1]
    final_train_acc = history['train_acc'][-1]
    final_val_acc = history['val_acc'][-1]
    
    print(f"\nResultados:")
    print(f"  Train Loss: {final_train_loss:.4f} | Accuracy: {final_train_acc:.4f}")
    print(f"  Val Loss: {final_val_loss:.4f} | Accuracy: {final_val_acc:.4f}")
    print(f"  Gap: {final_val_loss - final_train_loss:.4f}")
    print(f"  Tiempo: {end_time - start_time:.2f}s")
    
    return {
        'config': config_name,
        'history': history,
        'train_loss': final_train_loss,
        'val_loss': final_val_loss,
        'train_acc': final_train_acc,
        'val_acc': final_val_acc,
        'time': end_time - start_time
    }

# Ejecutar mÃºltiples experimentos
results = []

# Experimento 1: Baseline
results.append(run_experiment(
    create_model, X_train, y_train, X_val, y_val,
    "Baseline",
    trainer_class=SimpleTrainer,
    trainer_params={'learning_rate': 0.01},
    epochs=100
))

# Experimento 2: Con mini-batches
results.append(run_experiment(
    create_model, X_train, y_train, X_val, y_val,
    "Mini-batch SGD (batch=32)",
    trainer_class=BatchTrainer,
    trainer_params={'learning_rate': 0.01, 'batch_size': 32},
    epochs=100
))

# Experimento 3: Con early stopping
results.append(run_experiment(
    create_model, X_train, y_train, X_val, y_val,
    "Early Stopping (patience=10)",
    trainer_class=TrainerWithEarlyStopping,
    trainer_params={'learning_rate': 0.01, 'batch_size': 32},
    epochs=200  # MÃ¡s Ã©pocas pero con early stopping
))

# Comparar resultados
print("\n" + "="*70)
print("COMPARACIÃ“N DE EXPERIMENTOS")
print("="*70)

for result in results:
    print(f"\n{result['config']}:")
    print(f"  Val Accuracy: {result['val_acc']:.4f}")
    print(f"  Val Loss: {result['val_loss']:.4f}")
    print(f"  Gap: {result['val_loss'] - result['train_loss']:.4f}")
    print(f"  Tiempo: {result['time']:.2f}s")
```

## ğŸ¯ EJERCICIOS PROPUESTOS

### Nivel BÃ¡sico

**Ejercicio 1:** Loop de Entrenamiento BÃ¡sico
```
Implementa un loop de entrenamiento desde cero para:
- ClasificaciÃ³n binaria en dataset sintÃ©tico
- Mostrar progreso cada 10 Ã©pocas
- Graficar curvas de aprendizaje
```

**Ejercicio 2:** DivisiÃ³n de Datos
```
Dado un dataset, implementa:
- DivisiÃ³n train/val/test (70/15/15)
- NormalizaciÃ³n apropiada
- VerificaciÃ³n de distribuciÃ³n de clases
```

**Ejercicio 3:** Batch Processing
```
Compara el entrenamiento con diferentes batch sizes:
- Batch completo (batch size = tamaÃ±o del dataset)
- Mini-batch (32, 64, 128)
- Stochastic (batch size = 1)
Analiza tiempo y convergencia.
```

### Nivel Intermedio

**Ejercicio 4:** Early Stopping
```
Implementa early stopping con:
- Patience configurable
- Guardado del mejor modelo
- RestauraciÃ³n automÃ¡tica
- VisualizaciÃ³n de cuÃ¡ndo se detuvo
```

**Ejercicio 5:** Learning Rate Finder
```
Implementa el mÃ©todo "learning rate range test":
- Incrementa LR exponencialmente
- Grafica pÃ©rdida vs LR
- Encuentra el LR Ã³ptimo automÃ¡ticamente
```

**Ejercicio 6:** RegularizaciÃ³n
```
Compara modelos con:
- Sin regularizaciÃ³n
- L1 regularizaciÃ³n
- L2 regularizaciÃ³n
- L1 + L2 (Elastic Net)
Analiza impacto en overfitting.
```

### Nivel Avanzado

**Ejercicio 7:** Sistema Completo de Entrenamiento
```
Implementa un sistema con:
- Mini-batch SGD
- Early stopping
- LR scheduling (reduce on plateau)
- Checkpointing
- Logging completo
- Dashboard de visualizaciÃ³n
```

**Ejercicio 8:** Optimizadores Avanzados
```
Implementa desde cero:
- SGD con Momentum
- RMSprop
- Adam
Compara convergencia en diferentes problemas.
```

**Ejercicio 9:** K-Fold Cross-Validation
```
Implementa K-fold CV para:
- Evaluar robustez del modelo
- Estimar error de generalizaciÃ³n
- Seleccionar hiperparÃ¡metros
Promedia resultados de K modelos.
```

## ğŸ“ Entregables

### 1. CÃ³digo Fuente
- `trainer.py`: Clase principal de entrenamiento
- `early_stopping.py`: ImplementaciÃ³n de early stopping
- `lr_scheduler.py`: Schedulers de learning rate
- `regularization.py`: TÃ©cnicas de regularizaciÃ³n
- `monitor.py`: Sistema de monitoreo
- `experiments.ipynb`: Notebook con experimentos

### 2. Experimentos
- ComparaciÃ³n de batch sizes
- AnÃ¡lisis de early stopping
- EvaluaciÃ³n de regularizaciÃ³n
- ComparaciÃ³n de LR schedules
- Resultados en datasets reales

### 3. Visualizaciones
- Curvas de aprendizaje
- Dashboards de entrenamiento
- Comparaciones de configuraciones
- AnÃ¡lisis de convergencia

### 4. Reporte (3-4 pÃ¡ginas)
- MetodologÃ­a de experimentaciÃ³n
- Resultados y anÃ¡lisis
- Conclusiones sobre mejores prÃ¡cticas
- Recomendaciones para diferentes escenarios

## ğŸ¯ Criterios de EvaluaciÃ³n (CDIO)

### Conceive (Concebir) - 25%
- [ ] ComprensiÃ³n del proceso de entrenamiento completo
- [ ] IdentificaciÃ³n de hiperparÃ¡metros clave
- [ ] DiseÃ±o de experimentos apropiados
- [ ] PlanificaciÃ³n de estrategias de validaciÃ³n

### Design (DiseÃ±ar) - 25%
- [ ] ImplementaciÃ³n correcta del loop de entrenamiento
- [ ] CÃ³digo modular y extensible
- [ ] Sistema de monitoreo efectivo
- [ ] Manejo apropiado de datos

### Implement (Implementar) - 30%
- [ ] Early stopping funciona correctamente
- [ ] RegularizaciÃ³n reduce overfitting
- [ ] LR scheduling mejora convergencia
- [ ] Resultados reproducibles

### Operate (Operar) - 20%
- [ ] Experimentos bien diseÃ±ados
- [ ] AnÃ¡lisis crÃ­tico de resultados
- [ ] Comparaciones significativas
- [ ] DocumentaciÃ³n clara

## ğŸ“‹ RÃºbrica de EvaluaciÃ³n

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|--------------|---------------------|------------------|
| **Loop Entrenamiento** | Completo, robusto, eficiente | Funcional y correcto | BÃ¡sico pero funciona | Errores o incompleto |
| **Early Stopping** | Implementado perfectamente | Funciona bien | ImplementaciÃ³n bÃ¡sica | No funciona |
| **RegularizaciÃ³n** | MÃºltiples tÃ©cnicas, bien aplicadas | Al menos una tÃ©cnica | Intentado pero limitado | No implementado |
| **Experimentos** | Extensivos, bien diseÃ±ados | Buenos experimentos | Experimentos bÃ¡sicos | Experimentos insuficientes |
| **AnÃ¡lisis** | Profundo, insights valiosos | Buen anÃ¡lisis | AnÃ¡lisis superficial | AnÃ¡lisis pobre |

## ğŸ“š Referencias Adicionales

### Papers Fundamentales
1. Bottou, L. (2010). "Large-Scale Machine Learning with Stochastic Gradient Descent"
2. Hinton, G. et al. (2012). "Improving neural networks by preventing co-adaptation of feature detectors" (Dropout)
3. Ioffe, S. & Szegedy, C. (2015). "Batch Normalization"
4. Smith, L. N. (2017). "Cyclical Learning Rates for Training Neural Networks"

### Recursos Online
- **Deep Learning Book** (Goodfellow): CapÃ­tulo 8 - Optimization
- **CS231n Stanford**: Notas sobre entrenamiento de redes neuronales
- **Fast.ai**: Practical Deep Learning for Coders
- **Distill.pub**: Visualizaciones interactivas sobre optimizaciÃ³n

### Herramientas
- Scikit-learn: train_test_split, cross_validation
- TensorBoard: VisualizaciÃ³n de entrenamiento
- Weights & Biases: Tracking de experimentos

## ğŸ“ Notas Finales

### Mejores PrÃ¡cticas

1. **Siempre normaliza tus datos**: Hace que el entrenamiento sea mÃ¡s estable y rÃ¡pido.

2. **Usa early stopping**: Previene overfitting y ahorra tiempo de cÃ³mputo.

3. **Monitorea train y validation**: La relaciÃ³n entre ambas te dice mucho sobre tu modelo.

4. **Empieza simple**: Baseline simple primero, luego aÃ±ade complejidad.

5. **Guarda checkpoints**: Nunca sabes cuÃ¡ndo necesitarÃ¡s volver a un modelo anterior.

### Errores Comunes

âŒ **No mezclar datos antes de crear batches**: Lleva a mal entrenamiento
âŒ **Usar datos de test para early stopping**: Â¡Test debe ser intocable!
âŒ **No normalizar datos**: Entrenamiento inestable
âŒ **Learning rate muy alto**: Divergencia
âŒ **No monitorear validation**: No detectas overfitting

### Checklist de Entrenamiento

Antes de entrenar:
- [ ] Datos normalizados
- [ ] DivisiÃ³n train/val/test correcta
- [ ] Batch size razonable (16-128)
- [ ] Learning rate inicial apropiado (0.001-0.01)
- [ ] Early stopping configurado
- [ ] Monitoreo activado

Durante entrenamiento:
- [ ] Verificar que pÃ©rdida baja
- [ ] Monitorear gap train-val
- [ ] Observar convergencia
- [ ] Verificar tiempo por Ã©poca

DespuÃ©s de entrenar:
- [ ] Evaluar en test set
- [ ] Analizar errores
- [ ] Guardar modelo
- [ ] Documentar configuraciÃ³n

### ReflexiÃ³n Final

**El entrenamiento es donde la teorÃ­a se encuentra con la prÃ¡ctica**. Puedes tener el mejor algoritmo del mundo, pero sin un buen proceso de entrenamiento, no funcionarÃ¡.

Las tÃ©cnicas que aprendiste aquÃ­:
- Son usadas en TODOS los modelos de producciÃ³n
- Son la diferencia entre 80% y 95% de accuracy
- Te permiten diagnosticar y solucionar problemas
- Son transferibles a cualquier framework (PyTorch, TensorFlow)

### PrÃ³ximos Pasos

En el siguiente laboratorio (Lab 07), aprenderÃ¡s:
- MÃ©tricas de evaluaciÃ³n detalladas
- Matriz de confusiÃ³n
- Precision, Recall, F1-Score
- ROC curves y AUC
- AnÃ¡lisis de errores sistemÃ¡tico

Â¡El entrenamiento es donde todo cobra vida! ğŸš€

---

**"In theory, there is no difference between theory and practice. In practice, there is."** - Yogi Berra

**Â¡El entrenamiento es donde todo cobra vida! ğŸš€**
