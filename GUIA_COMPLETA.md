# Gu√≠a Completa del Curso de Redes Neuronales

## üìö Descripci√≥n General

Este repositorio contiene un curso completo de Redes Neuronales, Deep Learning e Inteligencia Artificial Generativa, dise√±ado para aprender desde cero con un enfoque muy did√°ctico, basado en el libro "Neural Networks from Scratch in Python".

## üéØ Objetivos del Curso

Al completar este curso, ser√°s capaz de:

1. ‚úÖ Comprender los fundamentos matem√°ticos de las redes neuronales
2. ‚úÖ Implementar redes neuronales completamente desde cero en Python
3. ‚úÖ Entrenar modelos para problemas reales de clasificaci√≥n y regresi√≥n
4. ‚úÖ Dominar arquitecturas especializadas: CNNs, RNNs/LSTMs y Transformers
5. ‚úÖ Procesar im√°genes con Redes Neuronales Convolucionales
6. ‚úÖ Trabajar con datos secuenciales usando RNNs y LSTMs
7. ‚úÖ Entender y aplicar mecanismos de atenci√≥n y Transformers
8. ‚úÖ Usar frameworks modernos como PyTorch y TensorFlow
9. ‚úÖ Crear modelos de IA Generativa (VAE, GAN)
10. ‚úÖ Aplicar buenas pr√°cticas en el desarrollo de modelos de ML

## üìã Estructura del Curso

### M√≥dulo 1: Fundamentos (Labs 01-02)

#### [Lab 01: Introducci√≥n a las Neuronas](Lab01_Introduccion_Neuronas/)
**Duraci√≥n estimada**: 2-3 horas

**Aprender√°s**:
- Qu√© es una neurona artificial
- Pesos, bias y producto punto
- Implementaci√≥n desde cero con NumPy
- Procesamiento en batch

**Archivos**:
- `teoria.md`: Fundamentos te√≥ricos completos
- `practica.ipynb`: Ejercicios interactivos
- `codigo/neurona.py`: Implementaci√≥n completa con ejemplos

**Conceptos clave**: Neurona, Pesos, Bias, Forward Pass, NumPy

---

#### [Lab 02: Primera Red Neuronal](Lab02_Primera_Red_Neuronal/)
**Duraci√≥n estimada**: 3-4 horas

**Aprender√°s**:
- Arquitectura de redes neuronales multicapa
- Conectar capas de neuronas
- Forward propagation
- Dise√±o de arquitecturas

**Archivos**:
- `teoria.md`: Arquitecturas y dimensiones
- `practica.ipynb`: Construcci√≥n de redes
- `codigo/red_neuronal.py`: Red neuronal completa

**Conceptos clave**: Capas, Arquitectura, Forward Propagation, Par√°metros

---

### M√≥dulo 2: Componentes Esenciales (Labs 03-04)

#### [Lab 03: Funciones de Activaci√≥n](Lab03_Funciones_Activacion/)
**Duraci√≥n estimada**: 3-4 horas

**Aprender√°s**:
- ReLU, Sigmoid, Tanh, Softmax
- Por qu√© necesitamos no-linealidad
- Derivadas de funciones de activaci√≥n
- Cu√°ndo usar cada funci√≥n

**Archivos**:
- `teoria.md`: Matem√°ticas y casos de uso
- `practica.ipynb`: Comparaci√≥n visual
- `codigo/activaciones.py`: Todas las funciones implementadas

**Conceptos clave**: No-linealidad, ReLU, Sigmoid, Softmax, Gradientes

---

#### [Lab 04: Funciones de P√©rdida](Lab04_Funciones_Perdida/)
**Duraci√≥n estimada**: 3-4 horas

**Aprender√°s**:
- MSE, MAE, Cross-Entropy
- C√≥mo medir el error de una red
- Descenso de gradiente b√°sico
- Optimizaci√≥n

**Archivos**:
- `teoria.md`: Funciones de p√©rdida explicadas
- `practica.ipynb`: Comparaci√≥n de loss functions
- `codigo/perdida.py`: Implementaciones completas

**Conceptos clave**: Loss Function, MSE, Cross-Entropy, Gradient Descent

---

### M√≥dulo 3: Entrenamiento (Labs 05-06)

#### [Lab 05: Backpropagation](Lab05_Backpropagation/)
**Duraci√≥n estimada**: 4-5 horas

**Aprender√°s**:
- Regla de la cadena
- Grafos computacionales
- Algoritmo de backpropagation completo
- C√°lculo de gradientes

**Archivos**:
- `teoria.md`: Matem√°ticas del backprop
- `practica.ipynb`: Implementaci√≥n paso a paso
- `codigo/backprop.py`: Backprop completo

**Conceptos clave**: Chain Rule, Gradientes, Backward Pass, Derivadas

---

#### [Lab 06: Entrenamiento de Redes](Lab06_Entrenamiento/)
**Duraci√≥n estimada**: 4-5 horas

**Aprender√°s**:
- Loop de entrenamiento completo
- Epochs, batches, learning rate
- Validaci√≥n y overfitting
- Entrenar en datos reales

**Archivos**:
- `teoria.md`: Proceso de entrenamiento
- `practica.ipynb`: Entrenamiento real
- `codigo/entrenamiento.py`: Sistema completo

**Conceptos clave**: Training Loop, Epochs, Batches, Validation, Overfitting

---

### M√≥dulo 4: Evaluaci√≥n y M√©tricas (Lab 07)

#### [Lab 07: M√©tricas de Evaluaci√≥n y Matriz de Confusi√≥n](Lab07_Metricas_Evaluacion/)
**Duraci√≥n estimada**: 3-4 horas

**Aprender√°s**:
- Matriz de confusi√≥n y sus componentes
- M√©tricas: Accuracy, Precision, Recall, F1-Score
- Validaci√≥n cruzada (K-Fold)
- Evaluaci√≥n en datasets balanceados y desbalanceados
- Optimizaci√≥n de umbrales de clasificaci√≥n

**Archivos**:
- `teoria.md`: Fundamentos de evaluaci√≥n de modelos
- `practica.ipynb`: Ejercicios con datasets reales
- `codigo/metricas.py`: Implementaci√≥n de m√©tricas desde cero

**Conceptos clave**: Matriz de confusi√≥n, TP/FP/FN/TN, Precision, Recall, F1-Score, Cross-Validation

---

### M√≥dulo 5: Frameworks y Herramientas (Lab 08)

#### [Lab 08: Frameworks de Deep Learning](Lab08_Frameworks_DeepLearning/)
**Duraci√≥n estimada**: 3-4 horas

**Aprender√°s**:
- PyTorch b√°sico
- TensorFlow/Keras b√°sico
- Comparaci√≥n de frameworks
- Migrar de c√≥digo manual a frameworks

**Archivos**:
- `teoria.md`: Comparaci√≥n PyTorch vs TensorFlow
- `practica.ipynb`: Mismo modelo en ambos frameworks
- `codigo/pytorch_ejemplo.py`: Ejemplo completo PyTorch
- `codigo/tensorflow_ejemplo.py`: Ejemplo completo TensorFlow

**Conceptos clave**: PyTorch, TensorFlow, High-level APIs, Autograd

---

### M√≥dulo 6: Arquitecturas Especializadas (Labs 10-12)

#### [Lab 10: Redes Neuronales Convolucionales (CNN)](Lab10_Redes_Neuronales_Convolucionales/)
**Duraci√≥n estimada**: 4-5 horas

**Aprender√°s**:
- Operaci√≥n de convoluci√≥n y correlaci√≥n cruzada
- Arquitectura de CNNs: capas convolucionales, pooling, fully connected
- Filtros y feature maps
- Aplicaciones en visi√≥n por computadora
- Implementaci√≥n desde cero y con PyTorch/TensorFlow

**Archivos**:
- `teoria.md`: Matem√°ticas de convoluci√≥n, arquitecturas CNN cl√°sicas
- `practica.ipynb`: Construcci√≥n de CNN para clasificaci√≥n de im√°genes
- `codigo/cnn.py`: Implementaci√≥n completa de CNN
- `codigo/cnn_pytorch.py`: CNN usando PyTorch
- `codigo/cnn_tensorflow.py`: CNN usando TensorFlow/Keras

**Conceptos clave**: Convoluci√≥n, Filtros, Feature Maps, Pooling, Stride, Padding, VGG, ResNet

---

#### [Lab 11: Redes Neuronales Recurrentes y LSTM](Lab11_Redes_Neuronales_Recurrentes_LSTM/)
**Duraci√≥n estimada**: 5-6 horas

**Aprender√°s**:
- Arquitectura de RNNs para datos secuenciales
- Backpropagation Through Time (BPTT)
- Problema del vanishing gradient en RNNs
- LSTMs: puertas de olvido, entrada y salida
- GRU como alternativa simplificada
- Aplicaciones en procesamiento de texto y series temporales

**Archivos**:
- `teoria.md`: RNNs, LSTMs, GRUs y sus matem√°ticas
- `practica.ipynb`: Predicci√≥n de series temporales y generaci√≥n de texto
- `codigo/rnn.py`: Implementaci√≥n RNN desde cero
- `codigo/lstm.py`: Implementaci√≥n LSTM completa
- `codigo/lstm_pytorch.py`: LSTM usando PyTorch
- `codigo/lstm_tensorflow.py`: LSTM usando TensorFlow/Keras

**Conceptos clave**: RNN, LSTM, GRU, Secuencias, Estado Oculto, Gates, BPTT, Vanishing Gradient

---

#### [Lab 12: Transformers y Mecanismos de Atenci√≥n](Lab12_Transformers/)
**Duraci√≥n estimada**: 6-7 horas

**Aprender√°s**:
- Mecanismo de self-attention
- Queries, Keys y Values (Q, K, V)
- Multi-head attention
- Positional encoding
- Arquitectura completa del Transformer
- Diferencias entre modelos encoder, decoder y encoder-decoder
- Aplicaciones modernas: BERT, GPT, Vision Transformers

**Archivos**:
- `teoria.md`: Arquitectura Transformer, atenci√≥n y positional encoding
- `practica.ipynb`: Construcci√≥n de Transformer paso a paso
- `codigo/attention.py`: Implementaci√≥n de mecanismos de atenci√≥n
- `codigo/transformer.py`: Transformer completo desde cero
- `codigo/transformer_pytorch.py`: Transformer usando PyTorch
- `codigo/transformer_tensorflow.py`: Transformer usando TensorFlow/Keras

**Conceptos clave**: Self-Attention, Multi-Head Attention, Q-K-V, Positional Encoding, Transformer, BERT, GPT, Encoder-Decoder

---

### M√≥dulo 7: IA Generativa (Lab 09)

#### [Lab 09: Inteligencia Artificial Generativa](Lab09_IA_Generativa/)
**Duraci√≥n estimada**: 4-5 horas

**Aprender√°s**:
- Conceptos de IA Generativa
- VAE (Variational Autoencoders) b√°sicos
- GAN (Generative Adversarial Networks) b√°sicos
- Aplicaciones de modelos generativos

**Archivos**:
- `teoria.md`: Fundamentos de IA Generativa
- `practica.ipynb`: Modelos generativos simples
- `codigo/generativo.py`: VAE y GAN b√°sicos

**Conceptos clave**: Generative AI, VAE, GAN, Latent Space, Generation

---

## üöÄ C√≥mo Empezar

### Requisitos Previos

**Conocimientos**:
- Python b√°sico (variables, funciones, clases)
- Matem√°ticas b√°sicas (√°lgebra, c√°lculo b√°sico)
- Opcional: NumPy b√°sico

**Software**:
- Python 3.8 o superior
- pip (gestor de paquetes)
- Jupyter Notebook
- Editor de c√≥digo (VS Code, PyCharm, etc.)

### Instalaci√≥n

1. **Clonar el repositorio**:
```bash
git clone https://github.com/eastaizah/Lab_NN.git
cd Lab_NN
```

2. **Crear entorno virtual** (recomendado):
```bash
# Linux/Mac
python -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
venv\Scripts\activate
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

4. **Verificar instalaci√≥n**:
```bash
python -c "import numpy, matplotlib, torch; print('‚úì Todo instalado correctamente')"
```

### Ejecutar los Laboratorios

**Opci√≥n 1: Jupyter Notebooks** (Recomendado para aprender)
```bash
jupyter notebook
# Navega a cada laboratorio y abre practica.ipynb
```

**Opci√≥n 2: Scripts Python** (Para ver ejemplos completos)
```bash
# Ejecutar ejemplo de Lab 01
python Lab01_Introduccion_Neuronas/codigo/neurona.py

# Ejecutar ejemplo de Lab 02
python Lab02_Primera_Red_Neuronal/codigo/red_neuronal.py

# Y as√≠ sucesivamente...
```

## üìñ Metodolog√≠a de Aprendizaje

### Para cada laboratorio:

1. **Leer la teor√≠a** (30-40 min)
   - Abre `teoria.md`
   - Lee cuidadosamente los conceptos
   - Toma notas de dudas

2. **Practicar con el notebook** (60-90 min)
   - Abre `practica.ipynb` en Jupyter
   - Ejecuta cada celda
   - Experimenta modificando valores
   - Completa los ejercicios

3. **Revisar el c√≥digo completo** (20-30 min)
   - Abre los archivos en `codigo/`
   - Estudia las implementaciones
   - Compara con tus ejercicios

4. **Experimentar y profundizar** (30-60 min)
   - Modifica par√°metros
   - Prueba diferentes arquitecturas
   - Resuelve los desaf√≠os

5. **Reflexionar** (10-15 min)
   - Responde las preguntas de reflexi√≥n
   - Anota conceptos clave
   - Identifica √°reas para revisar

## üìä Progreso Recomendado

### Semana 1: Fundamentos
- **D√≠a 1-2**: Lab 01 - Neuronas
- **D√≠a 3-4**: Lab 02 - Redes Neuronales
- **D√≠a 5**: Revisi√≥n y pr√°ctica adicional

### Semana 2: Componentes
- **D√≠a 1-2**: Lab 03 - Funciones de Activaci√≥n
- **D√≠a 3-4**: Lab 04 - Funciones de P√©rdida
- **D√≠a 5**: Proyecto integrador 1

### Semana 3: Entrenamiento y Evaluaci√≥n
- **D√≠a 1-3**: Lab 05 - Backpropagation
- **D√≠a 4-5**: Lab 06 - Entrenamiento

### Semana 4: M√©tricas y Frameworks
- **D√≠a 1-2**: Lab 07 - M√©tricas y Evaluaci√≥n
- **D√≠a 3-5**: Lab 08 - PyTorch/TensorFlow

### Semana 5: Visi√≥n por Computadora
- **D√≠a 1-3**: Lab 10 - CNNs
- **D√≠a 4-5**: Proyectos con im√°genes

### Semana 6: Procesamiento Secuencial
- **D√≠a 1-4**: Lab 11 - RNNs y LSTMs
- **D√≠a 5**: Proyectos con series temporales/texto

### Semana 7: Arquitecturas Modernas
- **D√≠a 1-5**: Lab 12 - Transformers y Atenci√≥n

### Semana 8: IA Generativa y Proyecto Final
- **D√≠a 1-3**: Lab 09 - IA Generativa
- **D√≠a 4-5**: Proyecto final integrador

## üõ§Ô∏è Camino de Aprendizaje

### Progresi√≥n Pedag√≥gica

El curso sigue una progresi√≥n cuidadosamente dise√±ada:

**Fase 1: Fundamentos (Labs 01-02)**
```
Neurona individual ‚Üí Capas de neuronas ‚Üí Redes neuronales densas
```

**Fase 2: Componentes Core (Labs 03-04)**
```
Funciones de activaci√≥n ‚Üí Funciones de p√©rdida ‚Üí Optimizaci√≥n b√°sica
```

**Fase 3: Mecanismos de Aprendizaje (Labs 05-07)**
```
Backpropagation ‚Üí Entrenamiento completo ‚Üí Evaluaci√≥n y m√©tricas
```

**Fase 4: Herramientas Profesionales (Lab 08)**
```
C√≥digo manual ‚Üí PyTorch/TensorFlow ‚Üí Desarrollo profesional
```

**Fase 5: Arquitecturas Especializadas (Labs 10-12)**
```
Visi√≥n (CNNs) ‚Üí Secuencias (RNNs/LSTMs) ‚Üí Atenci√≥n (Transformers)
```

**Fase 6: Generaci√≥n (Lab 09)**
```
Modelos discriminativos ‚Üí Modelos generativos ‚Üí VAE y GAN
```

### ¬øPor qu√© este orden?

1. **Labs 01-06**: Base s√≥lida antes de especializaciones
2. **Lab 07**: Evaluaci√≥n y m√©tricas - esencial antes de frameworks
3. **Lab 08**: Frameworks antes de arquitecturas complejas
4. **Lab 10 (CNNs)**: M√°s intuitivo, introduce convoluci√≥n
5. **Lab 11 (RNNs/LSTMs)**: Secuencias y memoria
6. **Lab 12 (Transformers)**: Combina conceptos de CNNs y RNNs
7. **Lab 09 (IA Generativa)**: Culminaci√≥n, usa todas las t√©cnicas anteriores

## üéì Evaluaci√≥n y Proyectos

### Proyectos Sugeridos

**Proyecto 1** (Despu√©s de Lab 02):
- Crear una red para clasificar flores Iris
- Implementar desde cero sin frameworks

**Proyecto 2** (Despu√©s de Lab 04):
- Red para reconocer d√≠gitos MNIST
- Incluir funciones de activaci√≥n y p√©rdida

**Proyecto 3** (Despu√©s de Lab 06):
- Sistema de clasificaci√≥n completo
- Con entrenamiento, validaci√≥n y evaluaci√≥n

**Proyecto 4** (Despu√©s de Lab 07):
- Reimplementar proyectos anteriores usando PyTorch o TensorFlow
- Comparar rendimiento y facilidad de uso

**Proyecto 5** (Despu√©s de Lab 09):
- Clasificador de im√°genes con CNN
- Usar CIFAR-10 o ImageNet subset
- Experimentar con data augmentation

**Proyecto 6** (Despu√©s de Lab 10):
- Predictor de series temporales (precio de acciones, clima)
- O generador de texto con LSTM
- Analizar an√°lisis de sentimiento

**Proyecto 7** (Despu√©s de Lab 11):
- Implementar mini-GPT o mini-BERT
- Tarea de NLP: clasificaci√≥n, QA o generaci√≥n
- Explorar fine-tuning de modelos pre-entrenados

**Proyecto Final** (Despu√©s de Lab 08):
- Modelo generativo para crear im√°genes (GAN)
- O sistema de text-to-image simplificado
- O chatbot usando Transformers
- Integrar m√∫ltiples conceptos del curso

## üìö Recursos Adicionales

### Libros
- **"Neural Networks from Scratch in Python"** - Harrison Kinsley & Daniel Kukie≈Ça
- **"Deep Learning"** - Ian Goodfellow, Yoshua Bengio, Aaron Courville
- **"Neural Networks and Deep Learning"** - Michael Nielsen
- **"Dive into Deep Learning"** - Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola
- **"Attention Is All You Need"** - Paper original de Transformers (Vaswani et al., 2017)

### Cursos Online
- [3Blue1Brown - Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk)
- [Fast.ai - Practical Deep Learning](https://www.fast.ai/)
- [Stanford CS231n - CNNs for Visual Recognition](http://cs231n.stanford.edu/)
- [Stanford CS224n - NLP with Deep Learning](http://web.stanford.edu/class/cs224n/)
- [DeepLearning.AI - Coursera](https://www.coursera.org/specializations/deep-learning)

### Herramientas Interactivas
- [TensorFlow Playground](http://playground.tensorflow.org/)
- [CNN Explainer](https://poloclub.github.io/cnn-explainer/)
- [Distill.pub](https://distill.pub/)
- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)
- [LSTMVis](http://lstm.seas.harvard.edu/)

### Papers Fundamentales
- **AlexNet** (2012): ImageNet Classification with Deep CNNs
- **VGGNet** (2014): Very Deep CNNs
- **ResNet** (2015): Deep Residual Learning
- **LSTM** (1997): Long Short-Term Memory
- **Attention** (2014): Neural Machine Translation by Jointly Learning to Align and Translate
- **Transformer** (2017): Attention Is All You Need
- **BERT** (2018): Pre-training of Deep Bidirectional Transformers
- **GPT** series (2018-2023): Language Models are Unsupervised Multitask Learners
- **Vision Transformer** (2020): An Image is Worth 16x16 Words

## ‚ùì FAQ (Preguntas Frecuentes)

**P: ¬øNecesito saber matem√°ticas avanzadas?**
R: No. El curso explica los conceptos matem√°ticos necesarios. √Ålgebra y c√°lculo b√°sico son suficientes.

**P: ¬øCu√°nto tiempo toma completar el curso?**
R: Aproximadamente 6-8 semanas dedicando 2-3 horas diarias para el curso completo (11 labs). Puedes ir a tu propio ritmo. El curso b√°sico (Labs 01-07) toma 4-5 semanas.

**P: ¬øPuedo saltar laboratorios?**
R: No recomendado. Cada lab construye sobre los anteriores. El orden es importante.

**P: ¬øQu√© hago si me atasco?**
R: 
1. Revisa la teor√≠a nuevamente
2. Estudia el c√≥digo de ejemplo
3. Busca en los recursos adicionales
4. Abre un issue en GitHub

**P: ¬øNecesito una GPU?**
R: No para Labs 01-07. Labs 09-11 funcionan en CPU pero GPU acelera significativamente. Lab 08 (GANs) se beneficia de GPU. Google Colab ofrece GPUs gratuitas.

**P: ¬øCu√°l es la diferencia entre CNNs, RNNs y Transformers?**
R: CNNs son ideales para datos espaciales (im√°genes). RNNs/LSTMs procesan secuencias (texto, series temporales). Transformers usan atenci√≥n, son m√°s r√°pidos y potentes que RNNs para secuencias largas.

**P: ¬øDebo aprender todos los labs en orden?**
R: S√≠ para Labs 01-07 (fundamentos). Labs 09-11 se pueden hacer en orden diferente si ya dominas los fundamentos, pero el orden recomendado es pedag√≥gicamente √≥ptimo.

## ü§ù Contribuir

¬øEncontraste un error? ¬øTienes una sugerencia?
1. Abre un issue describiendo el problema/sugerencia
2. O env√≠a un pull request con la mejora

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver archivo LICENSE para m√°s detalles.

## üôè Agradecimientos

Inspirado en:
- "Neural Networks from Scratch in Python" por Harrison Kinsley y Daniel Kukie≈Ça
- La comunidad de deep learning y open source
- Todos los recursos educativos mencionados

---

## üìû Contacto

Para preguntas, sugerencias o colaboraciones:
- GitHub Issues: [Lab_NN Issues](https://github.com/eastaizah/Lab_NN/issues)
- Discusiones: [Lab_NN Discussions](https://github.com/eastaizah/Lab_NN/discussions)

---

**¬°Feliz aprendizaje! üöÄüß†**

*√öltima actualizaci√≥n: Diciembre 2024*
