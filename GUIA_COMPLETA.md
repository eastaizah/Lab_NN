# Gu√≠a Completa del Curso de Redes Neuronales

## üìö Descripci√≥n General

Este repositorio contiene un curso completo de Redes Neuronales, Deep Learning e Inteligencia Artificial Generativa, dise√±ado para aprender desde cero con un enfoque muy did√°ctico, basado en el libro "Neural Networks from Scratch in Python".

## üéØ Objetivos del Curso

Al completar este curso, ser√°s capaz de:

1. ‚úÖ Comprender los fundamentos matem√°ticos de las redes neuronales
2. ‚úÖ Implementar redes neuronales completamente desde cero en Python
3. ‚úÖ Entrenar modelos para problemas reales de clasificaci√≥n y regresi√≥n
4. ‚úÖ Usar frameworks modernos como PyTorch y TensorFlow
5. ‚úÖ Entender los conceptos b√°sicos de IA Generativa (VAE, GAN)
6. ‚úÖ Aplicar buenas pr√°cticas en el desarrollo de modelos de ML

## üìã Estructura del Curso

### M√≥dulo 1: Fundamentos (Labs 01-02)

#### [Lab 01: Introducci√≥n a las Neuronas](Lab01_Introduccion_Neuronas/)
**Duraci√≥n estimada**: 2-3 horas

**Aprender√°s**:
- Qu√© es una neurona artificial
- Pesos, bias y producto punto
- Implementaci√≥n desde cero con NumPy
- Procesamiento en batch

**Archivos**:
- `teoria.md`: Fundamentos te√≥ricos completos
- `practica.ipynb`: Ejercicios interactivos
- `codigo/neurona.py`: Implementaci√≥n completa con ejemplos

**Conceptos clave**: Neurona, Pesos, Bias, Forward Pass, NumPy

---

#### [Lab 02: Primera Red Neuronal](Lab02_Primera_Red_Neuronal/)
**Duraci√≥n estimada**: 3-4 horas

**Aprender√°s**:
- Arquitectura de redes neuronales multicapa
- Conectar capas de neuronas
- Forward propagation
- Dise√±o de arquitecturas

**Archivos**:
- `teoria.md`: Arquitecturas y dimensiones
- `practica.ipynb`: Construcci√≥n de redes
- `codigo/red_neuronal.py`: Red neuronal completa

**Conceptos clave**: Capas, Arquitectura, Forward Propagation, Par√°metros

---

### M√≥dulo 2: Componentes Esenciales (Labs 03-04)

#### [Lab 03: Funciones de Activaci√≥n](Lab03_Funciones_Activacion/)
**Duraci√≥n estimada**: 3-4 horas

**Aprender√°s**:
- ReLU, Sigmoid, Tanh, Softmax
- Por qu√© necesitamos no-linealidad
- Derivadas de funciones de activaci√≥n
- Cu√°ndo usar cada funci√≥n

**Archivos**:
- `teoria.md`: Matem√°ticas y casos de uso
- `practica.ipynb`: Comparaci√≥n visual
- `codigo/activaciones.py`: Todas las funciones implementadas

**Conceptos clave**: No-linealidad, ReLU, Sigmoid, Softmax, Gradientes

---

#### [Lab 04: Funciones de P√©rdida](Lab04_Funciones_Perdida/)
**Duraci√≥n estimada**: 3-4 horas

**Aprender√°s**:
- MSE, MAE, Cross-Entropy
- C√≥mo medir el error de una red
- Descenso de gradiente b√°sico
- Optimizaci√≥n

**Archivos**:
- `teoria.md`: Funciones de p√©rdida explicadas
- `practica.ipynb`: Comparaci√≥n de loss functions
- `codigo/perdida.py`: Implementaciones completas

**Conceptos clave**: Loss Function, MSE, Cross-Entropy, Gradient Descent

---

### M√≥dulo 3: Entrenamiento (Labs 05-06)

#### [Lab 05: Backpropagation](Lab05_Backpropagation/)
**Duraci√≥n estimada**: 4-5 horas

**Aprender√°s**:
- Regla de la cadena
- Grafos computacionales
- Algoritmo de backpropagation completo
- C√°lculo de gradientes

**Archivos**:
- `teoria.md`: Matem√°ticas del backprop
- `practica.ipynb`: Implementaci√≥n paso a paso
- `codigo/backprop.py`: Backprop completo

**Conceptos clave**: Chain Rule, Gradientes, Backward Pass, Derivadas

---

#### [Lab 06: Entrenamiento de Redes](Lab06_Entrenamiento/)
**Duraci√≥n estimada**: 4-5 horas

**Aprender√°s**:
- Loop de entrenamiento completo
- Epochs, batches, learning rate
- Validaci√≥n y overfitting
- Entrenar en datos reales

**Archivos**:
- `teoria.md`: Proceso de entrenamiento
- `practica.ipynb`: Entrenamiento real
- `codigo/entrenamiento.py`: Sistema completo

**Conceptos clave**: Training Loop, Epochs, Batches, Validation, Overfitting

---

### M√≥dulo 4: Frameworks y IA Generativa (Labs 07-08)

#### [Lab 07: Frameworks de Deep Learning](Lab07_Frameworks_DeepLearning/)
**Duraci√≥n estimada**: 3-4 horas

**Aprender√°s**:
- PyTorch b√°sico
- TensorFlow/Keras b√°sico
- Comparaci√≥n de frameworks
- Migrar de c√≥digo manual a frameworks

**Archivos**:
- `teoria.md`: Comparaci√≥n PyTorch vs TensorFlow
- `practica.ipynb`: Mismo modelo en ambos frameworks
- `codigo/pytorch_ejemplo.py`: Ejemplo completo PyTorch
- `codigo/tensorflow_ejemplo.py`: Ejemplo completo TensorFlow

**Conceptos clave**: PyTorch, TensorFlow, High-level APIs, Autograd

---

#### [Lab 08: Inteligencia Artificial Generativa](Lab08_IA_Generativa/)
**Duraci√≥n estimada**: 4-5 horas

**Aprender√°s**:
- Conceptos de IA Generativa
- VAE (Variational Autoencoders) b√°sicos
- GAN (Generative Adversarial Networks) b√°sicos
- Aplicaciones de modelos generativos

**Archivos**:
- `teoria.md`: Fundamentos de IA Generativa
- `practica.ipynb`: Modelos generativos simples
- `codigo/generativo.py`: VAE y GAN b√°sicos

**Conceptos clave**: Generative AI, VAE, GAN, Latent Space, Generation

---

## üöÄ C√≥mo Empezar

### Requisitos Previos

**Conocimientos**:
- Python b√°sico (variables, funciones, clases)
- Matem√°ticas b√°sicas (√°lgebra, c√°lculo b√°sico)
- Opcional: NumPy b√°sico

**Software**:
- Python 3.8 o superior
- pip (gestor de paquetes)
- Jupyter Notebook
- Editor de c√≥digo (VS Code, PyCharm, etc.)

### Instalaci√≥n

1. **Clonar el repositorio**:
```bash
git clone https://github.com/eastaizah/Lab_NN.git
cd Lab_NN
```

2. **Crear entorno virtual** (recomendado):
```bash
# Linux/Mac
python -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
venv\Scripts\activate
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

4. **Verificar instalaci√≥n**:
```bash
python -c "import numpy, matplotlib, torch; print('‚úì Todo instalado correctamente')"
```

### Ejecutar los Laboratorios

**Opci√≥n 1: Jupyter Notebooks** (Recomendado para aprender)
```bash
jupyter notebook
# Navega a cada laboratorio y abre practica.ipynb
```

**Opci√≥n 2: Scripts Python** (Para ver ejemplos completos)
```bash
# Ejecutar ejemplo de Lab 01
python Lab01_Introduccion_Neuronas/codigo/neurona.py

# Ejecutar ejemplo de Lab 02
python Lab02_Primera_Red_Neuronal/codigo/red_neuronal.py

# Y as√≠ sucesivamente...
```

## üìñ Metodolog√≠a de Aprendizaje

### Para cada laboratorio:

1. **Leer la teor√≠a** (30-40 min)
   - Abre `teoria.md`
   - Lee cuidadosamente los conceptos
   - Toma notas de dudas

2. **Practicar con el notebook** (60-90 min)
   - Abre `practica.ipynb` en Jupyter
   - Ejecuta cada celda
   - Experimenta modificando valores
   - Completa los ejercicios

3. **Revisar el c√≥digo completo** (20-30 min)
   - Abre los archivos en `codigo/`
   - Estudia las implementaciones
   - Compara con tus ejercicios

4. **Experimentar y profundizar** (30-60 min)
   - Modifica par√°metros
   - Prueba diferentes arquitecturas
   - Resuelve los desaf√≠os

5. **Reflexionar** (10-15 min)
   - Responde las preguntas de reflexi√≥n
   - Anota conceptos clave
   - Identifica √°reas para revisar

## üìä Progreso Recomendado

### Semana 1: Fundamentos
- **D√≠a 1-2**: Lab 01 - Neuronas
- **D√≠a 3-4**: Lab 02 - Redes Neuronales
- **D√≠a 5**: Revisi√≥n y pr√°ctica adicional

### Semana 2: Componentes
- **D√≠a 1-2**: Lab 03 - Funciones de Activaci√≥n
- **D√≠a 3-4**: Lab 04 - Funciones de P√©rdida
- **D√≠a 5**: Proyecto integrador 1

### Semana 3: Entrenamiento
- **D√≠a 1-3**: Lab 05 - Backpropagation
- **D√≠a 4-5**: Lab 06 - Entrenamiento

### Semana 4: Frameworks y Generativa
- **D√≠a 1-2**: Lab 07 - PyTorch/TensorFlow
- **D√≠a 3-4**: Lab 08 - IA Generativa
- **D√≠a 5**: Proyecto final

## üéì Evaluaci√≥n y Proyectos

### Proyectos Sugeridos

**Proyecto 1** (Despu√©s de Lab 02):
- Crear una red para clasificar flores Iris
- Implementar desde cero sin frameworks

**Proyecto 2** (Despu√©s de Lab 04):
- Red para reconocer d√≠gitos MNIST
- Incluir funciones de activaci√≥n y p√©rdida

**Proyecto 3** (Despu√©s de Lab 06):
- Sistema de clasificaci√≥n completo
- Con entrenamiento, validaci√≥n y evaluaci√≥n

**Proyecto Final** (Despu√©s de Lab 08):
- Modelo generativo para crear im√°genes
- O clasificador usando PyTorch/TensorFlow

## üìö Recursos Adicionales

### Libros
- **"Neural Networks from Scratch in Python"** - Harrison Kinsley & Daniel Kukie≈Ça
- **"Deep Learning"** - Ian Goodfellow, Yoshua Bengio, Aaron Courville
- **"Neural Networks and Deep Learning"** - Michael Nielsen

### Cursos Online
- [3Blue1Brown - Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk)
- [Fast.ai - Practical Deep Learning](https://www.fast.ai/)
- [Stanford CS231n](http://cs231n.stanford.edu/)

### Herramientas Interactivas
- [TensorFlow Playground](http://playground.tensorflow.org/)
- [CNN Explainer](https://poloclub.github.io/cnn-explainer/)
- [Distill.pub](https://distill.pub/)

## ‚ùì FAQ (Preguntas Frecuentes)

**P: ¬øNecesito saber matem√°ticas avanzadas?**
R: No. El curso explica los conceptos matem√°ticos necesarios. √Ålgebra y c√°lculo b√°sico son suficientes.

**P: ¬øCu√°nto tiempo toma completar el curso?**
R: Aproximadamente 4-6 semanas dedicando 2-3 horas diarias. Puedes ir a tu propio ritmo.

**P: ¬øPuedo saltar laboratorios?**
R: No recomendado. Cada lab construye sobre los anteriores. El orden es importante.

**P: ¬øQu√© hago si me atasco?**
R: 
1. Revisa la teor√≠a nuevamente
2. Estudia el c√≥digo de ejemplo
3. Busca en los recursos adicionales
4. Abre un issue en GitHub

**P: ¬øNecesito una GPU?**
R: No para Labs 01-06. Labs 07-08 funcionan en CPU, aunque GPU acelera el entrenamiento.

## ü§ù Contribuir

¬øEncontraste un error? ¬øTienes una sugerencia?
1. Abre un issue describiendo el problema/sugerencia
2. O env√≠a un pull request con la mejora

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver archivo LICENSE para m√°s detalles.

## üôè Agradecimientos

Inspirado en:
- "Neural Networks from Scratch in Python" por Harrison Kinsley y Daniel Kukie≈Ça
- La comunidad de deep learning y open source
- Todos los recursos educativos mencionados

---

## üìû Contacto

Para preguntas, sugerencias o colaboraciones:
- GitHub Issues: [Lab_NN Issues](https://github.com/eastaizah/Lab_NN/issues)
- Discusiones: [Lab_NN Discussions](https://github.com/eastaizah/Lab_NN/discussions)

---

**¬°Feliz aprendizaje! üöÄüß†**

*√öltima actualizaci√≥n: Febrero 2026*
