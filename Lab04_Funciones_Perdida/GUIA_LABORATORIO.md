# Gu√≠a de Laboratorio: Funciones de P√©rdida y Optimizaci√≥n

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Funciones de P√©rdida y Optimizaci√≥n  
**C√≥digo:** Lab 04  
**Duraci√≥n:** 2-3 horas  
**Nivel:** B√°sico-Intermedio  

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Comprender qu√© son funciones de p√©rdida y su prop√≥sito
2. Implementar MSE, MAE, Cross-Entropy desde cero
3. Elegir funci√≥n de p√©rdida apropiada para cada problema
4. Implementar gradient descent b√°sico
5. Comprender efecto del learning rate
6. Reconocer y detectar overfitting
7. Calcular derivadas de funciones de p√©rdida
8. Combinar p√©rdida con activaci√≥n eficientemente
9. Aplicar regularizaci√≥n b√°sica

## üìö Prerrequisitos

### Conocimientos

- Completar Lab 01-03
- Python intermedio (clases, funciones, NumPy)
- √Ålgebra lineal b√°sica
- Comprensi√≥n de conceptos de labs anteriores

### Software

- Python 3.8+
- NumPy 1.19+
- Matplotlib 3.0+
- Jupyter Notebook (recomendado)

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` - Marco te√≥rico completo
- `README.md` - Visi√≥n general del laboratorio

## üìñ Introducci√≥n

Las **funciones de p√©rdida** cuantifican qu√© tan bien las predicciones del modelo se ajustan a los datos reales. Son el coraz√≥n del aprendizaje en redes neuronales.

### Contexto del Problema

Hasta ahora construimos redes y generamos predicciones, pero ¬øc√≥mo sabemos si son buenas? Necesitamos una m√©trica que:
- Cuantifique el error
- Gu√≠e la optimizaci√≥n
- Permita comparar modelos

### Funciones de P√©rdida

Miden discrepancia entre predicciones (≈∑) y valores reales (y):

```
L = f(y, ≈∑)
Objetivo: L ‚Üí 0 (minimizar p√©rdida)
```

### Conceptos Fundamentales

**1. Tipos principales:**
- **MSE:** Regresi√≥n (penaliza errores grandes)
- **MAE:** Regresi√≥n (robusta a outliers)
- **Binary Cross-Entropy:** Clasificaci√≥n binaria
- **Categorical Cross-Entropy:** Clasificaci√≥n multiclase

**2. Optimizaci√≥n:** Gradient descent ajusta par√°metros para minimizar p√©rdida

**3. Learning Rate:** Controla tama√±o del paso de actualizaci√≥n

### Aplicaciones

- Regresi√≥n: Predecir precios, temperaturas
- Clasificaci√≥n: Spam detection, reconocimiento de im√°genes
- Optimizaci√≥n: Entrenar cualquier modelo de ML

---

## ü§î Preguntas de Reflexi√≥n

> Antes de comenzar a programar, dedica unos minutos a reflexionar sobre las siguientes preguntas. No necesitas tener las respuestas correctas ahora; el objetivo es activar tu pensamiento cr√≠tico y motivar el aprendizaje.

1. **Sobre la elecci√≥n de la funci√≥n de p√©rdida:** Si tienes un problema de predicci√≥n de precios de casas donde algunos valores son extremadamente altos (mansiones), ¬øqu√© funci√≥n de p√©rdida crees que ser√≠a m√°s adecuada, MSE o MAE? ¬øPor qu√© los errores grandes deber√≠an o no deber√≠an penalizarse m√°s?

2. **Sobre la interpretaci√≥n probabil√≠stica:** En clasificaci√≥n binaria, la cross-entropy utiliza logaritmos. ¬øQu√© crees que sucede con la p√©rdida cuando el modelo predice una probabilidad de 0.99 para la clase correcta? ¬øY cuando predice 0.01? ¬øPor qu√© el logaritmo captura mejor esta asimetr√≠a que el error cuadr√°tico?

3. **Sobre el learning rate:** Imagina que est√°s bajando una monta√±a en la oscuridad. El learning rate ser√≠a el tama√±o de cada paso. ¬øQu√© pasar√≠a si tus pasos fueran demasiado grandes? ¬øY demasiado peque√±os? ¬øExiste un tama√±o de paso "perfecto" universal?

4. **Sobre overfitting:** Un modelo entrena durante 1000 √©pocas y logra un error de entrenamiento casi cero, pero su error en datos nuevos es 10 veces mayor. ¬øQu√© crees que est√° ocurriendo? ¬øC√≥mo distinguir√≠as este fen√≥meno durante el entrenamiento?

5. **Sobre regularizaci√≥n:** Si la regularizaci√≥n penaliza los pesos grandes, ¬øestamos realmente "empeorando" el entrenamiento a prop√≥sito? ¬øPor qu√© sacrificar rendimiento en entrenamiento podr√≠a mejorar el rendimiento en datos nuevos?

6. **Sobre la relaci√≥n p√©rdida-derivada:** El gradiente de la funci√≥n de p√©rdida indica la direcci√≥n de mayor crecimiento. Si queremos minimizar la p√©rdida, ¬øen qu√© direcci√≥n deber√≠amos mover los par√°metros? ¬øPor qu√© substraemos el gradiente en lugar de sumarlo?

---

## üî¨ Parte 1: Funciones para Regresi√≥n (45 min)

### 1.1 Mean Squared Error (MSE)

**¬øQu√© hacemos?** Implementamos el Error Cuadr√°tico Medio (MSE) y su derivada, la funci√≥n de p√©rdida m√°s utilizada para problemas de regresi√≥n.

**¬øPor qu√© lo hacemos?** MSE mide el promedio de los cuadrados de las diferencias entre los valores predichos y los reales. Al elevar al cuadrado, se consiguen dos efectos deseables: los errores siempre son positivos (no se cancelan entre s√≠) y los errores grandes reciben una penalizaci√≥n desproporcionadamente mayor que los errores peque√±os. Matem√°ticamente:

$$\text{MSE}(y, \hat{y}) = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

La derivada respecto a las predicciones $\hat{y}$ es:

$$\frac{\partial \text{MSE}}{\partial \hat{y}} = \frac{2}{n}(\hat{y} - y)$$

Esta derivada es la que se utiliza en backpropagation para ajustar los par√°metros de la red.

**¬øC√≥mo lo hacemos?** Usamos operaciones vectorizadas de NumPy: calculamos la diferencia elemento a elemento, la elevamos al cuadrado y tomamos la media. La derivada es simplemente el doble de la diferencia normalizada por el tama√±o del conjunto.

**¬øQu√© resultados debemos esperar?** Para predicciones perfectas `y_pred == y_true`, MSE debe dar exactamente 0. A medida que las predicciones se alejan de los valores reales, MSE crece cuadr√°ticamente. Un error promedio de 1 unidad da MSE = 1, pero un error promedio de 2 unidades da MSE = 4 (no 2).

```python
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

def mse_derivada(y_true, y_pred):
    return 2 * (y_pred - y_true) / len(y_true)

# Ejemplo
y_true = np.array([2, 4, 3])
y_pred = np.array([3, 5, 2])
print(f"MSE: {mse(y_true, y_pred)}")  # 1.0
```

### 1.2 Mean Absolute Error (MAE)

**¬øQu√© hacemos?** Implementamos el Error Absoluto Medio (MAE) y su derivada (subgradiente), una alternativa a MSE m√°s robusta ante valores at√≠picos.

**¬øPor qu√© lo hacemos?** A diferencia de MSE, MAE trata todos los errores de forma lineal, sin importar su magnitud. Esto lo hace menos sensible a outliers. La f√≥rmula es:

$$\text{MAE}(y, \hat{y}) = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

Su derivada (t√©cnicamente un subgradiente, ya que el valor absoluto no es diferenciable en 0) es:

$$\frac{\partial \text{MAE}}{\partial \hat{y}} = \frac{1}{n} \cdot \text{sign}(\hat{y} - y)$$

donde $\text{sign}(x) = +1$ si $x > 0$ y $-1$ si $x < 0$.

**¬øC√≥mo lo hacemos?** NumPy provee `np.abs()` para el valor absoluto y `np.sign()` para la funci√≥n signo. N√≥tese que el subgradiente siempre tiene magnitud constante ¬±1/n, lo que puede hacer la optimizaci√≥n menos eficiente cerca del m√≠nimo.

**¬øQu√© resultados debemos esperar?** Para el mismo conjunto de predicciones, MAE generalmente da un valor menor o igual que la ra√≠z cuadrada de MSE. La diferencia se ampl√≠a cuando hay errores grandes (outliers).

```python
def mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

def mae_derivada(y_true, y_pred):
    return np.sign(y_pred - y_true) / len(y_true)

# Ejemplo
print(f"MAE: {mae(y_true, y_pred)}")  # 1.0
```

### 1.3 Comparaci√≥n con Outliers

**¬øQu√© hacemos?** Comparamos el comportamiento de MSE y MAE cuando el conjunto de datos contiene un valor at√≠pico (outlier) extremo.

**¬øPor qu√© lo hacemos?** Comprender la sensibilidad diferencial a outliers es fundamental para elegir la funci√≥n de p√©rdida correcta. En datos reales, los outliers son frecuentes (errores de medici√≥n, casos excepcionales) y pueden distorsionar el entrenamiento. MSE penaliza los outliers cuadr√°ticamente: un error 10 veces mayor produce una p√©rdida 100 veces mayor. MAE los trata linealmente, siendo mucho m√°s robusto.

**¬øC√≥mo lo hacemos?** Creamos un conjunto de datos donde todos los errores son peque√±os excepto uno que es extremadamente grande (100 en lugar del valor real 5). Calculamos MSE y MAE para comparar el impacto.

**¬øQu√© resultados debemos esperar?** MSE reportar√° un valor muy alto (dominado por el outlier elevado al cuadrado), mientras que MAE reportar√° un valor m√°s moderado. Esto ilustra por qu√© en problemas donde los outliers son inevitables o representativos (como detecci√≥n de fraude), MAE o funciones h√≠bridas como Huber Loss son preferibles.

```python
y_true = np.array([1, 2, 3, 4, 5])
y_pred = np.array([1.1, 2.1, 3.1, 4.1, 100])  # √∫ltimo es outlier

print(f"MSE: {mse(y_true, y_pred):.2f}")  # Alto (penaliza mucho outlier)
print(f"MAE: {mae(y_true, y_pred):.2f}")  # Menor (m√°s robusto)
```

### Actividades

1. Implementar Huber Loss
2. Comparar MSE vs MAE con diferentes datos
3. Visualizar curvas de p√©rdida

---

## üî¨ Parte 2: Funciones para Clasificaci√≥n (45 min)

### 2.1 Binary Cross-Entropy

**¬øQu√© hacemos?** Implementamos la Entrop√≠a Cruzada Binaria (BCE), la funci√≥n de p√©rdida est√°ndar para problemas de clasificaci√≥n binaria (dos clases), junto con su derivada.

**¬øPor qu√© lo hacemos?** Para clasificaci√≥n binaria, las predicciones son probabilidades $\hat{y} \in (0, 1)$ obtenidas con la funci√≥n Sigmoid. MSE no es adecuado aqu√≠ porque el espacio de probabilidades no es lineal. La BCE tiene una interpretaci√≥n probabil√≠stica directa: mide la log-verosimilitud negativa bajo un modelo de Bernoulli:

$$\text{BCE}(y, \hat{y}) = -\frac{1}{n}\sum_{i=1}^{n}\left[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)\right]$$

Cuando la predicci√≥n $\hat{y}$ es correcta y confiada (cercana a 1 para $y=1$, o a 0 para $y=0$), la p√©rdida es m√≠nima. Cuando el modelo predice con alta confianza la clase equivocada, el logaritmo genera una penalizaci√≥n muy grande (el logaritmo de un n√∫mero cercano a 0 tiende a $-\infty$). La derivada combinada con Sigmoid simplifica elegantemente a $\hat{y} - y$.

**¬øC√≥mo lo hacemos?** Usamos `np.clip()` para evitar el c√°lculo de $\log(0)$, que es indefinido. Esto a√±ade un epsilon num√©rico ($\epsilon = 10^{-15}$) como l√≠mite inferior y superior de las predicciones.

**¬øQu√© resultados debemos esperar?** Predicciones perfectas producen una p√©rdida cercana a 0. Para el ejemplo dado (predicciones de alta confianza correctas), esperamos una BCE muy peque√±a. Si intercambiamos `y_true` y `y_pred`, la p√©rdida aumentar√° dram√°ticamente.

```python
def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):
    # Evitar log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + 
                    (1 - y_true) * np.log(1 - y_pred))

def binary_cross_entropy_derivada(y_true, y_pred):
    return (y_pred - y_true) / (y_pred * (1 - y_pred))

# Ejemplo
y_true = np.array([1, 0, 1, 0])
y_pred = np.array([0.9, 0.1, 0.8, 0.2])
print(f"BCE: {binary_cross_entropy(y_true, y_pred):.4f}")
```

### 2.2 Categorical Cross-Entropy

**¬øQu√© hacemos?** Implementamos la Entrop√≠a Cruzada Categ√≥rica (CCE), la extensi√≥n de BCE para problemas de clasificaci√≥n multiclase (m√°s de dos clases), donde las etiquetas est√°n en formato *one-hot*.

**¬øPor qu√© lo hacemos?** Cuando hay $C$ clases posibles, las predicciones son vectores de probabilidad $\hat{y} \in \mathbb{R}^C$ producidos por Softmax, y las etiquetas reales se representan en formato one-hot (vector con un 1 en la posici√≥n de la clase correcta y 0 en el resto). La CCE mide qu√© tan lejos est√° la distribuci√≥n predicha de la distribuci√≥n real:

$$\text{CCE}(y, \hat{y}) = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})$$

Dado que $y$ es one-hot, en la pr√°ctica solo el t√©rmino correspondiente a la clase correcta contribuye a la suma. La p√©rdida es simplemente $-\log(\hat{y}_{\text{clase correcta}})$: cuanto mayor sea la probabilidad asignada a la clase correcta, menor ser√° la p√©rdida.

**¬øC√≥mo lo hacemos?** Multiplicamos elemento a elemento `y_true * np.log(y_pred)` y sumamos a lo largo del eje de las clases (`axis=1`), luego tomamos el negativo de la media. El clipping previene errores num√©ricos.

**¬øQu√© resultados debemos esperar?** Para el ejemplo dado, con probabilidades de 0.7 y 0.8 para las clases correctas, esperamos una p√©rdida peque√±a (alrededor de 0.2-0.3). Si el modelo asignara probabilidades bajas a las clases correctas, la p√©rdida aumentar√≠a significativamente.

```python
def categorical_cross_entropy(y_true, y_pred, epsilon=1e-15):
    # y_true: one-hot encoded
    # y_pred: probabilidades de softmax
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

# Ejemplo
y_true = np.array([[0, 1, 0], [1, 0, 0]])  # One-hot
y_pred = np.array([[0.1, 0.7, 0.2], [0.8, 0.1, 0.1]])  # Softmax
print(f"CCE: {categorical_cross_entropy(y_true, y_pred):.4f}")
```

### 2.3 Sparse Categorical Cross-Entropy

**¬øQu√© hacemos?** Implementamos la versi√≥n "sparse" de CCE, que acepta directamente los √≠ndices de clase en lugar de vectores one-hot.

**¬øPor qu√© lo hacemos?** Cuando el n√∫mero de clases $C$ es muy grande (por ejemplo, 10,000 categor√≠as en clasificaci√≥n de palabras), almacenar las etiquetas en formato one-hot requiere una matriz de tama√±o $n \times C$, lo cual puede ser prohibitivamente costoso en memoria. La Sparse CCE acepta simplemente el √≠ndice de la clase correcta (un entero), siendo matem√°ticamente equivalente a CCE pero mucho m√°s eficiente en memoria:

$$\text{Sparse CCE}(y, \hat{y}) = -\frac{1}{n}\sum_{i=1}^{n}\log(\hat{y}_{i, y_i})$$

donde $y_i$ es el √≠ndice (entero) de la clase correcta para la muestra $i$. En comparaci√≥n con CCE:
- **CCE**: etiquetas como `[[0, 1, 0], [1, 0, 0]]` (one-hot, m√°s memoria)
- **Sparse CCE**: etiquetas como `[1, 0]` (√≠ndices, mucho menos memoria)

**¬øC√≥mo lo hacemos?** Usamos indexaci√≥n avanzada de NumPy (`y_pred[range(n), y_true]`) para seleccionar directamente la probabilidad predicha para cada clase correcta, evitando construir la representaci√≥n one-hot.

**¬øQu√© resultados debemos esperar?** Para las mismas predicciones y etiquetas (expresadas de forma diferente), Sparse CCE debe producir exactamente el mismo resultado num√©rico que CCE. Esto sirve como verificaci√≥n de consistencia entre las dos implementaciones.

```python
def sparse_categorical_cross_entropy(y_true, y_pred, epsilon=1e-15):
    # y_true: √≠ndices de clase (no one-hot)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    n_samples = y_pred.shape[0]
    log_probs = np.log(y_pred[range(n_samples), y_true])
    return -np.mean(log_probs)

# Ejemplo
y_true = np.array([1, 0])  # √çndices
y_pred = np.array([[0.1, 0.7, 0.2], [0.8, 0.1, 0.1]])
print(f"Sparse CCE: {sparse_categorical_cross_entropy(y_true, y_pred):.4f}")
```

### Actividades

1. Implementar todas las p√©rdidas
2. Comparar BCE vs MSE para clasificaci√≥n
3. Verificar derivadas num√©ricamente

---

## üî¨ Parte 3: Gradient Descent (45 min)

### 3.1 Implementaci√≥n B√°sica

**¬øQu√© hacemos?** Implementamos el algoritmo de Gradient Descent (Descenso de Gradiente) desde cero para optimizar los par√°metros de un modelo de regresi√≥n lineal.

**¬øPor qu√© lo hacemos?** Gradient Descent es el algoritmo fundamental de optimizaci√≥n en deep learning. Su objetivo es encontrar los par√°metros $w$ (pesos) y $b$ (sesgo) que minimizan la funci√≥n de p√©rdida. El algoritmo sigue estos pasos en cada iteraci√≥n (√©poca):

1. **Forward pass:** Calcular predicciones $\hat{y} = X \cdot w + b$
2. **C√°lculo de p√©rdida:** $L = \text{MSE}(y, \hat{y})$
3. **C√°lculo de gradientes:** $\nabla_w L = \frac{\partial L}{\partial w}$ y $\nabla_b L = \frac{\partial L}{\partial b}$
4. **Actualizaci√≥n de par√°metros:** $w \leftarrow w - \alpha \cdot \nabla_w L$ y $b \leftarrow b - \alpha \cdot \nabla_b L$

donde $\alpha$ es el **learning rate** (tasa de aprendizaje). La clave est√° en el signo negativo: nos movemos en la direcci√≥n **opuesta** al gradiente, que es la direcci√≥n de mayor descenso.

**¬øC√≥mo lo hacemos?** Para regresi√≥n lineal con MSE, los gradientes tienen forma cerrada: $\frac{\partial L}{\partial w} = \frac{2}{n} X^T (\hat{y} - y)$ y $\frac{\partial L}{\partial b} = \frac{2}{n} \sum(\hat{y} - y)$. Usamos multiplicaci√≥n matricial (`@`) para eficiencia.

**¬øQu√© resultados debemos esperar?** La p√©rdida debe disminuir mon√≥tonamente con cada √©poca (para un learning rate apropiado). Al imprimir cada 10 √©pocas, veremos c√≥mo el modelo converge gradualmente hacia la soluci√≥n √≥ptima.

```python
def gradient_descent(X, y, learning_rate=0.01, epochs=100):
    # Inicializar par√°metros
    w = np.zeros(X.shape[1])
    b = 0
    
    losses = []
    
    for epoch in range(epochs):
        # Forward pass
        y_pred = X @ w + b
        
        # Calcular p√©rdida
        loss = mse(y, y_pred)
        losses.append(loss)
        
        # Calcular gradientes
        dw = 2 * X.T @ (y_pred - y) / len(y)
        db = 2 * np.mean(y_pred - y)
        
        # Actualizar par√°metros
        w -= learning_rate * dw
        b -= learning_rate * db
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss = {loss:.4f}")
    
    return w, b, losses
```

### 3.2 Efecto del Learning Rate

**¬øQu√© hacemos?** Comparamos visualmente el efecto de diferentes valores de learning rate sobre la curva de convergencia del entrenamiento.

**¬øPor qu√© lo hacemos?** El learning rate $\alpha$ es el hiperpar√°metro m√°s cr√≠tico en la optimizaci√≥n. Governa la din√°mica de convergencia:

- **$\alpha$ muy peque√±o** (ej. 0.0001): convergencia lenta, requiere muchas √©pocas, puede quedarse atascado en m√≠nimos locales.
- **$\alpha$ √≥ptimo** (ej. 0.01): convergencia estable y r√°pida hacia el m√≠nimo global.
- **$\alpha$ grande** (ej. 0.5): oscilaciones alrededor del m√≠nimo; el algoritmo "salta" de un lado al otro sin converger.
- **$\alpha$ muy grande** (ej. 1.0+): divergencia; la p√©rdida **aumenta** en lugar de disminuir, el entrenamiento falla completamente.

Esta propiedad se relaciona con el radio espectral de la matriz hessiana de la funci√≥n de p√©rdida: existe una tasa de aprendizaje m√°xima te√≥rica m√°s all√° de la cual el gradiente descent diverge.

**¬øC√≥mo lo hacemos?** Entrenamos el mismo modelo con los mismos datos usando cuatro valores de learning rate diferentes, y graficamos todas las curvas de p√©rdida en la misma figura para comparaci√≥n directa.

**¬øQu√© resultados debemos esperar?** Veremos cuatro comportamientos claramente distintos: convergencia lenta, convergencia √≥ptima, oscilaciones y divergencia. La gr√°fica resultante es una de las visualizaciones m√°s instructivas en el aprendizaje de deep learning.

```python
def comparar_learning_rates(X, y):
    lrs = [0.001, 0.01, 0.1, 1.0]
    
    plt.figure(figsize=(12, 4))
    
    for lr in lrs:
        _, _, losses = gradient_descent(X, y, learning_rate=lr, epochs=100)
        plt.plot(losses, label=f'LR={lr}')
    
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Efecto del Learning Rate')
    plt.grid(True)
    plt.savefig('learning_rates.png')
```

### 3.3 Variantes de Gradient Descent

**¬øQu√© hacemos?** Implementamos las tres variantes principales del algoritmo de Gradient Descent: Batch GD, Stochastic GD (SGD) y Mini-Batch GD.

**¬øPor qu√© lo hacemos?** La diferencia fundamental entre las variantes radica en **cu√°ntos datos** se usan para calcular el gradiente en cada actualizaci√≥n. Esto crea un trade-off entre precisi√≥n del gradiente y velocidad de actualizaci√≥n:

| Variante | Datos por update | Gradiente | Velocidad | Uso de memoria | Convergencia |
|----------|-----------------|-----------|-----------|----------------|--------------|
| **Batch GD** | Todo el dataset | Exacto | Lenta | Alta | Suave, estable |
| **SGD** | 1 muestra | Ruidoso | Muy r√°pida | M√≠nima | Ruidosa, puede escapar m√≠nimos locales |
| **Mini-Batch GD** | $k$ muestras ($k$=32-256) | Aproximado | Balanceada | Moderada | **Est√°ndar en pr√°ctica** |

Mini-Batch GD combina lo mejor de ambos mundos: es lo suficientemente r√°pido (m√∫ltiples actualizaciones por √©poca) y lo suficientemente preciso (el gradiente promediado sobre un batch es una buena estimaci√≥n del gradiente real).

**¬øC√≥mo lo hacemos?** Para SGD procesamos cada muestra individualmente. Para Mini-Batch, mezclamos aleatoriamente los datos en cada √©poca (`np.random.permutation`) y procesamos en chunks de tama√±o `batch_size`. El shuffle es crucial para evitar que el modelo "memorice" el orden de los datos.

**¬øQu√© resultados debemos esperar?** Con los mismos datos y √©pocas, los tres m√©todos deber√≠an llegar a soluciones similares. Sin embargo, SGD tendr√° una curva de p√©rdida ruidosa (zigzagueante), mientras que Batch GD tendr√° una curva perfectamente suave pero m√°s lenta por √©poca.

```python
# Batch Gradient Descent (ya implementado arriba)

# Stochastic Gradient Descent
def sgd(X, y, learning_rate=0.01, epochs=100):
    w = np.zeros(X.shape[1])
    b = 0
    
    for epoch in range(epochs):
        for i in range(len(X)):
            xi = X[i:i+1]
            yi = y[i:i+1]
            
            y_pred = xi @ w + b
            dw = 2 * xi.T @ (y_pred - yi)
            db = 2 * (y_pred - yi)
            
            w -= learning_rate * dw.flatten()
            b -= learning_rate * db[0]
    
    return w, b

# Mini-batch Gradient Descent
def mini_batch_gd(X, y, batch_size=32, learning_rate=0.01, epochs=100):
    w = np.zeros(X.shape[1])
    b = 0
    
    for epoch in range(epochs):
        indices = np.random.permutation(len(X))
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        for i in range(0, len(X), batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            y_pred = X_batch @ w + b
            dw = 2 * X_batch.T @ (y_pred - y_batch) / len(X_batch)
            db = 2 * np.mean(y_pred - y_batch)
            
            w -= learning_rate * dw
            b -= learning_rate * db
    
    return w, b
```

### Actividades

1. Implementar gradient descent para clasificaci√≥n
2. Comparar batch, SGD, mini-batch
3. Encontrar learning rate √≥ptimo

---

## üî¨ Parte 4: Overfitting y Regularizaci√≥n (40 min)

### 4.1 Demostraci√≥n de Overfitting

**¬øQu√© hacemos?** Demostramos el fen√≥meno de overfitting ajustando modelos polinomiales de diferente complejidad a un conjunto de datos peque√±o, observando c√≥mo la brecha entre error de entrenamiento y validaci√≥n crece con la complejidad del modelo.

**¬øPor qu√© lo hacemos?** Overfitting es el problema m√°s fundamental y frecuente en machine learning. Surge del **trade-off sesgo-varianza** (*bias-variance tradeoff*):

- **Underfitting (alto sesgo):** El modelo es demasiado simple para capturar los patrones reales. Alto error tanto en entrenamiento como en validaci√≥n.
- **Overfitting (alta varianza):** El modelo es demasiado complejo y "memoriza" los datos de entrenamiento, incluyendo el ruido. Error muy bajo en entrenamiento pero muy alto en validaci√≥n.
- **Balance √≥ptimo:** El modelo captura los patrones reales sin memorizar el ruido. Ambos errores son bajos y similares.

Matem√°ticamente, el error esperado de un modelo puede descomponerse como:

$$\text{Error} = \text{Sesgo}^2 + \text{Varianza} + \text{Ruido irreducible}$$

Aumentar la complejidad del modelo reduce el sesgo pero aumenta la varianza. El objetivo es encontrar la complejidad que minimiza la suma total.

**¬øC√≥mo lo hacemos?** Usamos regresi√≥n polinomial con grados 1 (lineal), 3 (c√∫bico) y 10 (alto grado). Para datos generados con una relaci√≥n lineal m√°s ruido gaussiano, el modelo de grado 10 tendr√° suficiente capacidad para "memorizar" los 15 puntos de entrenamiento perfectamente, pero fallar√° en los 5 puntos de validaci√≥n.

**¬øQu√© resultados debemos esperar?** Para grado 1: errores similares en train y val (underfitting moderado). Para grado 3: ambos errores bajos (modelo adecuado). Para grado 10: error de entrenamiento muy bajo pero error de validaci√≥n muy alto (overfitting severo). Esta divergencia es la "se√±al de alarma" del overfitting.

```python
def simular_overfitting():
    # Generar datos
    np.random.seed(42)
    X = np.linspace(0, 10, 20).reshape(-1, 1)
    y = 2 * X + 1 + np.random.randn(20, 1) * 2
    
    # Split train/val
    X_train, X_val = X[:15], X[15:]
    y_train, y_val = y[:15], y[15:]
    
    # Entrenar con diferentes complejidades
    for degree in [1, 3, 10]:
        # Crear features polinomiales
        X_poly_train = np.column_stack([X_train**i for i in range(1, degree+1)])
        X_poly_val = np.column_stack([X_val**i for i in range(1, degree+1)])
        
        w, b, _ = gradient_descent(X_poly_train, y_train, epochs=1000)
        
        train_loss = mse(y_train, X_poly_train @ w + b)
        val_loss = mse(y_val, X_poly_val @ w + b)
        
        print(f"Degree {degree}: Train={train_loss:.2f}, Val={val_loss:.2f}")
```

### 4.2 L2 Regularization

**¬øQu√© hacemos?** Implementamos la regularizaci√≥n L2 (tambi√©n llamada *Ridge* o *weight decay*), a√±adiendo un t√©rmino de penalizaci√≥n a la funci√≥n de p√©rdida que desincentiva pesos grandes.

**¬øPor qu√© lo hacemos?** La regularizaci√≥n L2 es la t√©cnica m√°s cl√°sica para combatir el overfitting. La intuici√≥n es elegante: pesos grandes indican que el modelo depende excesivamente de caracter√≠sticas espec√≠ficas del dataset de entrenamiento (incluyendo el ruido). Al penalizar los pesos grandes, forzamos al modelo a distribuir la "responsabilidad" entre m√°s caracter√≠sticas, generalizando mejor.

La funci√≥n de p√©rdida regularizada es:

$$L_{\text{reg}}(w) = L(w) + \lambda \|w\|_2^2 = L(w) + \lambda \sum_{j} w_j^2$$

donde $\lambda$ (lambda) es el **coeficiente de regularizaci√≥n** que controla el balance entre ajustar los datos y mantener pesos peque√±os. El gradiente de la p√©rdida regularizada es:

$$\frac{\partial L_{\text{reg}}}{\partial w} = \frac{\partial L}{\partial w} + 2\lambda w$$

El t√©rmino $2\lambda w$ act√∫a como una "fuerza restauradora" que empuja continuamente los pesos hacia cero en cada actualizaci√≥n: $w \leftarrow w(1 - 2\alpha\lambda) - \alpha \frac{\partial L}{\partial w}$. Por eso tambi√©n se llama *weight decay* (decaimiento de pesos).

**¬øC√≥mo lo hacemos?** A√±adimos `lambda_reg * np.sum(w**2)` a la p√©rdida calculada y `2 * lambda_reg * w` al gradiente de los pesos. Nota importante: el sesgo $b$ generalmente **no** se regulariza, ya que no contribuye al overfitting de la misma manera.

**¬øQu√© resultados debemos esperar?** Con regularizaci√≥n, el modelo de alto grado polinomial deber√≠a producir pesos m√°s peque√±os y una brecha train/val reducida. Con $\lambda$ demasiado grande, el modelo underfit (ignora los datos). El valor √≥ptimo de $\lambda$ se encuentra mediante validaci√≥n cruzada.

```python
def gradient_descent_l2(X, y, lambda_reg=0.01, learning_rate=0.01, epochs=100):
    w = np.zeros(X.shape[1])
    b = 0
    
    for epoch in range(epochs):
        y_pred = X @ w + b
        
        # P√©rdida con regularizaci√≥n
        loss = mse(y, y_pred) + lambda_reg * np.sum(w**2)
        
        # Gradientes con regularizaci√≥n
        dw = 2 * X.T @ (y_pred - y) / len(y) + 2 * lambda_reg * w
        db = 2 * np.mean(y_pred - y)
        
        w -= learning_rate * dw
        b -= learning_rate * db
    
    return w, b
```

### 4.3 Early Stopping

**¬øQu√© hacemos?** Implementamos el algoritmo de Early Stopping (parada temprana), una t√©cnica de regularizaci√≥n impl√≠cita que detiene el entrenamiento cuando el rendimiento en el conjunto de validaci√≥n deja de mejorar.

**¬øPor qu√© lo hacemos?** A diferencia de L2 que modifica la funci√≥n de p√©rdida, Early Stopping es una forma de regularizaci√≥n "gratis": no requiere cambiar el modelo ni a√±adir hiperpar√°metros de regularizaci√≥n. La idea es simple pero poderosa: durante el entrenamiento, la p√©rdida de entrenamiento disminuye monot√≥nicamente, pero la p√©rdida de validaci√≥n t√≠picamente tiene forma de "U" (primero baja, luego sube cuando empieza el overfitting). Early Stopping detiene el entrenamiento en el "valle" de esa U.

El mecanismo de **paciencia** (*patience*) es crucial: no detenemos el entrenamiento ante la primera √©poca en que la validaci√≥n no mejora (podr√≠a ser una fluctuaci√≥n temporal), sino solo si no mejora durante $p$ √©pocas consecutivas. Esto hace el algoritmo m√°s robusto a oscilaciones en la p√©rdida de validaci√≥n. El proceso:

1. Monitorear la p√©rdida de validaci√≥n en cada √©poca
2. Si mejora ‚Üí guardar los pesos actuales como "mejor modelo" y resetear contador
3. Si no mejora ‚Üí incrementar contador de paciencia
4. Si contador ‚â• paciencia ‚Üí detener y restaurar los mejores pesos

**¬øC√≥mo lo hacemos?** Guardamos copias de los mejores pesos (`best_w`, `best_b`) y un contador de paciencia. Al final del entrenamiento (ya sea por paciencia o por completar todas las √©pocas), devolvemos los mejores pesos encontrados, no los √∫ltimos.

**¬øQu√© resultados debemos esperar?** El entrenamiento se detendr√° antes de las 1000 √©pocas configuradas. El mensaje "Early stopping en epoch X" indica cu√°ndo se activ√≥. Los pesos devueltos corresponden al mejor modelo en validaci√≥n, no al modelo final sobreajustado.

```python
def train_with_early_stopping(X_train, y_train, X_val, y_val, patience=10):
    w = np.zeros(X_train.shape[1])
    b = 0
    
    best_val_loss = float('inf')
    patience_counter = 0
    best_w, best_b = w.copy(), b
    
    for epoch in range(1000):
        # Entrenar
        y_pred = X_train @ w + b
        dw = 2 * X_train.T @ (y_pred - y_train) / len(y_train)
        db = 2 * np.mean(y_pred - y_train)
        w -= 0.01 * dw
        b -= 0.01 * db
        
        # Validar
        y_val_pred = X_val @ w + b
        val_loss = mse(y_val, y_val_pred)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_w, best_b = w.copy(), b
            patience_counter = 0
        else:
            patience_counter += 1
        
        if patience_counter >= patience:
            print(f"Early stopping en epoch {epoch}")
            break
    
    return best_w, best_b
```

### Actividades

1. Demostrar overfitting con datos polinomiales
2. Aplicar L2 regularization
3. Implementar early stopping

---

## üìä An√°lisis Final de Rendimiento

### Por qu√© el An√°lisis de Rendimiento es Fundamental

**¬øPor qu√© analizamos el rendimiento de nuestras implementaciones?** En el contexto de deep learning, la eficiencia computacional no es un lujo sino una necesidad. Las redes neuronales reales se entrenan con millones de par√°metros y millones de ejemplos; una implementaci√≥n 10 veces m√°s lenta puede significar d√≠as de entrenamiento adicionales. Comprender el rendimiento de nuestras implementaciones nos permite:

- **Identificar cuellos de botella:** Saber qu√© parte del c√≥digo consume m√°s tiempo permite optimizarla prioritariamente.
- **Escalar adecuadamente:** Entender c√≥mo el tiempo de ejecuci√≥n crece con el tama√±o de los datos (complejidad algor√≠tmica).
- **Tomar decisiones informadas:** Elegir entre claridad de c√≥digo y eficiencia computacional seg√∫n el contexto.
- **Prepararse para frameworks:** NumPy vectorizado se asemeja al comportamiento de TensorFlow/PyTorch en CPU; entender estos patrones de rendimiento facilita la transici√≥n.

El an√°lisis de rendimiento tambi√©n revela la importancia de la **vectorizaci√≥n**: reemplazar loops de Python con operaciones matriciales de NumPy puede producir aceleraciones de 100x o m√°s, ya que NumPy delega las operaciones a rutinas optimizadas en C/Fortran (BLAS/LAPACK).

### Comparaci√≥n de Implementaciones

**¬øQu√© m√©tricas comparamos?**
- **Tiempo de forward pass:** Cu√°nto tarda calcular la p√©rdida dado un conjunto de predicciones
- **Escalabilidad:** C√≥mo var√≠a el tiempo con el n√∫mero de muestras ($n$) y caracter√≠sticas ($d$)
- **Eficiencia de gradient descent:** Tiempo por √©poca para batch vs. mini-batch vs. SGD

```python
import time
import numpy as np

def benchmark_loss_functions(n_samples=10000):
    """
    Mide y compara el tiempo de ejecuci√≥n de cada funci√≥n de p√©rdida
    para un conjunto de datos de tama√±o n_samples.
    """
    np.random.seed(42)
    y_true = np.random.rand(n_samples)
    y_pred = np.random.rand(n_samples)

    funciones = {
        'MSE': lambda: mse(y_true, y_pred),
        'MAE': lambda: mae(y_true, y_pred),
        'BCE': lambda: binary_cross_entropy(y_true, y_pred),
    }

    print(f"{'Funci√≥n':<20} {'Tiempo (ms)':>15} {'Resultado':>15}")
    print("-" * 52)

    for nombre, fn in funciones.items():
        # Warm-up para evitar efectos de cach√© fr√≠a
        fn()
        # Medici√≥n con m√∫ltiples repeticiones para mayor precisi√≥n
        repeticiones = 100
        inicio = time.perf_counter()
        for _ in range(repeticiones):
            resultado = fn()
        fin = time.perf_counter()
        tiempo_ms = (fin - inicio) / repeticiones * 1000
        print(f"{nombre:<20} {tiempo_ms:>14.4f}ms {resultado:>15.6f}")


def benchmark_gradient_descent(n_samples=1000, n_features=10, epochs=50):
    """
    Compara el tiempo por √©poca de las tres variantes de gradient descent.
    """
    np.random.seed(42)
    X = np.random.randn(n_samples, n_features)
    y = np.random.randn(n_samples)

    variantes = {
        'Batch GD':      lambda: gradient_descent(X, y, epochs=epochs),
        'Mini-Batch GD': lambda: mini_batch_gd(X, y, batch_size=64, epochs=epochs),
        'SGD':           lambda: sgd(X, y, epochs=epochs),
    }

    print(f"\n{'Variante':<20} {'Tiempo total (s)':>18} {'Tiempo/√©poca (ms)':>20}")
    print("-" * 60)

    for nombre, fn in variantes.items():
        inicio = time.perf_counter()
        fn()
        fin = time.perf_counter()
        tiempo_total = fin - inicio
        tiempo_por_epoca_ms = (tiempo_total / epochs) * 1000
        print(f"{nombre:<20} {tiempo_total:>17.4f}s {tiempo_por_epoca_ms:>19.2f}ms")


def analizar_escalabilidad():
    """
    Analiza c√≥mo escala el tiempo de MSE con el tama√±o del dataset.
    """
    tamanios = [100, 1_000, 10_000, 100_000, 1_000_000]
    tiempos = []

    print(f"\n{'N muestras':<15} {'Tiempo MSE (ms)':>18}")
    print("-" * 35)

    for n in tamanios:
        y_true = np.random.rand(n)
        y_pred = np.random.rand(n)
        inicio = time.perf_counter()
        for _ in range(10):
            mse(y_true, y_pred)
        fin = time.perf_counter()
        t_ms = (fin - inicio) / 10 * 1000
        tiempos.append(t_ms)
        print(f"{n:<15,} {t_ms:>17.4f}ms")

    return tamanios, tiempos


# Ejecutar benchmarks
print("=" * 52)
print("BENCHMARK: FUNCIONES DE P√âRDIDA")
print("=" * 52)
benchmark_loss_functions()

print("\n" + "=" * 60)
print("BENCHMARK: VARIANTES DE GRADIENT DESCENT")
print("=" * 60)
benchmark_gradient_descent()

print("\n" + "=" * 35)
print("ESCALABILIDAD DE MSE")
print("=" * 35)
analizar_escalabilidad()
```

**¬øQu√© resultados debemos esperar?**

- **Funciones de p√©rdida:** MSE y MAE deber√≠an ser muy r√°pidas (~0.1-1 ms para 10k muestras). BCE ser√° ligeramente m√°s lenta por el c√°lculo de logaritmos.
- **Variantes de GD:** Batch GD tendr√° el tiempo por √©poca m√°s predecible. SGD ser√° el m√°s lento en tiempo total por los loops de Python. Mini-Batch GD ser√° el m√°s eficiente.
- **Escalabilidad:** MSE deber√≠a escalar aproximadamente de forma lineal con el n√∫mero de muestras (complejidad O(n)), lo que confirma la eficiencia de la vectorizaci√≥n de NumPy.

### Criterios de Comparaci√≥n

Al evaluar implementaciones, considera estos cuatro ejes:

| Criterio | ¬øQu√© medir? | ¬øCu√°ndo priorizar? |
|----------|-------------|-------------------|
| **Velocidad** | `time.perf_counter()`, repeticiones | Producci√≥n, datasets grandes |
| **Memoria** | Evitar copias innecesarias, in-place ops | Datasets que no caben en RAM |
| **Claridad** | ¬øSe entiende qu√© hace el c√≥digo? | Educaci√≥n, prototipado |
| **Mantenibilidad** | ¬øEs f√°cil modificar/extender? | Proyectos a largo plazo |

---

## üéØ EJERCICIOS PROPUESTOS

### Ejercicio 1: Implementar Huber Loss (B√°sico)

```python
Huber(y, ≈∑) = 0.5(y-≈∑)¬≤ si |y-≈∑| ‚â§ Œ¥
            = Œ¥|y-≈∑| - 0.5Œ¥¬≤ en otro caso
```

### Ejercicio 2: Learning Rate Scheduler (Intermedio)

Implementa:
```python
lr_new = lr_initial * decay_rate^epoch
```

Compara con LR fijo.

### Ejercicio 3: Early Stopping Mejorado (Intermedio)

- Monitorea p√©rdida de validaci√≥n
- Guarda mejor modelo
- Restaura al finalizar

### Ejercicio 4: Comparaci√≥n de Optimizadores (Avanzado)

Compara:
- Batch GD
- SGD
- Mini-batch GD
- Momentum (bonus)

### Ejercicio 5: Detecci√≥n Autom√°tica de Overfitting (Proyecto)

Sistema que:
- Detecta divergencia train/val
- Recomienda Œª de regularizaci√≥n
- Aplica early stopping autom√°ticamente

---

## üìù Entregables

### 1. C√≥digo Implementado (60%)

**Requisitos m√≠nimos:**
- Implementaciones completas y funcionales
- C√≥digo limpio y bien documentado
- Pruebas y validaci√≥n
- Manejo apropiado de errores

### 2. Notebook de Experimentaci√≥n (25%)

**Debe incluir:**
- Experimentos con diferentes configuraciones
- Visualizaciones claras
- An√°lisis de resultados
- Comparaciones y conclusiones

### 3. Reporte T√©cnico (15%)

**Secciones:**
1. Introducci√≥n y objetivos
2. Metodolog√≠a
3. Resultados experimentales
4. An√°lisis y discusi√≥n
5. Conclusiones

**Extensi√≥n:** 3-5 p√°ginas

### Formato de Entrega

```
Lab04_Entrega/
‚îú‚îÄ‚îÄ codigo/
‚îÇ   ‚îî‚îÄ‚îÄ [archivos .py]
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ experimentos.ipynb
‚îú‚îÄ‚îÄ reporte/
‚îÇ   ‚îî‚îÄ‚îÄ reporte_lab04.pdf
‚îî‚îÄ‚îÄ README.md
```

---

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Concebir (25%)

**Comprender el problema:**
- Identificar requisitos y restricciones
- Analizar alternativas de soluci√≥n
- Reconocer implicaciones de decisiones de dise√±o

### Dise√±ar (25%)

**Planificar soluciones:**
- Dise√±ar arquitecturas apropiadas
- Estructurar c√≥digo eficientemente
- Considerar escalabilidad y mantenibilidad

### Implementar (30%)

**Construcci√≥n:**
- C√≥digo funcional y correcto
- Implementaci√≥n eficiente
- Documentaci√≥n adecuada
- Pruebas comprehensivas

### Operar (20%)

**Validaci√≥n y an√°lisis:**
- Experimentaci√≥n sistem√°tica
- An√°lisis cr√≠tico de resultados
- Visualizaciones informativas
- Conclusiones fundamentadas

---

## üìä R√∫brica de Evaluaci√≥n

| Criterio | Excelente (90-100%) | Bueno (75-89%) | Satisfactorio (60-74%) | Insuficiente (<60%) |
|----------|-------------------|---------------|---------------------|-------------------|
| **Implementaci√≥n** | Impecable, eficiente, documentado | Funcional con docs | B√°sico funcional | Con errores |
| **Experimentaci√≥n** | An√°lisis profundo | Completo | B√°sico | Incompleto |
| **Documentaci√≥n** | Excelente | Buena | B√°sica | Pobre |
| **Comprensi√≥n** | Dominio total | Buen entendimiento | Comprensi√≥n b√°sica | Comprensi√≥n limitada |

---

## üìö Referencias Adicionales

### Libros

1. **"Deep Learning" - Goodfellow, Bengio, Courville**
   - Cap√≠tulos relevantes para este lab
   - www.deeplearningbook.org

2. **"Neural Networks and Deep Learning" - Michael Nielsen**
   - neuralnetworksanddeeplearning.com

### Recursos Online

1. **CS231n: Stanford**
   - http://cs231n.stanford.edu/

2. **3Blue1Brown: Neural Networks**
   - Videos educativos excelentes

3. **TensorFlow Playground**
   - https://playground.tensorflow.org/

### Documentaci√≥n

- NumPy: https://numpy.org/doc/
- Matplotlib: https://matplotlib.org/
- Python: https://docs.python.org/3/

---

## üéì Notas Finales

### Conceptos Clave para Recordar

1. **P√©rdida cuantifica error:** Entre predicciones y realidad
2. **MSE para regresi√≥n:** Penaliza errores grandes
3. **Cross-Entropy para clasificaci√≥n:** Interpretaci√≥n probabil√≠stica
4. **Gradient descent minimiza:** Ajusta par√°metros iterativamente
5. **Learning rate crucial:** Balance velocidad/estabilidad
6. **Mini-batch es est√°ndar:** Mejor que batch o SGD puro
7. **Overfitting es com√∫n:** Entrenar mucho en pocos datos
8. **Regularizaci√≥n ayuda:** L2, L1, dropout, early stopping

### Preparaci√≥n para el Siguiente Lab

**Lab 05: Backpropagation**

Aprender√°s:
- Chain rule para redes
- Grafos computacionales
- C√°lculo eficiente de gradientes
- Implementaci√≥n completa de backprop

Prep√°rate repasando c√°lculo y chain rule.

### Consejos de Estudio

1. **Implementa desde cero** - No uses frameworks todav√≠a
2. **Visualiza** - Dibuja y grafica para entender
3. **Experimenta** - Prueba diferentes configuraciones
4. **Debug sistem√°ticamente** - Verifica paso a paso
5. **Documenta** - Anota hallazgos y experimentos

### Soluci√≥n de Problemas Comunes

**Errores de dimensiones:**
- Verifica shape de todas las matrices
- Usa print(variable.shape) liberalmente

**Resultados inesperados:**
- Verifica inicializaci√≥n
- Asegura reproducibilidad con seed
- Revisa cada paso del c√°lculo

**C√≥digo lento:**
- Usa vectorizaci√≥n de NumPy
- Evita loops innecesarios
- Procesa en batches

### Certificaci√≥n de Completitud

Has completado exitosamente Lab 04 cuando puedas:

- [ ] Comprender qu√© son funciones de p√©rdida y su prop√≥sito
- [ ] Implementar MSE, MAE, Cross-Entropy desde cero
- [ ] Elegir funci√≥n de p√©rdida apropiada para cada problema
- [ ] Implementar gradient descent b√°sico
- [ ] Comprender efecto del learning rate
- [ ] Reconocer y detectar overfitting
- [ ] Calcular derivadas de funciones de p√©rdida
- [ ] Combinar p√©rdida con activaci√≥n eficientemente
- [ ] Aplicar regularizaci√≥n b√°sica

**¬°Felicitaciones!** Contin√∫a con el siguiente laboratorio.

---

**¬øPreguntas?** Revisa teor√≠a, experimenta, y consulta referencias.

**¬°√âxito en tu aprendizaje! üöÄ**
