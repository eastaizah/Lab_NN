{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04: Funciones de P√©rdida y Optimizaci√≥n\n",
    "\n",
    "## Objetivos\n",
    "1. Implementar funciones de p√©rdida desde cero\n",
    "2. Comparar MSE, MAE y Cross-Entropy\n",
    "3. Implementar Gradient Descent\n",
    "4. Experimentar con learning rates\n",
    "5. Detectar overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('codigo/')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Funciones de P√©rdida para Regresi√≥n\n",
    "\n",
    "### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Mean Squared Error\n",
    "    MSE = (1/n) * Œ£(y_pred - y_true)¬≤\n",
    "    \"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def mse_gradient(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Gradiente de MSE\n",
    "    ‚àÇMSE/‚àÇy_pred = 2(y_pred - y_true) / n\n",
    "    \"\"\"\n",
    "    n = y_true.shape[0]\n",
    "    return 2 * (y_pred - y_true) / n\n",
    "\n",
    "# Probar\n",
    "y_true = np.array([1, 2, 3, 4, 5])\n",
    "y_pred = np.array([1.1, 2.3, 2.8, 4.2, 4.9])\n",
    "\n",
    "print(f\"MSE: {mse(y_pred, y_true):.4f}\")\n",
    "print(f\"Gradiente: {mse_gradient(y_pred, y_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error\n",
    "    MAE = (1/n) * Œ£|y_pred - y_true|\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(y_pred - y_true))\n",
    "\n",
    "# Probar\n",
    "print(f\"MAE: {mae(y_pred, y_true):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaci√≥n: MSE vs MAE con Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos sin outlier\n",
    "y_true_clean = np.array([1, 2, 3, 4, 5])\n",
    "y_pred_clean = np.array([1.1, 2.2, 2.9, 4.1, 5.0])\n",
    "\n",
    "# Datos con outlier\n",
    "y_true_outlier = y_true_clean.copy()\n",
    "y_pred_outlier = y_pred_clean.copy()\n",
    "y_pred_outlier[2] = 10  # Outlier!\n",
    "\n",
    "print(\"SIN OUTLIER:\")\n",
    "print(f\"  MSE: {mse(y_pred_clean, y_true_clean):.4f}\")\n",
    "print(f\"  MAE: {mae(y_pred_clean, y_true_clean):.4f}\")\n",
    "\n",
    "print(\"\\nCON OUTLIER:\")\n",
    "print(f\"  MSE: {mse(y_pred_outlier, y_true_outlier):.4f}\")\n",
    "print(f\"  MAE: {mae(y_pred_outlier, y_true_outlier):.4f}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è MSE aumenta dram√°ticamente con el outlier debido al t√©rmino cuadr√°tico.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Funciones de P√©rdida para Clasificaci√≥n\n",
    "\n",
    "### Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy\n",
    "    BCE = -[y*log(≈∑) + (1-y)*log(1-≈∑)]\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Ejemplos\n",
    "print(\"EJEMPLOS DE BINARY CROSS-ENTROPY:\\n\")\n",
    "\n",
    "# Caso 1: Predicci√≥n correcta con alta confianza\n",
    "print(\"1. Clase real=1, Predicci√≥n=0.9 (correcto, alta confianza)\")\n",
    "print(f\"   BCE: {binary_crossentropy(np.array([0.9]), np.array([1])):.4f} (baja p√©rdida)\\n\")\n",
    "\n",
    "# Caso 2: Predicci√≥n incorrecta con alta confianza\n",
    "print(\"2. Clase real=1, Predicci√≥n=0.1 (incorrecto, alta confianza)\")\n",
    "print(f\"   BCE: {binary_crossentropy(np.array([0.1]), np.array([1])):.4f} (alta p√©rdida)\\n\")\n",
    "\n",
    "# Caso 3: Predicci√≥n incierta\n",
    "print(\"3. Clase real=1, Predicci√≥n=0.5 (incierto)\")\n",
    "print(f\"   BCE: {binary_crossentropy(np.array([0.5]), np.array([1])):.4f} (p√©rdida media)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizaci√≥n de Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_range = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# P√©rdida cuando y_true = 1\n",
    "loss_class_1 = [-np.log(p) for p in y_pred_range]\n",
    "\n",
    "# P√©rdida cuando y_true = 0\n",
    "loss_class_0 = [-np.log(1 - p) for p in y_pred_range]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_pred_range, loss_class_1, label='Clase Real = 1', linewidth=2)\n",
    "plt.plot(y_pred_range, loss_class_0, label='Clase Real = 0', linewidth=2)\n",
    "plt.xlabel('Probabilidad Predicha', fontsize=12)\n",
    "plt.ylabel('P√©rdida (Binary Cross-Entropy)', fontsize=12)\n",
    "plt.title('Binary Cross-Entropy vs Predicci√≥n', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 5)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observaciones:\")\n",
    "print(\"- Cuando y=1: p√©rdida baja si predicci√≥n cercana a 1\")\n",
    "print(\"- Cuando y=0: p√©rdida baja si predicci√≥n cercana a 0\")\n",
    "print(\"- Predicciones confiadas pero incorrectas tienen p√©rdida muy alta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Gradient Descent\n",
    "\n",
    "Implementemos gradient descent para minimizar una funci√≥n simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion_objetivo(x):\n",
    "    \"\"\"Funci√≥n a minimizar: f(x) = x¬≤ - 4x + 4 = (x-2)¬≤\"\"\"\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "def gradiente(x):\n",
    "    \"\"\"Gradiente de f(x)\"\"\"\n",
    "    return 2 * (x - 2)\n",
    "\n",
    "def gradient_descent(x_init, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Implementaci√≥n b√°sica de Gradient Descent\n",
    "    \"\"\"\n",
    "    x = x_init\n",
    "    history = [x]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calcular gradiente\n",
    "        grad = gradiente(x)\n",
    "        \n",
    "        # Actualizar par√°metro\n",
    "        x = x - learning_rate * grad\n",
    "        \n",
    "        history.append(x)\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Ejecutar con diferentes learning rates\n",
    "x_init = 5.0\n",
    "iterations = 15\n",
    "\n",
    "lrs = [0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "for idx, lr in enumerate(lrs):\n",
    "    x_final, history = gradient_descent(x_init, lr, iterations)\n",
    "    \n",
    "    plt.subplot(1, 4, idx + 1)\n",
    "    \n",
    "    # Graficar funci√≥n\n",
    "    x_range = np.linspace(-1, 6, 100)\n",
    "    y_range = [funcion_objetivo(x) for x in x_range]\n",
    "    plt.plot(x_range, y_range, 'b-', alpha=0.3, linewidth=2)\n",
    "    \n",
    "    # Graficar trayectoria de GD\n",
    "    y_history = [funcion_objetivo(x) for x in history]\n",
    "    plt.plot(history, y_history, 'ro-', markersize=4, linewidth=1)\n",
    "    \n",
    "    # Marcar m√≠nimo\n",
    "    plt.plot(2, 0, 'g*', markersize=15, label='M√≠nimo')\n",
    "    \n",
    "    plt.title(f'LR = {lr}', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-1, 12)\n",
    "    \n",
    "    print(f\"Learning Rate {lr}:\")\n",
    "    print(f\"  Inicio: x = {x_init}\")\n",
    "    print(f\"  Final: x = {x_final:.4f}\")\n",
    "    print(f\"  Error: {abs(x_final - 2):.4f}\\n\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Observaciones:\")\n",
    "print(\"- LR muy peque√±o (0.01): Convergencia lenta\")\n",
    "print(\"- LR moderado (0.1, 0.5): Buena convergencia\")\n",
    "print(\"- LR muy grande (1.0): Puede oscilar o diverger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Entrenamiento con Gradient Descent\n",
    "\n",
    "Apliquemos GD a un problema real de regresi√≥n lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos sint√©ticos\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1) * 0.5\n",
    "\n",
    "# Visualizar datos\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X, y, alpha=0.6)\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Datos de Entrenamiento', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"N√∫mero de muestras: {len(X)}\")\n",
    "print(f\"Relaci√≥n real: y = 4 + 3*X + ruido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar par√°metros\n",
    "theta = np.random.randn(2, 1)  # [bias, weight]\n",
    "X_b = np.c_[np.ones((len(X), 1)), X]  # A√±adir columna de 1s para el bias\n",
    "\n",
    "# Hiperpar√°metros\n",
    "learning_rate = 0.1\n",
    "n_iterations = 100\n",
    "\n",
    "# Historia\n",
    "loss_history = []\n",
    "\n",
    "# Gradient Descent\n",
    "for iteration in range(n_iterations):\n",
    "    # Predicciones\n",
    "    y_pred = X_b.dot(theta)\n",
    "    \n",
    "    # Calcular p√©rdida (MSE)\n",
    "    loss = mse(y_pred, y)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Calcular gradiente\n",
    "    gradients = 2/len(X) * X_b.T.dot(y_pred - y)\n",
    "    \n",
    "    # Actualizar par√°metros\n",
    "    theta = theta - learning_rate * gradients\n",
    "    \n",
    "    if iteration % 20 == 0:\n",
    "        print(f\"Iteraci√≥n {iteration}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nPar√°metros finales:\")\n",
    "print(f\"  Bias (Œ∏0): {theta[0][0]:.4f} (esperado: ~4)\")\n",
    "print(f\"  Weight (Œ∏1): {theta[1][0]:.4f} (esperado: ~3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ajuste del modelo\n",
    "ax1.scatter(X, y, alpha=0.6, label='Datos')\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta)\n",
    "ax1.plot(X_new, y_predict, 'r-', linewidth=2, label='Modelo')\n",
    "ax1.set_xlabel('X', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_title('Regresi√≥n Lineal con Gradient Descent', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Curva de aprendizaje\n",
    "ax2.plot(loss_history, linewidth=2)\n",
    "ax2.set_xlabel('Iteraci√≥n', fontsize=12)\n",
    "ax2.set_ylabel('MSE Loss', fontsize=12)\n",
    "ax2.set_title('Curva de Aprendizaje', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì El modelo converge correctamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Overfitting\n",
    "\n",
    "Demostremos el concepto de overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos\n",
    "np.random.seed(42)\n",
    "n = 15\n",
    "X_train = np.linspace(0, 3, n)\n",
    "y_train = 2 * np.sin(X_train) + np.random.randn(n) * 0.3\n",
    "\n",
    "# Crear conjunto de test m√°s grande\n",
    "X_test = np.linspace(0, 3, 100)\n",
    "y_test = 2 * np.sin(X_test)\n",
    "\n",
    "# Ajustar polinomios de diferentes grados\n",
    "degrees = [1, 3, 10]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    \n",
    "    # Ajustar polinomio\n",
    "    coeffs = np.polyfit(X_train, y_train, degree)\n",
    "    poly = np.poly1d(coeffs)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_train_pred = poly(X_train)\n",
    "    y_test_pred = poly(X_test)\n",
    "    \n",
    "    # Calcular errores\n",
    "    train_mse = mse(y_train_pred, y_train)\n",
    "    test_mse = mse(y_test_pred, y_test)\n",
    "    \n",
    "    # Graficar\n",
    "    plt.scatter(X_train, y_train, s=50, alpha=0.7, label='Train')\n",
    "    plt.plot(X_test, y_test, 'g--', alpha=0.5, label='Verdadero')\n",
    "    plt.plot(X_test, y_test_pred, 'r-', linewidth=2, label='Modelo')\n",
    "    \n",
    "    plt.xlabel('X', fontsize=11)\n",
    "    plt.ylabel('y', fontsize=11)\n",
    "    \n",
    "    status = \"Underfitting\" if degree == 1 else (\"Good fit\" if degree == 3 else \"Overfitting\")\n",
    "    plt.title(f'Grado {degree} - {status}\\nTrain MSE={train_mse:.3f}, Test MSE={test_mse:.3f}', \n",
    "             fontsize=11, fontweight='bold')\n",
    "    plt.legend(fontsize=9)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ö†Ô∏è Observaciones:\")\n",
    "print(\"- Grado 1: Modelo muy simple, no captura la complejidad (underfitting)\")\n",
    "print(\"- Grado 3: Balance adecuado\")\n",
    "print(\"- Grado 10: Modelo muy complejo, memoriza el ruido (overfitting)\")\n",
    "print(\"\\n  En overfitting: Train MSE bajo, pero Test MSE alto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desaf√≠o: Mini-batch Gradient Descent\n",
    "\n",
    "Implementa mini-batch gradient descent y comp√°ralo con batch GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X, y, learning_rate, n_epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Mini-batch Gradient Descent\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    theta = np.random.randn(2, 1)\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Mezclar datos\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Dividir en mini-batches\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Gradiente en el batch\n",
    "            y_pred = X_batch.dot(theta)\n",
    "            gradients = 2/len(X_batch) * X_batch.T.dot(y_pred - y_batch)\n",
    "            theta = theta - learning_rate * gradients\n",
    "        \n",
    "        # Calcular p√©rdida en todos los datos\n",
    "        y_pred_all = X.dot(theta)\n",
    "        loss = mse(y_pred_all, y)\n",
    "        loss_history.append(loss)\n",
    "    \n",
    "    return theta, loss_history\n",
    "\n",
    "# Comparar batch sizes\n",
    "batch_sizes = [len(X_b), 32, 1]  # Batch, Mini-batch, Stochastic\n",
    "labels = ['Batch GD', 'Mini-batch (32)', 'SGD (1)']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for batch_size, label in zip(batch_sizes, labels):\n",
    "    theta, loss_hist = minibatch_gradient_descent(X_b, y, 0.1, 50, batch_size)\n",
    "    plt.plot(loss_hist, label=label, linewidth=2)\n",
    "\n",
    "plt.xlabel('√âpoca', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('Comparaci√≥n de Variantes de Gradient Descent', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Mini-batch GD balancea velocidad y estabilidad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "### Funciones de P√©rdida:\n",
    "- **MSE**: Regresi√≥n, sensible a outliers\n",
    "- **MAE**: Regresi√≥n, robusta a outliers\n",
    "- **Binary CE**: Clasificaci√≥n binaria\n",
    "- **Categorical CE**: Clasificaci√≥n multiclase\n",
    "\n",
    "### Gradient Descent:\n",
    "- Minimiza la p√©rdida iterativamente\n",
    "- Learning rate es crucial\n",
    "- Mini-batch es el est√°ndar\n",
    "\n",
    "### Overfitting:\n",
    "- Modelo muy complejo\n",
    "- Memoriza en lugar de aprender\n",
    "- Detectar: comparar train vs test loss\n",
    "\n",
    "**Pr√≥ximo paso**: En Lab 05 aprenderemos Backpropagation, el algoritmo que hace posible calcular gradientes en redes profundas.\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Excelente trabajo! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
