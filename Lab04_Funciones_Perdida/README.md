# Lab 04: Funciones de P√©rdida y Optimizaci√≥n

## Objetivos de Aprendizaje

Al completar este laboratorio, ser√°s capaz de:

1. Comprender qu√© son las funciones de p√©rdida y por qu√© son necesarias
2. Implementar desde cero MSE, MAE, Binary Cross-Entropy y Categorical Cross-Entropy
3. Entender las diferencias entre p√©rdidas para regresi√≥n y clasificaci√≥n
4. Implementar gradient descent b√°sico
5. Reconocer overfitting y sus causas
6. Elegir la funci√≥n de p√©rdida apropiada para diferentes problemas

## Estructura del Laboratorio

```
Lab04_Funciones_Perdida/
‚îú‚îÄ‚îÄ README.md                # Esta gu√≠a
‚îú‚îÄ‚îÄ teoria.md               # Fundamentos te√≥ricos
‚îú‚îÄ‚îÄ practica.ipynb         # Notebook interactivo
‚îî‚îÄ‚îÄ codigo/
    ‚îî‚îÄ‚îÄ perdida.py         # Implementaciones completas
```

## Requisitos Previos

- Completar Labs 01-03
- Comprensi√≥n de funciones de activaci√≥n
- Conocimientos b√°sicos de c√°lculo (derivadas)

## Contenido Te√≥rico

El archivo `teoria.md` cubre:

- **Funci√≥n de p√©rdida**: Qu√© es y por qu√© la necesitamos
- **MSE y MAE**: Para problemas de regresi√≥n
- **Cross-Entropy**: Para clasificaci√≥n binaria y multiclase
- **Gradient Descent**: Algoritmo de optimizaci√≥n fundamental
- **Learning Rate**: Su importancia y c√≥mo elegirlo
- **Overfitting**: Qu√© es y c√≥mo detectarlo
- **Regularizaci√≥n**: T√©cnicas b√°sicas

## Pr√°ctica

### Parte 1: Ejecutar C√≥digo Principal (20 min)

```bash
cd codigo/
python perdida.py
```

Esto generar√°:
- Comparaci√≥n MSE vs MAE con outliers
- Visualizaci√≥n de Binary Cross-Entropy
- Demostraci√≥n de Gradient Descent con diferentes learning rates
- Ejemplo de overfitting

### Parte 2: Notebook Interactivo (60 min)

Abre `practica.ipynb` y completa los ejercicios:

```bash
jupyter notebook practica.ipynb
```

Incluye:
1. Implementaci√≥n de funciones de p√©rdida
2. Comparaci√≥n de p√©rdidas en diferentes escenarios
3. Gradient descent paso a paso
4. Experimentos con learning rates
5. Detecci√≥n de overfitting

### Parte 3: Experimentos (30 min)

1. **Experimento 1**: Comparar MSE vs MAE con diferentes niveles de outliers
2. **Experimento 2**: Probar gradient descent con learning rates extremos
3. **Experimento 3**: Entrenar un modelo hasta que ocurra overfitting

## Conceptos Clave

### 1. Elecci√≥n de Funci√≥n de P√©rdida

```
Tipo de Problema          | Funci√≥n de P√©rdida
------------------------- | ------------------------
Regresi√≥n                 | MSE, MAE
Clasificaci√≥n Binaria     | Binary Cross-Entropy
Clasificaci√≥n Multiclase  | Categorical Cross-Entropy
```

### 2. Combinaciones Ideales

| Problema | Activaci√≥n | P√©rdida | Por qu√© |
|----------|-----------|---------|---------|
| Regresi√≥n | Lineal | MSE | Natural para valores continuos |
| Binaria | Sigmoid | Binary CE | Derivada simple |
| Multiclase | Softmax | Categorical CE | Derivada simple |

### 3. Learning Rate

```python
Œ± muy peque√±o (0.0001):  Lento pero estable
Œ± moderado (0.01):       Balance ideal (usual)
Œ± muy grande (1.0):      R√°pido pero inestable
```

## Ejercicios

### Ejercicio 1: Implementar Huber Loss

Combina MSE y MAE:
```python
Huber(y, ≈∑) = 0.5 * (y - ≈∑)¬≤     si |y - ≈∑| ‚â§ Œ¥
             = Œ¥ * |y - ≈∑| - 0.5Œ¥¬≤  en otro caso
```

### Ejercicio 2: Learning Rate Adaptativo

Implementa un scheduler que reduce el learning rate:
```python
lr_new = lr_initial * 0.95^epoch
```

### Ejercicio 3: Early Stopping

Implementa early stopping para prevenir overfitting:
- Monitorea p√©rdida de validaci√≥n
- Det√©n si no mejora en N √©pocas

### Ejercicio 4: Comparaci√≥n de Optimizadores

Compara:
- Batch Gradient Descent
- Stochastic Gradient Descent
- Mini-batch Gradient Descent

## Preguntas de Reflexi√≥n

1. **¬øPor qu√© MSE penaliza m√°s los errores grandes?**
   
   Pista: Piensa en el t√©rmino cuadr√°tico.

2. **¬øPor qu√© usamos Cross-Entropy en lugar de MSE para clasificaci√≥n?**
   
   Pista: Considera la interpretaci√≥n probabil√≠stica.

3. **¬øQu√© pasa si el learning rate es demasiado grande?**
   
   Pista: Piensa en t√©rminos de convergencia.

4. **¬øC√≥mo detectas overfitting?**
   
   Pista: Compara p√©rdida de train vs validaci√≥n.

## Verificaci√≥n de Comprensi√≥n

Despu√©s de completar el laboratorio, deber√≠as poder:

- [ ] Explicar qu√© mide una funci√≥n de p√©rdida
- [ ] Implementar MSE, MAE y Cross-Entropy desde cero
- [ ] Elegir la p√©rdida correcta para un problema dado
- [ ] Implementar gradient descent b√°sico
- [ ] Comprender el efecto del learning rate
- [ ] Identificar overfitting en gr√°ficas de entrenamiento
- [ ] Calcular derivadas de funciones de p√©rdida

## Errores Comunes

### Error 1: Usar MSE para Clasificaci√≥n

**Problema**: MSE no es adecuada para clasificaci√≥n

**Soluci√≥n**: Usar Binary o Categorical Cross-Entropy

**Por qu√©**: Cross-Entropy tiene mejor interpretaci√≥n probabil√≠stica

### Error 2: Learning Rate Muy Grande

**S√≠ntoma**: P√©rdida oscila o diverge

**Soluci√≥n**: Reducir learning rate en √≥rdenes de magnitud

**T√≠pico**: Probar 0.1, 0.01, 0.001

### Error 3: No Normalizar Datos

**S√≠ntoma**: Convergencia lenta o inestable

**Soluci√≥n**: Normalizar/estandarizar entradas

```python
X = (X - mean) / std
```

### Error 4: Confundir P√©rdida y M√©trica

**Recordar**:
- **P√©rdida**: Lo que optimizamos (ej: Cross-Entropy)
- **M√©trica**: Lo que reportamos (ej: Accuracy)

## Visualizaciones Importantes

### 1. Curva de Aprendizaje

```
P√©rdida |
        |  \
        |   \_____ convergencia
        |
        +----------------> √âpocas
```

### 2. Overfitting

```
P√©rdida |  train \___
        |              
        |  val    /‚Äæ‚Äæ‚Äæ
        +----------------> √âpocas
```

### 3. Learning Rate

```
P√©rdida |     lr grande \/\/\/
        |     
        |     lr peque√±o \____
        +---------------------  ‚Äæ> √âpocas
```

## Recursos Adicionales

### Lecturas

1. **Loss Functions**: Deep Learning Book, Chapter 5
2. **Optimization**: Deep Learning Book, Chapter 8
3. **Cross-Entropy**: "Pattern Recognition and Machine Learning" (Bishop)

### Papers

- "Adam: A Method for Stochastic Optimization" (Kingma & Ba, 2014)
- "On the importance of initialization and momentum in deep learning" (Sutskever et al., 2013)

### Herramientas Interactivas

- [Loss Landscape Visualization](https://losslandscape.com)
- [TensorFlow Playground](https://playground.tensorflow.org)

## Soluci√≥n de Problemas

### P√©rdida no disminuye

**Posibles causas**:
1. Learning rate muy peque√±o ‚Üí Aumentar
2. Inicializaci√≥n mala ‚Üí Re-inicializar
3. Datos no normalizados ‚Üí Normalizar
4. Arquitectura inadecuada ‚Üí Revisar modelo

### P√©rdida es NaN

**Posibles causas**:
1. Learning rate muy grande ‚Üí Reducir
2. Overflow en exp() ‚Üí Usar estabilizaci√≥n num√©rica
3. Divisi√≥n por cero ‚Üí A√±adir epsilon

**Soluci√≥n**:
```python
# Softmax estable
exp_x = np.exp(x - np.max(x))

# Evitar log(0)
epsilon = 1e-15
loss = -np.log(y_pred + epsilon)
```

### Convergencia lenta

**Soluciones**:
1. Aumentar learning rate
2. Usar momentum (lab posterior)
3. Mejor inicializaci√≥n
4. Normalizar datos

## Pr√≥ximo Laboratorio

En **Lab 05: Backpropagation**, aprenderemos:
- Chain rule para redes neuronales
- Grafos computacionales
- Implementaci√≥n completa de backpropagation
- C√°lculo eficiente de gradientes

Backpropagation es el algoritmo que hace posible el entrenamiento de redes profundas al calcular los gradientes necesarios para gradient descent.

## Notas Finales

Las funciones de p√©rdida son fundamentales porque:
1. Cuantifican el error del modelo
2. Gu√≠an la optimizaci√≥n
3. Permiten comparar modelos

La elecci√≥n correcta de la funci√≥n de p√©rdida puede hacer la diferencia entre un modelo que funciona y uno que no.

**Recuerda**: 
- MSE para regresi√≥n
- Cross-Entropy para clasificaci√≥n
- Learning rate: empieza con 0.01
- Monitorea overfitting siempre

---

**¬øPreguntas?** Revisa la teor√≠a, experimenta con los notebooks, y recuerda: ¬°la pr√°ctica hace al maestro!

**¬°√âxito! üéØ**
