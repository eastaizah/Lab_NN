# GuÃ­a de Laboratorio: Transformers y Mecanismos de Auto-AtenciÃ³n
## ğŸ“‹ InformaciÃ³n del Laboratorio
**TÃ­tulo:** Transformers, Self-Attention y Modelos de Lenguaje Pre-entrenados  
**CÃ³digo:** Lab 12  
**DuraciÃ³n:** 4-5 horas  
**Nivel:** Avanzado  
## ğŸ¯ Objetivos EspecÃ­ficos
Al completar este laboratorio, serÃ¡s capaz de:
1. Comprender y derivar el mecanismo de **Self-Attention** con matrices Query, Key y Value
2. Implementar **Scaled Dot-Product Attention** desde cero con NumPy
3. Construir **Multi-Head Attention** con mÃºltiples cabezas en paralelo
4. Aplicar **Positional Encoding** sinusoidal para codificar informaciÃ³n de posiciÃ³n
5. Ensamblar un **Transformer Encoder Block** completo con PyTorch
6. Aplicar **fine-tuning** de BERT para tareas de clasificaciÃ³n de sentimientos
7. Generar texto de forma autoregresiva con **GPT-2**
8. Visualizar e interpretar **mapas de atenciÃ³n**
9. Comparar la complejidad computacional de Transformers vs. RNNs/LSTMs
10. Reconocer el ecosistema de **modelos pre-entrenados** y cuÃ¡ndo utilizarlos
## ğŸ“š Prerrequisitos
### Conocimientos
- Python intermedio-avanzado (clases, decoradores, comprensiÃ³n de listas)
- Ãlgebra lineal (multiplicaciÃ³n matricial, transpuesta, softmax)
- Redes neuronales feedforward (Lab 01â€“06)
- Frameworks de Deep Learning â€” PyTorch (Lab 08)
- Redes Recurrentes y LSTMs (Lab 11) â€” comparativa clave
- Conceptos de NLP: tokens, embeddings, vocabulario
### Software
- Python 3.8+
- NumPy 1.21+
- PyTorch 1.12+
- Matplotlib 3.4+
- Transformers (Hugging Face) 4.20+
- Datasets (Hugging Face) 2.0+
- Jupyter Notebook (recomendado)
```bash
pip install numpy matplotlib torch transformers datasets sentencepiece
```
### Material de Lectura
Antes de comenzar, lee:
- `teoria.md` â€” Marco teÃ³rico completo sobre Transformers y Self-Attention
- `README.md` â€” Estructura del laboratorio y recursos disponibles
- **Vaswani et al. (2017)** â€” "Attention Is All You Need" (abstract y figuras 1-2)
## ğŸ“– IntroducciÃ³n
Los **Transformers** representan el avance mÃ¡s significativo en Deep Learning de la Ãºltima dÃ©cada. Introducidos en 2017 con el paper "Attention Is All You Need" (Vaswani et al.), reemplazaron a las redes recurrentes en prÃ¡cticamente todas las tareas de procesamiento de lenguaje natural y estÃ¡n expandiÃ©ndose a visiÃ³n computacional, audio, bioinformÃ¡tica y mÃ¡s. Modelos como GPT-4, BERT, DALL-E, Whisper y AlphaFold estÃ¡n todos construidos sobre esta arquitectura.
### Contexto del Problema: Las Limitaciones de las RNNs
En el Lab 11 trabajaste con RNNs y LSTMs. Estas arquitecturas procesan secuencias **token por token**, lo que genera tres problemas fundamentales:
1. **Procesamiento secuencial**: No es posible paralelizar â€” el token en la posiciÃ³n *t* depende del estado oculto de la posiciÃ³n *t-1*. Esto hace el entrenamiento lento.
2. **Cuello de botella de informaciÃ³n**: Toda la informaciÃ³n de una secuencia larga debe comprimirse en un Ãºnico vector de estado oculto de dimensiÃ³n fija.
3. **Gradientes que desaparecen o explotan**: Aunque las LSTMs mitigan este problema con compuertas, no lo eliminan completamente en secuencias muy largas (> 500 tokens).
```
RNN/LSTM (secuencial â€” lento):
xâ‚ â†’ [hâ‚] â†’ xâ‚‚ â†’ [hâ‚‚] â†’ xâ‚ƒ â†’ [hâ‚ƒ] â†’ ... â†’ xâ‚™ â†’ [hâ‚™] â†’ salida

Transformer (paralelo â€” rÃ¡pido):
xâ‚ â”€â”
xâ‚‚ â”€â”¤â”€â”€> [Self-Attention] â”€â”€> [FFN] â”€â”€> salidaâ‚, salidaâ‚‚, ..., salida_n
xâ‚ƒ â”€â”¤        (todos al mismo tiempo)
xâ‚™ â”€â”˜
```
### La SoluciÃ³n: Mecanismo de AtenciÃ³n
La idea clave de los Transformers es el **mecanismo de atenciÃ³n**: en lugar de pasar informaciÃ³n a travÃ©s de estados ocultos secuenciales, cada posiciÃ³n de la secuencia puede "atender" directamente a **cualquier otra posiciÃ³n** con un coste O(1) en profundidad.
**AnalogÃ­a de bÃºsqueda en base de datos:**
Imagina que tienes una base de datos con entradas (Key â†’ Value). Cuando lanzas una consulta (Query), obtienes como resultado una combinaciÃ³n ponderada de todos los valores, donde el peso de cada uno depende de cuÃ¡n compatible es tu consulta con esa clave.
```
Query: "Â¿quiÃ©n estÃ¡ hambriento?"
Keys:  ["gato", "leche", "bowl"]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Score("gato")  = 0.85  â† Alta compatibilidad
Score("leche") = 0.10
Score("bowl")  = 0.05
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AtenciÃ³n â‰ˆ 0.85 Ã— V("gato") + 0.10 Ã— V("leche") + 0.05 Ã— V("bowl")
```
### Enfoque con Transformers
La arquitectura Transformer reemplaza la recurrencia con tres componentes clave:
```
INPUT TOKENS (xâ‚, xâ‚‚, ..., xâ‚™)
        â†“
TOKEN EMBEDDINGS + POSITIONAL ENCODING
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     TRANSFORMER ENCODER BLOCK        â”‚  Ã— N capas
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Multi-Head Self-Attention      â”‚ â”‚
â”‚  â”‚  (Q, K, V desde la misma seq.)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        Add & LayerNorm               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Feed-Forward Network (FFN)     â”‚ â”‚
â”‚  â”‚  (proyecciÃ³n lineal Ã— 2 + ReLU) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        Add & LayerNorm               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
REPRESENTACIONES CONTEXTUALES
```
### Conceptos Fundamentales
**1. Scaled Dot-Product Attention:**
$$\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$
Donde:
- **Q** (Queries): lo que queremos buscar â€” forma `(seq_len, d_k)`
- **K** (Keys): lo que se puede encontrar â€” forma `(seq_len, d_k)`
- **V** (Values): contenido a recuperar â€” forma `(seq_len, d_v)`
- **âˆšd_k**: factor de escala para estabilizar gradientes
**2. Multi-Head Attention:**
$$\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\ldots,\text{head}_h)\,W^O$$
$$\text{head}_i = \text{Attention}(QW_i^Q,\, KW_i^K,\, VW_i^V)$$
Cada cabeza aprende a atender diferentes tipos de relaciones (sintÃ¡cticas, semÃ¡nticas, de posiciÃ³n, etc.).
**3. Positional Encoding Sinusoidal:**
$$\text{PE}(\text{pos},\, 2i) = \sin\!\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)$$
$$\text{PE}(\text{pos},\, 2i+1) = \cos\!\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)$$
### Aplicaciones PrÃ¡cticas
Los Transformers son la base de la IA moderna:
- **NLP**: GPT-4 (generaciÃ³n), BERT (comprensiÃ³n), T5 (texto-a-texto), Llama (open-source)
- **VisiÃ³n**: ViT, DINO, Segment Anything Model (SAM) â€” tratan parches de imagen como tokens
- **Audio**: Whisper (reconocimiento de voz), AudioLM, MusicGen
- **Ciencia**: AlphaFold2 (plegamiento de proteÃ­nas), ESMFold, modelos de diseÃ±o de fÃ¡rmacos
- **Multimodal**: CLIP, DALL-E 3, GPT-4V, Gemini
### MotivaciÃ³n HistÃ³rica
La secuencia de hitos que llevÃ³ a los Transformers modernos:
- **1986** â€” Backpropagation (Rumelhart et al.)
- **1997** â€” LSTM (Hochreiter & Schmidhuber)
- **2014** â€” Mecanismo de atenciÃ³n para traducciÃ³n (Bahdanau et al.)
- **2017** â€” "Attention Is All You Need" â€” el Transformer original (Vaswani et al.)
- **2018** â€” BERT (Google) y GPT (OpenAI) â€” pre-entrenamiento masivo
- **2020** â€” GPT-3 (175B parÃ¡metros) â€” few-shot learning emergente
- **2022** â€” ChatGPT â€” RLHF aplicado a GPT
- **2023+** â€” GPT-4, Llama 2/3, Gemini, Claude 3 â€” era de los LLMs
## ğŸ”¬ Parte 1: Self-Attention desde Cero con NumPy (45 min)
### 1.1 IntroducciÃ³n Conceptual
El mecanismo de Self-Attention permite a cada token de una secuencia calcular su representaciÃ³n como una suma ponderada de **todos los otros tokens** (incluido Ã©l mismo). La intuiciÃ³n es que el significado de una palabra depende de su contexto.
**Ejemplo lingÃ¼Ã­stico:**
```
"El banco estaba lleno de peces"  â†’  "banco" debe atender a "peces"
"El banco rechazÃ³ mi prÃ©stamo"    â†’  "banco" debe atender a "prÃ©stamo"
```
Self-Attention resuelve esta ambigÃ¼edad contextualmente.
### 1.2 ImplementaciÃ³n de Scaled Dot-Product Attention
```python
import numpy as np
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Implementa Scaled Dot-Product Attention desde cero.
    
    Args:
        Q: Matriz de Queries  (seq_len_q, d_k)
        K: Matriz de Keys     (seq_len_k, d_k)
        V: Matriz de Values   (seq_len_k, d_v)
        mask: MÃ¡scara opcional (seq_len_q, seq_len_k), -inf en posiciones a ignorar
    
    Returns:
        output: RepresentaciÃ³n atendida (seq_len_q, d_v)
        pesos:  Pesos de atenciÃ³n       (seq_len_q, seq_len_k)
    """
    d_k = Q.shape[-1]
    
    # Paso 1: Calcular scores de similitud QÂ·Káµ€
    scores = Q @ K.T                          # (seq_len_q, seq_len_k)
    
    # Paso 2: Escalar para estabilizar gradientes
    scores = scores / np.sqrt(d_k)
    
    # Paso 3: Aplicar mÃ¡scara si se proporciona (para decoder)
    if mask is not None:
        scores = scores + mask                # -inf â†’ 0 en softmax
    
    # Paso 4: Softmax para obtener pesos de atenciÃ³n
    # Restar el mÃ¡ximo por fila para estabilidad numÃ©rica
    scores_estables = scores - np.max(scores, axis=-1, keepdims=True)
    exp_scores = np.exp(scores_estables)
    pesos = exp_scores / exp_scores.sum(axis=-1, keepdims=True)
    
    # Paso 5: Suma ponderada de Values
    output = pesos @ V                        # (seq_len_q, d_v)
    
    return output, pesos
# â”€â”€â”€â”€â”€ DemostraciÃ³n con secuencia simple â”€â”€â”€â”€â”€
np.random.seed(42)
seq_len = 4     # "El gato bebe leche"
d_model = 8    # dimensiÃ³n del embedding
# Simular embeddings de entrada (en prÃ¡ctica, estos vienen de la capa de embedding)
X = np.random.randn(seq_len, d_model)
# Proyecciones lineales para Q, K, V
d_k = d_v = d_model
W_Q = np.random.randn(d_model, d_k) * 0.1
W_K = np.random.randn(d_model, d_k) * 0.1
W_V = np.random.randn(d_model, d_v) * 0.1
Q = X @ W_Q    # (4, 8)
K = X @ W_K    # (4, 8)
V = X @ W_V    # (4, 8)
output, attention_weights = scaled_dot_product_attention(Q, K, V)
print(f"Input shape:           {X.shape}")
print(f"Q, K, V shapes:        {Q.shape}")
print(f"Output shape:          {output.shape}")
print(f"Attention weights:\n{np.round(attention_weights, 3)}")
print(f"\nCada fila suma a 1.0: {np.allclose(attention_weights.sum(axis=1), 1.0)}")
```
**Actividad 1.1**: Ejecuta el cÃ³digo y examina la matriz de pesos de atenciÃ³n. Â¿QuÃ© posiciÃ³n atiende mÃ¡s a sÃ­ misma? Â¿Por quÃ© tiene sentido?
**Actividad 1.2**: Modifica los embeddings de entrada de modo que los tokens 0 y 2 sean casi idÃ©nticos. Â¿CÃ³mo cambia la distribuciÃ³n de atenciÃ³n?
### 1.3 Self-Attention Completo como Clase
```python
class SelfAttentionNumPy:
    """
    Self-Attention completo con proyecciones Q, K, V aprendibles.
    """
    def __init__(self, d_model, d_k=None, d_v=None):
        self.d_model = d_model
        self.d_k = d_k or d_model
        self.d_v = d_v or d_model
        
        # InicializaciÃ³n Xavier
        scale = np.sqrt(2.0 / (d_model + self.d_k))
        self.W_Q = np.random.randn(d_model, self.d_k) * scale
        self.W_K = np.random.randn(d_model, self.d_k) * scale
        self.W_V = np.random.randn(d_model, self.d_v) * scale
        self.W_O = np.random.randn(self.d_v, d_model) * scale
    
    def forward(self, X, mask=None):
        """
        Args:
            X:    (seq_len, d_model)
            mask: (seq_len, seq_len) opcional
        Returns:
            output: (seq_len, d_model)
            weights: (seq_len, seq_len)
        """
        Q = X @ self.W_Q     # (seq_len, d_k)
        K = X @ self.W_K     # (seq_len, d_k)
        V = X @ self.W_V     # (seq_len, d_v)
        
        attn_out, weights = scaled_dot_product_attention(Q, K, V, mask)
        output = attn_out @ self.W_O    # (seq_len, d_model)
        
        return output, weights
# Prueba
np.random.seed(0)
sa = SelfAttentionNumPy(d_model=16)
X_test = np.random.randn(6, 16)    # secuencia de 6 tokens
out, w = sa.forward(X_test)
print(f"Input:  {X_test.shape}")
print(f"Output: {out.shape}")
print(f"Pesos de atenciÃ³n (6Ã—6):\n{np.round(w, 3)}")
```
**Actividad 1.3**: Implementa una **mÃ¡scara causal** (triangular inferior) para el decoder. En un decoder autoregresivo, el token en posiciÃ³n *t* solo puede atender a posiciones â‰¤ *t*.
```python
def crear_mascara_causal(seq_len):
    """
    Crea una mÃ¡scara triangular inferior para atenciÃ³n causal.
    Las posiciones superiores reciben -inf para ser ignoradas en softmax.
    """
    # Comienza con una matriz de ceros (posiciones permitidas)
    mask = np.zeros((seq_len, seq_len))
    # Asigna -inf a la parte triangular superior (posiciones futuras â€” prohibidas)
    mask[np.triu_indices(seq_len, k=1)] = -np.inf
    return mask
mascara = crear_mascara_causal(4)
print("MÃ¡scara causal (4Ã—4):")
print(mascara)
```
### Preguntas de ReflexiÃ³n
**Pregunta 1.1 (Concebir)**: Â¿Por quÃ© dividimos los scores por âˆšd_k? Â¿QuÃ© ocurrirÃ­a si no lo hiciÃ©ramos cuando d_k es grande (por ejemplo, d_k=512)?
**Pregunta 1.2 (DiseÃ±ar)**: Â¿CuÃ¡l es la diferencia fundamental entre Self-Attention y la atenciÃ³n de Bahdanau utilizada en los seq2seq con RNN?
**Pregunta 1.3 (Implementar)**: La complejidad computacional de Self-Attention es O(nÂ²Â·d). Para una secuencia de 1000 tokens con d_model=512, Â¿cuÃ¡ntas operaciones de punto flotante implica solo el cÃ¡lculo de QÂ·Káµ€?
**Pregunta 1.4 (Operar)**: Si tienes una frase ambigua como "Vi a la estudiante con el telescopio", Â¿cÃ³mo esperarÃ­as que se distribuyan los pesos de atenciÃ³n alrededor de la palabra "con"?
## ğŸ”¬ Parte 2: Multi-Head Attention (40 min)
### 2.1 MotivaciÃ³n: MÃºltiples Perspectivas
Una sola cabeza de atenciÃ³n solo puede enfocarse en un tipo de relaciÃ³n a la vez. **Multi-Head Attention** ejecuta *h* atenciones en paralelo, cada una en un subespacio diferente:
```
Cabeza 1: "Â¿QuiÃ©n hace quÃ©?" (relaciones sintÃ¡cticas sujeto-verbo)
Cabeza 2: "Â¿QuÃ© describe quÃ©?" (adjetivos y sustantivos)
Cabeza 3: "Â¿QuÃ© viene antes/despuÃ©s?" (dependencias posicionales)
Cabeza 4: "Â¿QuÃ© co-refiere?" (pronombres y sus antecedentes)
```
### 2.2 ImplementaciÃ³n de Multi-Head Attention
```python
class MultiHeadAttentionNumPy:
    """
    Multi-Head Attention desde cero con NumPy.
    
    Args:
        d_model: DimensiÃ³n del modelo
        num_heads: NÃºmero de cabezas de atenciÃ³n
    """
    def __init__(self, d_model, num_heads):
        assert d_model % num_heads == 0, \
            f"d_model ({d_model}) debe ser divisible por num_heads ({num_heads})"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads    # dimensiÃ³n por cabeza
        
        # Pesos para Q, K, V de TODAS las cabezas (concatenados)
        scale = np.sqrt(2.0 / (d_model + self.d_k))
        self.W_Q = np.random.randn(d_model, d_model) * scale   # (d_model, d_model)
        self.W_K = np.random.randn(d_model, d_model) * scale
        self.W_V = np.random.randn(d_model, d_model) * scale
        self.W_O = np.random.randn(d_model, d_model) * scale   # proyecciÃ³n final
    
    def split_heads(self, X, seq_len):
        """
        Reorganiza X de (seq_len, d_model) a (num_heads, seq_len, d_k)
        """
        X = X.reshape(seq_len, self.num_heads, self.d_k)
        return X.transpose(1, 0, 2)    # (num_heads, seq_len, d_k)
    
    def forward(self, Q_in, K_in, V_in, mask=None):
        """
        Args:
            Q_in, K_in, V_in: (seq_len, d_model)
            mask: (seq_len, seq_len) opcional
        Returns:
            output:  (seq_len, d_model)
            weights: (num_heads, seq_len, seq_len)
        """
        seq_len = Q_in.shape[0]
        
        # Proyecciones lineales
        Q = Q_in @ self.W_Q    # (seq_len, d_model)
        K = K_in @ self.W_K
        V = V_in @ self.W_V
        
        # Dividir en cabezas: (num_heads, seq_len, d_k)
        Q = self.split_heads(Q, seq_len)
        K = self.split_heads(K, seq_len)
        V = self.split_heads(V, seq_len)
        
        # AtenciÃ³n por cabeza
        all_heads = []
        all_weights = []
        for i in range(self.num_heads):
            head_out, head_w = scaled_dot_product_attention(Q[i], K[i], V[i], mask)
            all_heads.append(head_out)       # (seq_len, d_k)
            all_weights.append(head_w)       # (seq_len, seq_len)
        
        # Concatenar cabezas: (seq_len, d_model)
        concatenado = np.concatenate(all_heads, axis=-1)
        
        # ProyecciÃ³n final
        output = concatenado @ self.W_O    # (seq_len, d_model)
        
        return output, np.array(all_weights)    # weights: (h, seq_len, seq_len)
# â”€â”€â”€ DemostraciÃ³n â”€â”€â”€
np.random.seed(7)
d_model = 32
num_heads = 4
seq_len = 5
mha = MultiHeadAttentionNumPy(d_model=d_model, num_heads=num_heads)
X = np.random.randn(seq_len, d_model)
output, weights = mha.forward(X, X, X)
print(f"Input shape:            {X.shape}")
print(f"Output shape:           {output.shape}")
print(f"Weights shape (h,s,s):  {weights.shape}")
print(f"\nPesos de la cabeza 0:\n{np.round(weights[0], 3)}")
print(f"\nPesos de la cabeza 1:\n{np.round(weights[1], 3)}")
```
**Actividad 2.1**: Compara los mapas de atenciÃ³n de las diferentes cabezas. Â¿Observas patrones distintos entre ellas? Â¿QuÃ© sugiere esto sobre lo que aprende cada cabeza?
**Actividad 2.2**: Implementa la versiÃ³n **vectorizada** del bucle por cabezas usando `np.einsum` o reordenando los tensores para evitar el loop explÃ­cito.
### 2.3 VisualizaciÃ³n de Mapas de AtenciÃ³n
```python
import matplotlib.pyplot as plt
def visualizar_atencion(weights, tokens, titulo="Mapa de AtenciÃ³n", num_heads_mostrar=4):
    """
    Visualiza mapas de atenciÃ³n como heat maps.
    
    Args:
        weights: (num_heads, seq_len, seq_len)
        tokens:  lista de strings con los tokens
        titulo:  tÃ­tulo del grÃ¡fico
    """
    h = min(num_heads_mostrar, weights.shape[0])
    fig, axes = plt.subplots(1, h, figsize=(4 * h, 4))
    if h == 1:
        axes = [axes]
    
    for i, ax in enumerate(axes):
        im = ax.imshow(weights[i], cmap='Blues', vmin=0, vmax=1)
        ax.set_xticks(range(len(tokens)))
        ax.set_yticks(range(len(tokens)))
        ax.set_xticklabels(tokens, rotation=45, ha='right')
        ax.set_yticklabels(tokens)
        ax.set_title(f'Cabeza {i+1}')
        plt.colorbar(im, ax=ax)
    
    plt.suptitle(titulo, fontsize=13, fontweight='bold')
    plt.tight_layout()
    plt.savefig('atencion_multihead.png', dpi=120, bbox_inches='tight')
    plt.show()
    print("âœ… Guardado: atencion_multihead.png")
# Ejemplo de uso con tokens representativos
tokens_ejemplo = ["El", "gato", "bebe", "la", "leche"]
np.random.seed(42)
mha_demo = MultiHeadAttentionNumPy(d_model=16, num_heads=4)
X_demo = np.random.randn(5, 16)
_, w_demo = mha_demo.forward(X_demo, X_demo, X_demo)
visualizar_atencion(w_demo, tokens_ejemplo,
                    titulo="Multi-Head Attention â€” 4 Cabezas")
```
### Preguntas de ReflexiÃ³n
**Pregunta 2.1 (Concebir)**: Â¿Por quÃ© se divide d_model entre el nÃºmero de cabezas para obtener d_k? Â¿QuÃ© ocurrirÃ­a si cada cabeza tuviera d_k = d_model completo?
**Pregunta 2.2 (DiseÃ±ar)**: En la prÃ¡ctica, Â¿cuÃ¡ntas cabezas tienen BERT-base (12 capas) y GPT-2 (12 capas)? Â¿CuÃ¡l es d_k en cada caso?
**Pregunta 2.3 (Implementar)**: El nÃºmero total de parÃ¡metros en Multi-Head Attention es 4 Ã— d_modelÂ². Para BERT-base (d_model=768), Â¿cuÃ¡ntos parÃ¡metros tiene una sola capa de atenciÃ³n?
**Pregunta 2.4 (Operar)**: Visualiza los pesos de atenciÃ³n de tus 4 cabezas. Â¿Alguna muestra un patrÃ³n diagonal (cada token atiende principalmente a sÃ­ mismo)? Â¿QuÃ© implicaciones tiene eso?
## ğŸ”¬ Parte 3: Positional Encoding (30 min)
### 3.1 El Problema de la Invariancia al Orden
Self-Attention es **equivariante a permutaciones**: si desordenamos los tokens de entrada, las representaciones de salida tambiÃ©n se desordenan de la misma manera, pero no hay informaciÃ³n sobre el orden original. Para el modelo, "El gato persigue al perro" y "El perro persigue al gato" serÃ­an equivalentes sin Positional Encoding.
### 3.2 ImplementaciÃ³n de Positional Encoding Sinusoidal
```python
def positional_encoding_sinusoidal(seq_len, d_model):
    """
    Genera Positional Encoding sinusoidal (Vaswani et al., 2017).
    
    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    
    Args:
        seq_len: longitud mÃ¡xima de secuencia
        d_model: dimensiÃ³n del modelo
    
    Returns:
        PE: (seq_len, d_model)
    """
    PE = np.zeros((seq_len, d_model))
    
    posiciones = np.arange(seq_len).reshape(-1, 1)     # (seq_len, 1)
    indices_dim = np.arange(0, d_model, 2)              # 0, 2, 4, ..., d_model-2
    
    # Calcular los divisores: 10000^(2i/d_model)
    divisores = np.power(10000.0, indices_dim / d_model)    # (d_model/2,)
    
    # Asignar senos a dimensiones pares
    PE[:, 0::2] = np.sin(posiciones / divisores)
    # Asignar cosenos a dimensiones impares
    PE[:, 1::2] = np.cos(posiciones / divisores)
    
    return PE
# â”€â”€â”€ GeneraciÃ³n y visualizaciÃ³n â”€â”€â”€
seq_len = 50
d_model = 64
PE = positional_encoding_sinusoidal(seq_len, d_model)
print(f"Positional Encoding shape: {PE.shape}")
print(f"PE[0, :6] (pos=0):         {np.round(PE[0, :6], 4)}")
print(f"PE[1, :6] (pos=1):         {np.round(PE[1, :6], 4)}")
# Verificar propiedad: rango de valores siempre entre -1 y 1
print(f"\nRango de valores: [{PE.min():.2f}, {PE.max():.2f}]")
# Similitud entre posiciones consecutivas vs. distantes
dot_consec = PE[0] @ PE[1]
dot_lejos  = PE[0] @ PE[25]
print(f"\nProducto punto pos (0,1):  {dot_consec:.2f}  (posiciones cercanas)")
print(f"Producto punto pos (0,25): {dot_lejos:.2f}  (posiciones lejanas)")
```
### 3.3 VisualizaciÃ³n del Positional Encoding
```python
def visualizar_positional_encoding(PE, titulo="Positional Encoding Sinusoidal"):
    """Visualiza el mapa de calor del Positional Encoding."""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Mapa de calor completo
    im = axes[0].imshow(PE, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)
    axes[0].set_xlabel('DimensiÃ³n del embedding', fontsize=12)
    axes[0].set_ylabel('PosiciÃ³n en la secuencia', fontsize=12)
    axes[0].set_title('Mapa de calor completo')
    plt.colorbar(im, ax=axes[0])
    
    # Primeras 4 dimensiones a lo largo de la secuencia
    for dim in range(4):
        etiqueta = f'dim {dim} ({"sin" if dim % 2 == 0 else "cos"})'
        axes[1].plot(PE[:, dim], label=etiqueta)
    axes[1].set_xlabel('PosiciÃ³n', fontsize=12)
    axes[1].set_ylabel('Valor de encoding', fontsize=12)
    axes[1].set_title('Primeras 4 dimensiones')
    axes[1].legend()
    axes[1].grid(alpha=0.3)
    
    plt.suptitle(titulo, fontsize=13, fontweight='bold')
    plt.tight_layout()
    plt.savefig('positional_encoding.png', dpi=120, bbox_inches='tight')
    plt.show()
    print("âœ… Guardado: positional_encoding.png")
PE_vis = positional_encoding_sinusoidal(100, 128)
visualizar_positional_encoding(PE_vis)
```
**Actividad 3.1**: Calcula la **similitud de coseno** entre todos los pares de posiciones del encoding. Â¿QuÃ© observas? Â¿Las posiciones cercanas son mÃ¡s similares que las lejanas?
**Actividad 3.2**: Compara el encoding sinusoidal con un **encoding aprendible** (embeddings de posiciÃ³n aleatorios que se entrenarÃ­an). Â¿QuÃ© ventajas tiene el sinusoidal para secuencias mÃ¡s largas que las vistas en entrenamiento?
### Preguntas de ReflexiÃ³n
**Pregunta 3.1 (Concebir)**: Â¿Por quÃ© la base 10000 en la funciÃ³n sinusoidal? Prueba con base 100 y base 1000000 y visualiza la diferencia.
**Pregunta 3.2 (DiseÃ±ar)**: BERT usa embeddings de posiciÃ³n **aprendibles** (no sinusoidales). Â¿QuÃ© implicaciÃ³n tiene esto para secuencias mÃ¡s largas que la longitud mÃ¡xima de entrenamiento (512 tokens)?
**Pregunta 3.3 (Implementar)**: El Positional Encoding se **suma** a los embeddings de tokens (no se concatena). Â¿Por quÃ© suma y no concatenaciÃ³n? Â¿QuÃ© dimensionalidad se perderÃ­a con la concatenaciÃ³n?
**Pregunta 3.4 (Operar)**: Observa el mapa de calor del encoding. Â¿QuÃ© tipo de frecuencias corresponden a las primeras dimensiones vs. las Ãºltimas dimensiones?
## ğŸ”¬ Parte 4: Transformer Encoder Block con PyTorch (45 min)
### 4.1 De NumPy a PyTorch
Hasta ahora implementaste los componentes desde cero con NumPy. En esta parte construirÃ¡s un Transformer Encoder Block completo y diferenciable con PyTorch, listo para entrenamiento con backpropagation.
### 4.2 Transformer Encoder Block
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
class MultiHeadAttentionPyTorch(nn.Module):
    """
    Multi-Head Attention implementado con PyTorch.
    Utiliza nn.Linear para las proyecciones Q, K, V y O.
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Proyecciones lineales (sin bias para simplificar; en prÃ¡ctica se usa bias)
        self.W_Q = nn.Linear(d_model, d_model, bias=False)
        self.W_K = nn.Linear(d_model, d_model, bias=False)
        self.W_V = nn.Linear(d_model, d_model, bias=False)
        self.W_O = nn.Linear(d_model, d_model, bias=False)
        
        self.dropout = nn.Dropout(dropout)
    
    def split_heads(self, x, batch_size):
        """(batch, seq, d_model) â†’ (batch, heads, seq, d_k)"""
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)    # (batch, heads, seq, d_k)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # Proyecciones y split en cabezas
        Q = self.split_heads(self.W_Q(query), batch_size)    # (b, h, s, d_k)
        K = self.split_heads(self.W_K(key), batch_size)
        V = self.split_heads(self.W_V(value), batch_size)
        
        # Scaled dot-product attention
        scores = Q @ K.transpose(-2, -1) / (self.d_k ** 0.5)  # (b, h, s, s)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        pesos = F.softmax(scores, dim=-1)
        pesos = self.dropout(pesos)
        
        # Suma ponderada y concatenaciÃ³n de cabezas
        attn_out = pesos @ V                              # (b, h, s, d_k)
        attn_out = attn_out.transpose(1, 2).contiguous()  # (b, s, h, d_k)
        attn_out = attn_out.view(batch_size, -1, self.d_model)  # (b, s, d_model)
        
        return self.W_O(attn_out), pesos


class FeedForwardBlock(nn.Module):
    """
    Red Feed-Forward del Transformer: dos capas lineales con ReLU (o GELU).
    FFN(x) = max(0, xÂ·Wâ‚ + bâ‚)Â·Wâ‚‚ + bâ‚‚
    La dimensiÃ³n interna suele ser 4Ã— la del modelo.
    """
    def __init__(self, d_model, d_ff=None, dropout=0.1):
        super().__init__()
        d_ff = d_ff or 4 * d_model
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        return self.linear2(self.dropout(F.gelu(self.linear1(x))))


class TransformerEncoderBlock(nn.Module):
    """
    Bloque Encoder del Transformer (Vaswani et al., 2017):
    
    1. Multi-Head Self-Attention
    2. Add & LayerNorm  (conexiÃ³n residual)
    3. Feed-Forward Network
    4. Add & LayerNorm  (conexiÃ³n residual)
    """
    def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):
        super().__init__()
        
        self.attention = MultiHeadAttentionPyTorch(d_model, num_heads, dropout)
        self.ffn       = FeedForwardBlock(d_model, d_ff, dropout)
        
        self.norm1   = nn.LayerNorm(d_model)
        self.norm2   = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # Sub-capa 1: Self-Attention + residual + norm
        attn_out, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_out))
        
        # Sub-capa 2: FFN + residual + norm
        ffn_out = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_out))
        
        return x


class TransformerEncoder(nn.Module):
    """
    Encoder completo: apilamiento de N bloques encoder.
    Incluye embedding de tokens y positional encoding.
    """
    def __init__(self, vocab_size, d_model, num_heads, num_layers,
                 d_ff=None, max_seq_len=512, dropout=0.1):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = nn.Embedding(max_seq_len, d_model)    # aprendible
        self.dropout = nn.Dropout(dropout)
        
        self.layers = nn.ModuleList([
            TransformerEncoderBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        self.norm = nn.LayerNorm(d_model)
        
        # InicializaciÃ³n de pesos
        self._init_weights()
    
    def _init_weights(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, token_ids, mask=None):
        batch, seq_len = token_ids.shape
        
        # Embeddings de tokens + posicionales
        posiciones = torch.arange(seq_len, device=token_ids.device).unsqueeze(0)
        x = self.dropout(self.embedding(token_ids) + self.pos_encoding(posiciones))
        
        # Pasar por N bloques encoder
        for capa in self.layers:
            x = capa(x, mask)
        
        return self.norm(x)
# â”€â”€â”€ Prueba del modelo â”€â”€â”€
torch.manual_seed(42)
vocab_size = 1000
d_model    = 64
num_heads  = 4
num_layers = 2
encoder = TransformerEncoder(
    vocab_size=vocab_size,
    d_model=d_model,
    num_heads=num_heads,
    num_layers=num_layers,
    max_seq_len=100
)
# Batch de 3 secuencias de 10 tokens cada una
batch_tokens = torch.randint(0, vocab_size, (3, 10))
representaciones = encoder(batch_tokens)
print(f"Input (tokens):  {batch_tokens.shape}   â†’ (batch=3, seq_len=10)")
print(f"Output (repr.):  {representaciones.shape} â†’ (batch=3, seq_len=10, d_model=64)")
# Contar parÃ¡metros
total_params = sum(p.numel() for p in encoder.parameters())
print(f"\nTotal de parÃ¡metros: {total_params:,}")
print("Desglose por mÃ³dulo:")
for nombre, modulo in encoder.named_children():
    params = sum(p.numel() for p in modulo.parameters())
    print(f"  {nombre}: {params:,}")
```
**Actividad 4.1**: Construye un **TransformerDecoderBlock** que incluya (1) Masked Self-Attention, (2) Cross-Attention con las representaciones del encoder, y (3) FFN, cada uno seguido de Add & LayerNorm.
**Actividad 4.2**: Implementa un clasificador de texto simple aÃ±adiendo una capa lineal al final del encoder (sobre el token `[CLS]`) y entrÃ©nalo en un dataset de juguete con 2 clases.
### Preguntas de ReflexiÃ³n
**Pregunta 4.1 (Concebir)**: Las conexiones residuales (Add) son fundamentales para el entrenamiento de redes profundas. Â¿QuÃ© problema resuelven concretamente y cÃ³mo lo hacen?
**Pregunta 4.2 (DiseÃ±ar)**: Â¿Por quÃ© se usa Layer Normalization en los Transformers en lugar de Batch Normalization? Â¿QuÃ© diferencia hay en quÃ© dimensiÃ³n se normaliza?
**Pregunta 4.3 (Implementar)**: El paper original usa la variante "Post-LN" (Add & Norm despuÃ©s de la sub-capa). Los modelos modernos usan "Pre-LN" (Norm antes de la sub-capa). Â¿CuÃ¡l es mÃ¡s estable durante el entrenamiento y por quÃ©?
**Pregunta 4.4 (Operar)**: Experimenta con distintos valores de dropout (0.0, 0.1, 0.3). Â¿CÃ³mo afecta al overfitting cuando entrenas con pocos datos?
## ğŸ”¬ Parte 5: Fine-tuning de BERT con Hugging Face (50 min)
### 5.1 Transfer Learning con Transformers Pre-entrenados
Pre-entrenar un Transformer desde cero requiere recursos masivos (BERT-base fue entrenado durante 4 dÃ­as en 64 TPUs de Google con 3.3 mil millones de palabras). En la prÃ¡ctica, usamos modelos pre-entrenados y los **ajustamos (fine-tuning)** para nuestra tarea especÃ­fica.
```
PRE-TRAINING (una vez, muy costoso):
  BERT fue entrenado con:
  1. Masked Language Modeling (MLM): predecir tokens enmascarados
  2. Next Sentence Prediction (NSP): Â¿estas dos frases son consecutivas?

FINE-TUNING (rÃ¡pido, por tarea):
  Tomar BERT pre-entrenado + aÃ±adir capa de clasificaciÃ³n + entrenar con pocos datos
```
### 5.2 AnÃ¡lisis de Sentimientos con BERT
```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset
import torch
import numpy as np
# â”€â”€â”€ 1. Cargar tokenizador y modelo pre-entrenado â”€â”€â”€
model_name = "distilbert-base-uncased"    # versiÃ³n ligera de BERT (40% mÃ¡s rÃ¡pido)
tokenizer = AutoTokenizer.from_pretrained(model_name)
modelo = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2    # Positivo / Negativo
)
print(f"Modelo cargado: {model_name}")
print(f"ParÃ¡metros: {sum(p.numel() for p in modelo.parameters()):,}")
# â”€â”€â”€ 2. Explorar el tokenizador â”€â”€â”€
ejemplos = [
    "This movie was absolutely fantastic!",
    "I hated every minute of this film.",
    "The plot was interesting but the acting was poor."
]
for texto in ejemplos:
    tokens = tokenizer(texto, return_tensors='pt', truncation=True, max_length=64)
    print(f"\nTexto: {texto[:50]}...")
    print(f"  IDs: {tokens['input_ids'][0][:8].tolist()} ...")
    print(f"  Tokens: {tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][:8])}")
# â”€â”€â”€ 3. Preparar dataset â”€â”€â”€
dataset = load_dataset("imdb", split={'train': 'train[:2000]', 'test': 'test[:500]'})
def tokenizar(ejemplos_batch):
    return tokenizer(
        ejemplos_batch['text'],
        truncation=True,
        max_length=256,
        padding='max_length'
    )
dataset_tokenizado = dataset.map(tokenizar, batched=True)
dataset_tokenizado = dataset_tokenizado.rename_column("label", "labels")
dataset_tokenizado.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
print(f"\nDataset tokenizado:")
print(f"  Train: {len(dataset_tokenizado['train'])} muestras")
print(f"  Test:  {len(dataset_tokenizado['test'])} muestras")
# â”€â”€â”€ 4. Configurar entrenamiento â”€â”€â”€
def calcular_metricas(eval_pred):
    logits, labels = eval_pred
    predicciones = np.argmax(logits, axis=-1)
    accuracy = (predicciones == labels).mean()
    return {"accuracy": accuracy}
training_args = TrainingArguments(
    output_dir="./bert_sentimiento",
    num_train_epochs=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    learning_rate=2e-5,            # LR pequeÃ±o para fine-tuning
    warmup_ratio=0.1,
    weight_decay=0.01,
    fp16=torch.cuda.is_available(),
    logging_steps=50,
    report_to="none"
)
trainer = Trainer(
    model=modelo,
    args=training_args,
    train_dataset=dataset_tokenizado['train'],
    eval_dataset=dataset_tokenizado['test'],
    compute_metrics=calcular_metricas
)
# â”€â”€â”€ 5. Entrenar â”€â”€â”€
print("\nğŸš€ Iniciando fine-tuning...")
trainer.train()
print("\nâœ… Fine-tuning completado!")
# â”€â”€â”€ 6. EvaluaciÃ³n e inferencia â”€â”€â”€
resultados = trainer.evaluate()
print(f"\nğŸ“Š Resultados en test:")
print(f"  Accuracy: {resultados['eval_accuracy']:.4f}")
# Inferencia en nuevas frases
def predecir_sentimiento(textos, modelo, tokenizer):
    modelo.eval()
    codificado = tokenizer(textos, return_tensors='pt',
                           truncation=True, max_length=256, padding=True)
    with torch.no_grad():
        logits = modelo(**codificado).logits
    predicciones = torch.argmax(logits, dim=-1)
    etiquetas = ['Negativo', 'Positivo']
    return [(t, etiquetas[p.item()], torch.softmax(logits, dim=-1)[i].max().item())
            for i, (t, p) in enumerate(zip(textos, predicciones))]
nuevas_frases = [
    "The special effects were mind-blowing and the story was compelling.",
    "I fell asleep halfway through. Complete waste of time.",
    "Decent film, nothing extraordinary but enjoyable enough."
]
print("\nğŸ” Predicciones en nuevas frases:")
for texto, etiqueta, confianza in predecir_sentimiento(nuevas_frases, modelo, tokenizer):
    print(f"  '{texto[:60]}...'")
    print(f"  â†’ {etiqueta} ({confianza:.2%} confianza)")
```
**Actividad 5.1**: Experimenta con distintos learning rates (5e-5, 2e-5, 1e-5, 5e-6). Â¿QuÃ© ocurre con un LR demasiado alto durante el fine-tuning de BERT?
**Actividad 5.2**: Prueba **congelar los primeros 6 layers** del modelo y solo entrenar los Ãºltimos 6 + la capa de clasificaciÃ³n. Â¿CÃ³mo afecta al rendimiento y al tiempo de entrenamiento?
### Preguntas de ReflexiÃ³n
**Pregunta 5.1 (Concebir)**: Â¿Por quÃ© se usa un learning rate mucho mÃ¡s pequeÃ±o (2e-5) para fine-tuning que para entrenar desde cero (1e-3)? Â¿QuÃ© podrÃ­a ocurrir con un LR grande?
**Pregunta 5.2 (DiseÃ±ar)**: Â¿CuÃ¡l es la diferencia entre DistilBERT, BERT-base y BERT-large en tÃ©rminos de nÃºmero de capas, cabezas y parÃ¡metros? Â¿CuÃ¡ndo elegirÃ­as cada uno?
**Pregunta 5.3 (Implementar)**: El token `[CLS]` al inicio de cada secuencia en BERT acumula informaciÃ³n global. Â¿CÃ³mo podrÃ­as usar la representaciÃ³n de `[CLS]` directamente sin la API de Hugging Face?
**Pregunta 5.4 (Operar)**: EvalÃºa tu modelo con una frase ambigua como "It's not bad, just not what I expected." Â¿QuÃ© etiqueta asigna? Â¿Tiene sentido? Â¿QuÃ© dice esto sobre las limitaciones del modelo?
## ğŸ”¬ Parte 6: GeneraciÃ³n de Texto con GPT-2 (40 min)
### 6.1 Arquitectura Decoder: GPT vs. BERT
GPT es un modelo **decoder-only**: usa atenciÃ³n causal (mÃ¡scara triangular), de modo que cada token solo puede "ver" los tokens anteriores. Esto lo hace ideal para generaciÃ³n autoregresiva.
```
BERT (Encoder â€” bidireccional):
  xâ‚ â†” xâ‚‚ â†” xâ‚ƒ â†” xâ‚„   (cada token ve todos los demÃ¡s)
  â†’ Ideal para comprensiÃ³n

GPT (Decoder â€” causal/unidireccional):
  xâ‚ â†’ xâ‚‚ â†’ xâ‚ƒ â†’ xâ‚„   (cada token solo ve los anteriores)
  â†’ Ideal para generaciÃ³n
```
### 6.2 GeneraciÃ³n de Texto con GPT-2 y Hugging Face
```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
# â”€â”€â”€ 1. Cargar GPT-2 â”€â”€â”€
tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')
modelo_gpt2    = GPT2LMHeadModel.from_pretrained('gpt2')
modelo_gpt2.eval()
print(f"GPT-2 cargado. ParÃ¡metros: {sum(p.numel() for p in modelo_gpt2.parameters()):,}")
print(f"TamaÃ±o del vocabulario: {tokenizer_gpt2.vocab_size:,}")
# â”€â”€â”€ 2. GeneraciÃ³n bÃ¡sica (greedy) â”€â”€â”€
def generar_texto_greedy(prompt, max_new_tokens=50):
    """GeneraciÃ³n greedy: en cada paso elige el token mÃ¡s probable."""
    ids = tokenizer_gpt2.encode(prompt, return_tensors='pt')
    
    with torch.no_grad():
        output_ids = modelo_gpt2.generate(
            ids,
            max_new_tokens=max_new_tokens,
            do_sample=False    # greedy
        )
    
    return tokenizer_gpt2.decode(output_ids[0], skip_special_tokens=True)
# â”€â”€â”€ 3. GeneraciÃ³n con sampling â”€â”€â”€
def generar_texto_sampling(prompt, max_new_tokens=100,
                            temperature=0.8, top_k=50, top_p=0.92):
    """
    GeneraciÃ³n con muestreo:
    - temperature: controla la aleatoriedad (< 1 = mÃ¡s conservador)
    - top_k: considera solo los k tokens mÃ¡s probables en cada paso
    - top_p (nucleus sampling): considera tokens hasta cubrir probabilidad p
    """
    ids = tokenizer_gpt2.encode(prompt, return_tensors='pt')
    
    with torch.no_grad():
        output_ids = modelo_gpt2.generate(
            ids,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=1.2,    # penaliza repeticiones
            pad_token_id=tokenizer_gpt2.eos_token_id
        )
    
    return tokenizer_gpt2.decode(output_ids[0], skip_special_tokens=True)
# â”€â”€â”€ 4. DemostraciÃ³n â”€â”€â”€
prompts = [
    "Artificial intelligence will transform",
    "The history of neural networks began",
    "In the future, language models will"
]
print("=" * 60)
for prompt in prompts:
    print(f"\nğŸ“ Prompt: '{prompt}'")
    print(f"\nğŸ¤– Greedy:")
    print(generar_texto_greedy(prompt, max_new_tokens=40))
    print(f"\nğŸ² Sampling (temp=0.8):")
    print(generar_texto_sampling(prompt, max_new_tokens=60))
    print("-" * 60)
```
### 6.3 AnÃ¡lisis de Probabilidades de Tokens
```python
def analizar_probabilidades(prompt, top_n=10):
    """
    Muestra las probabilidades del siguiente token mÃ¡s probable dado un prompt.
    Ãštil para entender cÃ³mo GPT-2 "razona".
    """
    ids = tokenizer_gpt2.encode(prompt, return_tensors='pt')
    
    with torch.no_grad():
        logits = modelo_gpt2(ids).logits
    
    # Probabilidades del Ãºltimo token (siguiente a generar)
    probs = torch.softmax(logits[0, -1, :], dim=-1)
    top_probs, top_ids = probs.topk(top_n)
    
    print(f"Prompt: '{prompt}'")
    print(f"Siguiente token mÃ¡s probable:")
    for prob, idx in zip(top_probs, top_ids):
        token = tokenizer_gpt2.decode([idx.item()])
        print(f"  '{token}': {prob.item():.4f} ({prob.item()*100:.2f}%)")
analizar_probabilidades("The Transformer architecture was introduced in")
analizar_probabilidades("Deep learning models require large amounts of")
```
**Actividad 6.1**: Experimenta con distintos valores de `temperature` (0.3, 0.7, 1.0, 1.5). Â¿CÃ³mo afecta a la coherencia y creatividad del texto generado?
**Actividad 6.2**: Implementa **beam search** con `num_beams=5` y compara la calidad del texto con la generaciÃ³n greedy y por sampling.
### Preguntas de ReflexiÃ³n
**Pregunta 6.1 (Concebir)**: Â¿QuÃ© es el "hallucination problem" en LLMs? A partir de tus experimentos con GPT-2, Â¿puedes identificar casos donde el modelo genera texto plausible pero factualmente incorrecto?
**Pregunta 6.2 (DiseÃ±ar)**: DiseÃ±a un sistema de clasificaciÃ³n de texto usando GPT-2 con **prompting** (sin fine-tuning). Por ejemplo: "La siguiente reseÃ±a de pelÃ­cula es [positiva/negativa]: [texto]". Â¿QuÃ© ventajas y limitaciones tiene este enfoque vs. fine-tuning de BERT?
**Pregunta 6.3 (Implementar)**: La `temperature` en la generaciÃ³n se aplica dividiendo los logits antes del softmax: `probs = softmax(logits / T)`. Â¿QuÃ© ocurre matemÃ¡ticamente con Tâ†’0 (greedy) y Tâ†’âˆ (distribuciÃ³n uniforme)?
**Pregunta 6.4 (Operar)**: Mide el tiempo de generaciÃ³n de 100 tokens con GPT-2 en CPU vs. GPU (si disponible). Â¿CuÃ¡nto mÃ¡s rÃ¡pida es la GPU? Â¿QuÃ© implica esto para el despliegue de LLMs en producciÃ³n?
## ğŸ“Š AnÃ¡lisis Final de Rendimiento y Complejidad (30 min)
### Comparativa: Transformers vs. RNNs
```python
import numpy as np
import matplotlib.pyplot as plt
import time
# â”€â”€â”€ Complejidad teÃ³rica â”€â”€â”€
def analizar_complejidad_teorica():
    """
    Compara complejidad O() de RNN vs. Transformer
    
    RNN:       O(n * dÂ²)          tiempo, O(n) paralelo â†’ NO
    Self-Attn: O(nÂ² * d)          tiempo, O(1) paralelo â†’ SÃ
    FFN:       O(n * d * d_ff)    tiempo, O(1) paralelo â†’ SÃ
    """
    longitudes = np.arange(10, 1001, 10)
    d_model = 512
    d_ff = 2048
    
    # Operaciones de punto flotante (aprox.)
    ops_rnn     = longitudes * d_model ** 2        # secuencial
    ops_attn    = longitudes ** 2 * d_model        # cuadrÃ¡tico en longitud
    ops_ffn     = longitudes * d_model * d_ff      # lineal en longitud
    ops_total_t = ops_attn + ops_ffn               # Transformer total
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # GrÃ¡fica 1: FLOPs vs longitud
    axes[0].plot(longitudes, ops_rnn / 1e9,     label='RNN (secuencial)', color='red', linewidth=2)
    axes[0].plot(longitudes, ops_total_t / 1e9, label='Transformer (paralelo)', color='blue', linewidth=2)
    axes[0].plot(longitudes, ops_attn / 1e9,    label='Solo Self-Attention', color='blue',
                 linestyle='--', linewidth=1.5)
    axes[0].set_xlabel('Longitud de secuencia (tokens)', fontsize=12)
    axes[0].set_ylabel('GFLOPs (aprox.)', fontsize=12)
    axes[0].set_title('Complejidad Computacional', fontsize=13)
    axes[0].legend()
    axes[0].grid(alpha=0.3)
    
    # GrÃ¡fica 2: Memoria (attention matrix crece cuadrÃ¡ticamente)
    memoria_attn = longitudes ** 2 * d_model * 4 / (1024**2)  # MB (float32)
    memoria_rnn  = longitudes * d_model * 4 / (1024**2)        # MB
    
    axes[1].plot(longitudes, memoria_attn, label='Attention Matrix (Transformer)', color='blue', linewidth=2)
    axes[1].plot(longitudes, memoria_rnn,  label='Estado oculto (RNN)', color='red', linewidth=2)
    axes[1].set_xlabel('Longitud de secuencia (tokens)', fontsize=12)
    axes[1].set_ylabel('Memoria aproximada (MB)', fontsize=12)
    axes[1].set_title('Uso de Memoria', fontsize=13)
    axes[1].legend()
    axes[1].grid(alpha=0.3)
    
    plt.suptitle('Transformer vs. RNN â€” Complejidad y Escalabilidad', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('complejidad_transformer_rnn.png', dpi=120, bbox_inches='tight')
    plt.show()
    print("âœ… Guardado: complejidad_transformer_rnn.png")
    
    # Tabla resumen
    print("\n" + "="*70)
    print(f"{'MÃ©trica':<30} {'RNN/LSTM':<20} {'Transformer':<20}")
    print("="*70)
    metricas = [
        ("Complejidad tiempo/capa",  "O(nÂ·dÂ²)",     "O(nÂ²Â·d)"),
        ("ParalelizaciÃ³n",           "NO (secuenc.)", "SÃ (completa)"),
        ("Dependencias largas",      "DifÃ­cil",       "O(1) en profundidad"),
        ("Memoria (attention)",      "O(nÂ·d)",        "O(nÂ²Â·d)"),
        ("Escalabilidad con datos",  "Moderada",      "Muy alta"),
        ("Interpretabilidad",        "DifÃ­cil",       "Attention weights"),
    ]
    for metrica, rnn, transformer in metricas:
        print(f"  {metrica:<28} {rnn:<20} {transformer:<20}")
    print("="*70)
analizar_complejidad_teorica()
```
### AnÃ¡lisis de Escalabilidad EmpÃ­rica
```python
def benchmark_transformer_escalabilidad():
    """Benchmark empÃ­rico: tiempo de inferencia vs. longitud de secuencia."""
    import torch
    
    d_model = 128
    num_heads = 4
    
    class AttnSimple(nn.Module):
        def __init__(self):
            super().__init__()
            self.attn = MultiHeadAttentionPyTorch(d_model, num_heads, dropout=0.0)
        def forward(self, x):
            return self.attn(x, x, x)[0]
    
    modelo_bench = AttnSimple().eval()
    longitudes = [32, 64, 128, 256, 512]
    tiempos = []
    
    for seq_len in longitudes:
        x = torch.randn(1, seq_len, d_model)
        # Warm-up
        with torch.no_grad():
            for _ in range(5):
                modelo_bench(x)
        # Benchmark
        inicio = time.time()
        with torch.no_grad():
            for _ in range(50):
                modelo_bench(x)
        tiempo_medio = (time.time() - inicio) / 50 * 1000  # ms
        tiempos.append(tiempo_medio)
        print(f"  seq_len={seq_len:4d}: {tiempo_medio:.2f} ms")
    
    # Graficar
    plt.figure(figsize=(8, 4))
    plt.plot(longitudes, tiempos, 'o-', color='blue', linewidth=2, markersize=8)
    plt.xlabel('Longitud de secuencia', fontsize=12)
    plt.ylabel('Tiempo por inferencia (ms)', fontsize=12)
    plt.title('Tiempo de inferencia de Self-Attention vs. Longitud (CPU)', fontsize=13)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig('benchmark_atencion.png', dpi=120, bbox_inches='tight')
    plt.show()
    
    # Verificar si crece cuadrÃ¡ticamente
    ratios = [tiempos[i+1] / tiempos[i] for i in range(len(tiempos)-1)]
    print(f"\nRatios de tiempo al doblar la secuencia: {[f'{r:.2f}x' for r in ratios]}")
    print(f"Si O(nÂ²): esperarÃ­amos ratios de ~4x")
    print(f"Ratio observado promedio: {np.mean(ratios):.2f}x")
print("\nğŸ“Š Benchmark de escalabilidad:")
benchmark_transformer_escalabilidad()
```
## ğŸ¯ EJERCICIOS PROPUESTOS
### Ejercicio 1 â€” BÃ¡sico (30 min): Self-Attention Manual
Implementa Self-Attention completamente desde cero **sin usar las clases del laboratorio**, solo NumPy y la fÃ³rmula:
$$\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$
**Requisitos:**
1. Crea matrices Q, K, V aleatorias de forma (5, 8)
2. Calcula los attention scores paso a paso
3. Aplica scaling con âˆšd_k
4. Calcula softmax manualmente (sin `np.exp` de scipy, solo NumPy)
5. Multiplica por V para obtener el output
6. Verifica que los pesos de atenciÃ³n suman 1.0 por fila
7. Documenta cada paso con comentarios claros
**Entregable:** FunciÃ³n `mi_self_attention(Q, K, V)` completamente documentada.
### Ejercicio 2 â€” Intermedio (60 min): Transformer Block desde Cero
Implementa un **Transformer Encoder Block completo** usando Ãºnicamente NumPy (sin PyTorch):
**Requisitos:**
1. Clase `LayerNormNumPy` con parÃ¡metros Î³ y Î² aprendibles
2. Clase `FeedForwardNumPy` con dos capas lineales y activaciÃ³n ReLU
3. Clase `TransformerBlockNumPy` que integre:
   - Multi-Head Attention (usa tu implementaciÃ³n de la Parte 2)
   - ConexiÃ³n residual + LayerNorm
   - FFN + conexiÃ³n residual + LayerNorm
4. Prueba el bloque con una secuencia de 8 tokens y d_model=32
5. Verifica que las dimensiones de entrada y salida son idÃ©nticas
6. Implementa un paso de forward y backward **manual** para un parÃ¡metro (gradiente por definiciÃ³n)
**Entregable:** Clase `TransformerBlockNumPy` con tests de dimensiones y un diagrama ASCII del flujo de datos.
### Ejercicio 3 â€” Avanzado (90 min): Fine-tuning para ClasificaciÃ³n Multiclase
Fine-tunea **BERT** (o DistilBERT) para clasificaciÃ³n de **20 categorÃ­as** de noticias (dataset `20newsgroups`):
**Requisitos:**
1. Carga el dataset con Hugging Face `datasets` o `sklearn`
2. Preprocesa y tokeniza correctamente (max_length=256, truncation, padding)
3. Fine-tunea por mÃ­nimo 3 Ã©pocas con learning rate scheduling
4. Reporta accuracy, F1-macro y matriz de confusiÃ³n
5. Identifica las **3 categorÃ­as** con peor rendimiento y analiza por quÃ©
6. Compara con un baseline de TF-IDF + RegresiÃ³n LogÃ­stica
**Entregable:** Notebook con experimentaciÃ³n, tabla comparativa de mÃ©tricas y anÃ¡lisis crÃ­tico de errores.
### Ejercicio 4 â€” DesafÃ­o (2-3 horas): Transformer Miniatura para ClasificaciÃ³n de Secuencias
Construye un **mini-Transformer desde cero con PyTorch** para clasificar secuencias sintÃ©ticas:
**Requisitos:**
1. Genera un dataset sintÃ©tico: secuencias de nÃºmeros donde la etiqueta depende de patrones (e.g., "Â¿hay un 5 seguido de un 7 en la secuencia?")
2. Implementa un Transformer Encoder con:
   - Positional encoding sinusoidal (no aprendible)
   - 2 bloques encoder (d_model=64, num_heads=4, d_ff=256)
   - Capa de clasificaciÃ³n sobre el token `[CLS]`
3. Entrena con Adam, learning rate scheduling con warm-up
4. Grafica curvas de pÃ©rdida y accuracy en train/val
5. Visualiza los **mapas de atenciÃ³n** de ambas capas para 5 ejemplos de test
6. Compara contra un LSTM equivalente en mismo nÃºmero de parÃ¡metros
**Entregable:** CÃ³digo limpio, grÃ¡ficas comparativas y anÃ¡lisis de quÃ© aprende cada cabeza de atenciÃ³n.
### Ejercicio 5 â€” Proyecto (4+ horas): Sistema de Preguntas y Respuestas con BERT
Construye un sistema de **Question Answering (QA) extractivo** usando BERT:
**Requisitos:**
1. Usa el dataset SQuAD (Stanford Question Answering Dataset) con Hugging Face
2. Fine-tunea `bert-base-uncased` para la tarea de QA extractiva:
   - El modelo debe predecir la posiciÃ³n inicio/fin de la respuesta en el contexto
3. Implementa la lÃ³gica de inferencia: dado un contexto y una pregunta, extraer el span de respuesta
4. EvalÃºa con mÃ©tricas EM (Exact Match) y F1 token-level
5. Implementa una **interfaz de demostraciÃ³n interactiva** (CLI o Gradio)
6. Analiza casos donde el modelo falla (preguntas que requieren razonamiento vs. extracciÃ³n directa)
**Entregable completo:**
- CÃ³digo de entrenamiento y evaluaciÃ³n
- MÃ©tricas EM y F1 en el set de validaciÃ³n
- Interfaz de demostraciÃ³n funcional
- Reporte de 3 pÃ¡ginas con anÃ¡lisis de errores y propuestas de mejora
## ğŸ“ Entregables
Para completar este laboratorio, debes entregar:
### 1. CÃ³digo Implementado (60%)
- Archivo `transformers_scratch.py` con implementaciones NumPy (Partes 1-3)
- Archivo `transformer_pytorch.py` con el Transformer en PyTorch (Parte 4)
- Archivo `bert_finetune.py` con el fine-tuning de BERT (Parte 5)
- Archivo `gpt2_generation.py` con generaciÃ³n de texto (Parte 6)
- Todas las funciones y clases con docstrings completos
- CÃ³digo limpio, modular y con manejo de errores
### 2. Notebook de ExperimentaciÃ³n (25%)
- `practica_lab12.ipynb` con:
  - Todas las actividades completadas
  - Visualizaciones de mapas de atenciÃ³n claramente etiquetadas
  - GrÃ¡ficas de positional encoding
  - Curvas de entrenamiento del fine-tuning
  - Ejemplos de texto generado con GPT-2
  - Respuestas a todas las Preguntas de ReflexiÃ³n
### 3. Reporte TÃ©cnico (15%)
- Documento PDF (3-4 pÃ¡ginas) que incluya:
  - ExplicaciÃ³n del mecanismo de Self-Attention con fÃ³rmulas
  - Comparativa Transformers vs. RNNs (ventajas y limitaciones)
  - Resultados del fine-tuning (accuracy, curvas de entrenamiento)
  - AnÃ¡lisis de mapas de atenciÃ³n
  - Reflexiones sobre el impacto de los Transformers en IA
### Formato de Entrega
```
Lab12_NombreApellido/
â”œâ”€â”€ codigo/
â”‚   â”œâ”€â”€ transformers_scratch.py     # NumPy: Partes 1-3
â”‚   â”œâ”€â”€ transformer_pytorch.py      # PyTorch: Parte 4
â”‚   â”œâ”€â”€ bert_finetune.py            # BERT fine-tuning: Parte 5
â”‚   â””â”€â”€ gpt2_generation.py          # GPT-2: Parte 6
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ practica_lab12.ipynb        # Notebook principal
â”‚   â””â”€â”€ ejercicios.ipynb            # Ejercicios propuestos
â”œâ”€â”€ imagenes/
â”‚   â”œâ”€â”€ atencion_multihead.png
â”‚   â”œâ”€â”€ positional_encoding.png
â”‚   â”œâ”€â”€ complejidad_transformer_rnn.png
â”‚   â””â”€â”€ benchmark_atencion.png
â”œâ”€â”€ modelos/
â”‚   â””â”€â”€ bert_sentimiento/           # Modelo fine-tuneado (opcional)
â”œâ”€â”€ reporte/
â”‚   â””â”€â”€ reporte_lab12.pdf
â””â”€â”€ README.md
```
## ğŸ¯ Criterios de EvaluaciÃ³n (CDIO)
### Concebir (25%)
- âœ… Comprende el mecanismo de Self-Attention (Q, K, V) y su intuiciÃ³n
- âœ… Explica por quÃ© se necesita el escalado por âˆšd_k
- âœ… Identifica las diferencias entre Encoder (BERT) y Decoder (GPT)
- âœ… Reconoce las limitaciones O(nÂ²) de los Transformers y sus variantes eficientes
- âœ… Justifica cuÃ¡ndo usar fine-tuning vs. prompting vs. entrenamiento desde cero
**Evidencia**: Respuestas a preguntas de reflexiÃ³n, introducciÃ³n del reporte, anÃ¡lisis comparativo
### DiseÃ±ar (25%)
- âœ… DiseÃ±a la arquitectura apropiada para una tarea NLP dada (encoder vs. decoder vs. encoder-decoder)
- âœ… Elige hiperparÃ¡metros justificados (d_model, num_heads, num_layers, learning rate)
- âœ… DiseÃ±a el pipeline de tokenizaciÃ³n, padding y masking correctamente
- âœ… Planifica el proceso de fine-tuning considerando el riesgo de catastrophic forgetting
- âœ… Selecciona el modelo pre-entrenado apropiado para la tarea
**Evidencia**: Decisiones de diseÃ±o documentadas, tabla de comparaciÃ³n de arquitecturas, justificaciÃ³n de hiperparÃ¡metros
### Implementar (30%)
- âœ… Implementa Scaled Dot-Product Attention correctamente desde cero (NumPy)
- âœ… Construye Multi-Head Attention con las dimensiones correctas
- âœ… Implementa Positional Encoding sinusoidal verificando propiedades matemÃ¡ticas
- âœ… Construye el Transformer Encoder Block completo en PyTorch (atenciÃ³n + FFN + residual + norm)
- âœ… Fine-tunea BERT con el pipeline de Hugging Face correctamente
- âœ… Genera texto con GPT-2 usando diferentes estrategias de decodificaciÃ³n
- âœ… CÃ³digo modular, documentado y con tests de dimensiones
**Evidencia**: CÃ³digo fuente, dimensiones verificadas, output de ejecuciÃ³n sin errores
### Operar (20%)
- âœ… Analiza crÃ­ticamente los mapas de atenciÃ³n generados
- âœ… EvalÃºa el fine-tuning con mÃ©tricas apropiadas (accuracy, F1, confusion matrix)
- âœ… Identifica y documenta casos de fallo del modelo
- âœ… Compara empÃ­ricamente el rendimiento de Transformers vs. RNNs
- âœ… Propone mejoras concretas basadas en los resultados observados
- âœ… Resuelve problemas de memoria/GPU con estrategias prÃ¡cticas
**Evidencia**: Reporte tÃ©cnico, visualizaciones anotadas, anÃ¡lisis de errores, propuestas de mejora
### RÃºbrica Detallada
| Criterio | Excelente (100%) | Bueno (80%) | Aceptable (60%) | Insuficiente (<60%) |
|----------|------------------|-------------|-----------------|---------------------|
| **Self-Attention** | ImplementaciÃ³n perfecta, verifica todas las propiedades matemÃ¡ticas | Funciona correctamente, pequeÃ±os detalles | Funciona con limitaciones | Incompleto o incorrecto |
| **Multi-Head Attention** | ImplementaciÃ³n vectorizada, visualizaciÃ³n clara de mÃºltiples cabezas | Funciona con loop, visualizaciÃ³n bÃ¡sica | Funciona parcialmente | No funciona |
| **Positional Encoding** | ImplementaciÃ³n correcta, anÃ¡lisis de propiedades, visualizaciÃ³n clara | ImplementaciÃ³n correcta, visualizaciÃ³n bÃ¡sica | ImplementaciÃ³n con errores menores | Ausente o incorrecto |
| **Transformer PyTorch** | Bloque completo, conexiones residuales, LN, entrenamiento demostrado | Bloque funcional, algunos componentes faltantes | Estructura parcial | No funcional |
| **Fine-tuning BERT** | Accuracy > 90%, curvas de entrenamiento, anÃ¡lisis de errores | Accuracy > 85%, mÃ©tricas bÃ¡sicas | Accuracy > 75%, sin anÃ¡lisis | No se ejecuta |
| **GeneraciÃ³n GPT-2** | MÃºltiples estrategias comparadas, anÃ¡lisis de calidad | GeneraciÃ³n funcional, una estrategia | GeneraciÃ³n bÃ¡sica | No funciona |
| **AnÃ¡lisis** | Profundo, crÃ­tico, con comparativa Transformer vs. RNN | Completo y correcto | Superficial | Ausente |
| **DocumentaciÃ³n** | Excelente, profesional, con fÃ³rmulas y referencias | Buena, entendible | BÃ¡sica | Pobre o ausente |
## ğŸ“š Referencias Adicionales
### Libros
1. **Vaswani, A. et al.** (2017). "Attention Is All You Need"
   - ArtÃ­culo original del Transformer â€” lectura obligatoria
   - https://arxiv.org/abs/1706.03762
2. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). "Deep Learning"
   - CapÃ­tulo 10: Sequence Modeling: Recurrent and Recursive Nets
   - CapÃ­tulo 12: Applications (NLP)
   - http://www.deeplearningbook.org
3. **Jurafsky, D., & Martin, J.H.** (2023). "Speech and Language Processing" (3rd ed.)
   - CapÃ­tulo 9: Transformers and Pre-Trained Language Models
   - Disponible gratuitamente: https://web.stanford.edu/~jurafsky/slp3/
4. **Lewis, T., Fergus, R., & Conneau, A.** â€” Hugging Face Course
   - https://huggingface.co/course â€” gratuito e interactivo
### ArtÃ­culos AcadÃ©micos
1. **Vaswani, A. et al.** (2017). "Attention Is All You Need" â€” *NeurIPS 2017*
   - IntroducciÃ³n de la arquitectura Transformer
2. **Devlin, J. et al.** (2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" â€” *NAACL 2019*
   - https://arxiv.org/abs/1810.04805
3. **Brown, T. et al.** (2020). "Language Models are Few-Shot Learners" (GPT-3) â€” *NeurIPS 2020*
   - https://arxiv.org/abs/2005.14165
4. **Dosovitskiy, A. et al.** (2020). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" (ViT)
   - https://arxiv.org/abs/2010.11929
5. **Bahdanau, D. et al.** (2014). "Neural Machine Translation by Jointly Learning to Align and Translate"
   - Primer mecanismo de atenciÃ³n â€” contexto histÃ³rico
   - https://arxiv.org/abs/1409.0473
6. **Raffel, C. et al.** (2019). "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (T5)
   - https://arxiv.org/abs/1910.10683
### Recursos Online
1. **"The Illustrated Transformer"** â€” Jay Alammar
   - http://jalammar.github.io/illustrated-transformer/
   - La mejor explicaciÃ³n visual del Transformer â€” **lectura recomendada**
2. **"The Illustrated BERT"** â€” Jay Alammar
   - http://jalammar.github.io/illustrated-bert/
   - VisualizaciÃ³n del pre-entrenamiento y fine-tuning de BERT
3. **"The Annotated Transformer"** â€” Harvard NLP
   - http://nlp.seas.harvard.edu/2018/04/03/attention.html
   - ImplementaciÃ³n lÃ­nea a lÃ­nea del paper original
4. **Stanford CS224N** â€” Natural Language Processing with Deep Learning
   - http://web.stanford.edu/class/cs224n/
   - Slides y videos de clase sobre Transformers
5. **Andrej Karpathy** â€” "Let's build GPT: from scratch, in code, spelled out"
   - https://www.youtube.com/watch?v=kCc8FmEb1nY
   - Video de 2h construyendo un GPT desde cero â€” **altamente recomendado**
### Tutoriales Interactivos
1. **Hugging Face Course** â€” MÃ³dulo 1: Transformer Models
   - https://huggingface.co/course/chapter1
   - Interactivo, con cÃ³digo ejecutable en la nube
2. **Bertviz** â€” Herramienta para visualizar atenciÃ³n en BERT/GPT
   - https://github.com/jessevig/bertviz
   - `pip install bertviz`
3. **Attention Playground** â€” VisualizaciÃ³n interactiva de attention
   - https://poloclub.github.io/transformer-explainer/
4. **Google Colab Notebooks oficiales de Hugging Face**
   - Notebooks listos para ejecutar con GPU gratuita
   - https://github.com/huggingface/notebooks
### DocumentaciÃ³n TÃ©cnica
1. **Hugging Face Transformers Documentation**
   - https://huggingface.co/docs/transformers
   - Referencia completa de la librerÃ­a
2. **PyTorch nn.Transformer**
   - https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html
   - ImplementaciÃ³n oficial en PyTorch
3. **Hugging Face Model Hub**
   - https://huggingface.co/models
   - Miles de modelos pre-entrenados disponibles
4. **Papers With Code** â€” Transformers
   - https://paperswithcode.com/methods/category/transformer-based-architectures
   - Estado del arte actualizado
## ğŸ“ Notas Finales
### Conceptos Clave para Recordar
1. **Self-Attention = BÃºsqueda Diferenciable**
   - Q (quÃ© busco) Â· Káµ€ (quÃ© ofrezco) â†’ scores â†’ softmax â†’ pesos
   - Output = suma ponderada de Values
   - Complejidad O(nÂ²Â·d) â€” cuello de botella para secuencias largas
2. **Multi-Head Attention = MÃºltiples Perspectivas**
   - h cabezas con d_k = d_model/h dimensiones cada una
   - Cada cabeza aprende relaciones diferentes
   - Concatenar + proyectar = misma dimensiÃ³n que la entrada
3. **Positional Encoding = Inyectar Orden**
   - Self-Attention es invariante al orden â†’ necesitamos indicar posiciÃ³n
   - Sinusoidal: deterministico, generaliza a secuencias largas
   - Aprendible (BERT): mÃ¡s flexible, limitado a longitud de entrenamiento
4. **Transformer Block = AtenciÃ³n + FFN + Residual + Norm**
   - Conexiones residuales: resuelven gradiente que desaparece
   - Layer Norm: estabiliza la distribuciÃ³n de activaciones
   - FFN: transforma las representaciones en el espacio de caracterÃ­sticas
5. **Pre-Training + Fine-Tuning = Paradigma Dominante**
   - Pre-entrenamiento masivo captura conocimiento general del lenguaje
   - Fine-tuning con pocos datos adapta a la tarea especÃ­fica
   - LR pequeÃ±o (2e-5 a 5e-5) previene el olvido catastrÃ³fico
6. **BERT vs. GPT = ComprensiÃ³n vs. GeneraciÃ³n**
   - BERT: encoder bidireccional, Masked LM, ideal para clasificaciÃ³n/QA
   - GPT: decoder causal, next token prediction, ideal para generaciÃ³n
   - T5: encoder-decoder, text-to-text para cualquier tarea
### ğŸ‰ PreparaciÃ³n: Â¡Has Completado los 12 Laboratorios!
Â¡**Felicitaciones**! Has completado exitosamente el ciclo completo de 12 laboratorios de Deep Learning. Este Ãºltimo laboratorio coronÃ³ tu formaciÃ³n con la arquitectura mÃ¡s influyente de la IA moderna.
**El viaje que recorriste:**
```
Lab 01 â†’ Neuronas artificiales desde cero
Lab 02 â†’ Primera red neuronal (forward pass)
Lab 03 â†’ Funciones de activaciÃ³n (no linealidad)
Lab 04 â†’ Funciones de pÃ©rdida (optimizar quÃ©)
Lab 05 â†’ Backpropagation (cÃ³mo aprende la red)
Lab 06 â†’ Entrenamiento: SGD, Adam, regularizaciÃ³n
Lab 07 â†’ MÃ©tricas de evaluaciÃ³n y validaciÃ³n
Lab 08 â†’ Frameworks: PyTorch y TensorFlow
Lab 09 â†’ IA Generativa: GANs y VAEs
Lab 10 â†’ CNNs para visiÃ³n computacional
Lab 11 â†’ RNNs/LSTMs para secuencias
Lab 12 â†’ Transformers: la arquitectura del futuro âœ…
```
**Tu prÃ³ximo paso**: Con esta base sÃ³lida, estÃ¡s listo para explorar:
- **Modelos de Lenguaje Grandes (LLMs)**: Llama, Mistral, fine-tuning con QLoRA/LoRA
- **IA Multimodal**: CLIP, DALL-E, Stable Diffusion, GPT-4V
- **Reinforcement Learning from Human Feedback (RLHF)**: cÃ³mo se entrena ChatGPT
- **Efficient Transformers**: FlashAttention, Longformer, Mamba (State Space Models)
- **Proyectos de investigaciÃ³n**: contribuye a Hugging Face, reproduce un paper
### Consejos de Estudio
1. **Lee "The Illustrated Transformer"**: Es la mejor introducciÃ³n visual y toma menos de una hora. Hazlo antes de implementar.
2. **Construye desde cero**: Implementar Self-Attention manualmente con NumPy consolidarÃ¡ el concepto de forma que ninguna librerÃ­a puede hacerlo.
3. **Visualiza los mapas de atenciÃ³n**: El Transformer es uno de los modelos mÃ¡s interpretables gracias a los attention weights. Ãšsalos.
4. **Experimenta con prompts**: Con GPT-2 o GPT-4 API, la ingenierÃ­a de prompts es una habilidad prÃ¡ctica inmediata.
5. **Hugging Face es tu aliado**: Domina la librerÃ­a `transformers` â€” es el estÃ¡ndar de la industria para NLP.
6. **Conoce las limitaciones**: Los Transformers no son perfectos. Estudia Efficient Transformers (Longformer, BigBird, FlashAttention) para secuencias largas.
### SoluciÃ³n de Problemas Comunes
**Problema**: `CUDA out of memory` al cargar BERT o GPT-2
- **Causa**: GPU insuficiente o batch size muy grande
- **SoluciÃ³n 1**: Reducir `per_device_train_batch_size` a 8 o 4
- **SoluciÃ³n 2**: Usar `fp16=True` en TrainingArguments (requiere GPU con Tensor Cores)
- **SoluciÃ³n 3**: Usar `DistilBERT` en lugar de BERT-base (40% menos parÃ¡metros)
- **SoluciÃ³n 4**: Usar gradient checkpointing: `model.gradient_checkpointing_enable()`
**Problema**: Fine-tuning converge muy lento o no converge
- **Causa**: Learning rate inapropiado
- **SoluciÃ³n**: Usar learning rate entre 1e-5 y 5e-5 con warm-up (primeras 10% de iteraciones)
- **Verificar**: que `input_ids`, `attention_mask` y `labels` estÃ©n correctamente preparados
**Problema**: `RuntimeError: Expected all tensors to be on the same device`
- **Causa**: Modelo en GPU pero datos en CPU (o viceversa)
- **SoluciÃ³n**: `inputs = {k: v.to(device) for k, v in inputs.items()}`
**Problema**: Nan en loss durante el entrenamiento
- **Causa**: Learning rate demasiado alto o gradientes explosivos
- **SoluciÃ³n**: Gradient clipping: `max_grad_norm=1.0` en TrainingArguments
**Problema**: Tokenizador trunca textos importantes
- **Causa**: `max_length` por defecto (512 para BERT) es insuficiente
- **SoluciÃ³n**: Estrategia de sliding window para textos largos; usar modelos con contexto mayor (Longformer, BERT-large-512)
**Problema**: Attention weights contienen NaN
- **Causa**: Overflow en la exponencial de softmax con scores muy grandes
- **SoluciÃ³n**: Verificar que el scaling por âˆšd_k estÃ¡ aplicado; usar estabilidad numÃ©rica (restar el mÃ¡ximo antes del exp)
**Problema**: GeneraciÃ³n de texto con GPT-2 produce repeticiones
- **Causa**: Modo greedy o temperatura muy baja
- **SoluciÃ³n**: Usar `repetition_penalty=1.2` y `top_p=0.92` en `model.generate()`
### Comunidad y Soporte
- **Foro del curso**: Para preguntas tÃ©cnicas sobre implementaciÃ³n
- **Horas de oficina**: Consultas sobre fine-tuning y proyectos avanzados
- **Hugging Face Forums**: https://discuss.huggingface.co â€” comunidad muy activa
- **Stack Overflow**: Tag `pytorch`, `huggingface-transformers`
- **Papers With Code**: https://paperswithcode.com â€” para comparar implementaciones
- **Discord de Hugging Face**: Para preguntas en tiempo real
### Lista de VerificaciÃ³n de Completitud
Has completado exitosamente el Lab 12 cuando puedes:
- [ ] Explicar el mecanismo de Self-Attention (Q, K, V) sin consultar notas
- [ ] Implementar Scaled Dot-Product Attention desde cero con NumPy
- [ ] Construir Multi-Head Attention y visualizar los mapas de cada cabeza
- [ ] Generar Positional Encoding sinusoidal y explicar su propiedad de periodicidad
- [ ] Construir un Transformer Encoder Block completo en PyTorch
- [ ] Fine-tunear DistilBERT/BERT para una tarea de clasificaciÃ³n
- [ ] Generar texto con GPT-2 usando temperature, top-k y top-p sampling
- [ ] Comparar la complejidad O(nÂ²Â·d) de Transformers vs. O(nÂ·dÂ²) de RNNs
- [ ] Explicar la diferencia entre BERT (encoder) y GPT (decoder)
- [ ] Resolver al menos un ejercicio propuesto completo
---
**Â¡Felicitaciones por completar los 12 laboratorios de Deep Learning! ğŸ“ğŸ†**

Has recorrido un camino desde la neurona artificial mÃ¡s simple hasta los Transformers que impulsan la IA mÃ¡s avanzada del mundo. Con esta base, estÃ¡s preparado para investigar, desarrollar y contribuir al estado del arte en Deep Learning.

**"Attention Is All You Need"** â€” y ahora tÃº tambiÃ©n sabes por quÃ©. ğŸš€
---
*Ãšltima actualizaciÃ³n: 2024*  
*VersiÃ³n: 1.0*  
*Licencia: MIT - Uso educativo*
