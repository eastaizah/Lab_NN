# Lab 09: IA Generativa

## Objetivos
1. Comprender modelos generativos vs discriminativos
2. Entender arquitecturas VAE y GAN
3. Implementar VAE simple
4. Explorar espacio latente
5. Generar nuevas muestras

## Estructura
```
Lab08_IA_Generativa/
â”œâ”€â”€ README.md
â”œâ”€â”€ teoria.md
â”œâ”€â”€ practica.ipynb
â””â”€â”€ codigo/
    â””â”€â”€ generativo.py
```

## Modelos Generativos

### Discriminativos (lo que vimos):
- Aprenden P(y|X)
- Clasifican, predicen
- Ejemplo: "Â¿Es un gato?"

### Generativos (nuevos):
- Aprenden P(X)
- Crean contenido nuevo
- Ejemplo: "Genera un gato"

## Principales Arquitecturas

### 1. Autoencoder (AE)
```
X â†’ [Encoder] â†’ z â†’ [Decoder] â†’ X'
```
- CompresiÃ³n y reconstrucciÃ³n
- No genera bien cosas nuevas

### 2. Variational Autoencoder (VAE)
```
X â†’ [Encoder] â†’ Î¼, Ïƒ â†’ sample z â†’ [Decoder] â†’ X'
```
- Espacio latente probabilÃ­stico
- Genera nuevas muestras
- Loss: Reconstruction + KL divergence

### 3. GAN (Generative Adversarial Network)
```
Generator:      z â†’ G(z) â†’ fake image
Discriminator:  image â†’ real/fake
```
- Competencia adversarial
- ImÃ¡genes muy realistas
- Entrenamiento complejo

### 4. Diffusion Models
```
Forward:  X â†’ ... â†’ ruido
Backward: ruido â†’ ... â†’ X
```
- Muy alta calidad
- DALL-E, Stable Diffusion

## PrÃ¡ctica

### Ejecutar:
```bash
cd codigo/
python generativo.py
```

### Notebook:
```bash
jupyter notebook practica.ipynb
```

## Conceptos Clave

### Espacio Latente
RepresentaciÃ³n comprimida de datos:
- Continuidad
- InterpolaciÃ³n
- Control semÃ¡ntico

### Reparameterization Trick (VAE)
```python
z = Î¼ + Ïƒ * Îµ  # donde Îµ ~ N(0,1)
```
Permite backpropagation a travÃ©s de sampling.

### Adversarial Training (GAN)
```python
# Entrenar D
loss_D = -log(D(real)) - log(1 - D(fake))

# Entrenar G
loss_G = -log(D(fake))
```

## Aplicaciones

- **ImÃ¡genes**: Arte, diseÃ±o, ediciÃ³n
- **Texto**: ChatGPT, escritura
- **Audio**: SÃ­ntesis de voz, mÃºsica
- **Video**: Deepfakes, animaciÃ³n
- **Ciencia**: DiseÃ±o molecular

## Ejercicios

1. Entrenar VAE en MNIST
2. Explorar espacio latente
3. Generar interpolaciones
4. Implementar GAN simple (con framework)

## Ã‰tica

**Considerar**:
- Deepfakes y desinformaciÃ³n
- Sesgos en datos de entrenamiento
- Derechos de autor
- Uso responsable

## Frameworks Recomendados

Para IA generativa seria:
```bash
# PyTorch
pip install torch torchvision

# TensorFlow
pip install tensorflow

# Hugging Face (modelos pre-entrenados)
pip install transformers diffusers
```

## Modelos Pre-entrenados

- **Stable Diffusion**: Texto â†’ Imagen
- **GPT**: GeneraciÃ³n de texto
- **DALL-E**: Texto â†’ Imagen
- **StyleGAN**: GeneraciÃ³n de caras

## VerificaciÃ³n
- [ ] Entiendo diferencia generativo vs discriminativo
- [ ] Conozco arquitecturas VAE y GAN
- [ ] Puedo implementar VAE bÃ¡sico
- [ ] Entiendo espacio latente
- [ ] SÃ© sobre aplicaciones y Ã©tica

## Recursos

### Papers
- VAE: "Auto-Encoding Variational Bayes" (2013)
- GAN: "Generative Adversarial Networks" (2014)
- Diffusion: "Denoising Diffusion Probabilistic Models" (2020)

### Tutoriales
- PyTorch GAN Tutorial
- TensorFlow VAE Guide
- Hugging Face Diffusers

## ConclusiÃ³n

Has completado el curso de **Redes Neuronales desde Cero**:

1. âœ“ Fundamentos de neuronas
2. âœ“ Arquitecturas de redes
3. âœ“ Funciones de activaciÃ³n
4. âœ“ Funciones de pÃ©rdida
5. âœ“ Backpropagation
6. âœ“ Entrenamiento completo
7. âœ“ Frameworks modernos
8. âœ“ IA Generativa

**Â¡Ahora tienes las bases para deep learning! ðŸŽ“**

---

**El futuro es generativo - Ãºsalo responsablemente! ðŸŽ¨ðŸ¤–**
