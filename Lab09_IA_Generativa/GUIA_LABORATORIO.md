# Gu√≠a de Laboratorio: Inteligencia Artificial Generativa

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Deep Learning Avanzado - Modelos Generativos (VAE, GAN y Diffusion)  
**C√≥digo:** Lab 09  
**Duraci√≥n:** 3-4 horas  
**Nivel:** Avanzado  

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Distinguir claramente entre modelos discriminativos y modelos generativos
2. Comprender la arquitectura y funcionamiento de Autoencoders (AE) b√°sicos
3. Implementar un Variational Autoencoder (VAE) completo desde cero
4. Entender el concepto de espacio latente probabil√≠stico y su importancia
5. Aplicar el reparameterization trick para hacer backpropagation en VAEs
6. Calcular e interpretar la funci√≥n de p√©rdida combinada (reconstrucci√≥n + KL divergencia)
7. Comprender la arquitectura adversarial de las Generative Adversarial Networks (GANs)
8. Implementar un GAN simple para generaci√≥n de im√°genes
9. Explorar y manipular el espacio latente para generar nuevas muestras
10. Realizar interpolaciones entre puntos en el espacio latente
11. Identificar y mitigar problemas comunes como mode collapse en GANs
12. Conocer los fundamentos de Diffusion Models y su proceso de denoising
13. Evaluar la calidad de modelos generativos usando m√©tricas apropiadas
14. Reconocer las implicaciones √©ticas del uso de IA generativa

## üìö Prerrequisitos

### Conocimientos

- **Redes Neuronales**: Arquitecturas, forward/backward pass (Labs 01-02)
- **Funciones de Activaci√≥n**: ReLU, Sigmoid, Tanh (Lab 03)
- **Funciones de P√©rdida**: MSE, Binary Cross-Entropy (Lab 04)
- **Backpropagation**: C√°lculo de gradientes y actualizaci√≥n de pesos (Lab 05)
- **Entrenamiento**: Optimizadores, batch processing, epochs (Lab 06)
- **M√©tricas de Evaluaci√≥n**: An√°lisis de rendimiento de modelos (Lab 07)
- **Frameworks Modernos**: PyTorch o TensorFlow (Lab 08)
- **Conceptos Probabil√≠sticos**: Distribuciones normales, divergencia KL
- **√Ålgebra Lineal**: Operaciones matriciales avanzadas

### Software

- Python 3.8+
- PyTorch 1.10+ o TensorFlow 2.8+
- NumPy 1.19+
- Matplotlib y Seaborn (visualizaciones)
- Jupyter Notebook o JupyterLab
- torchvision o tensorflow-datasets (datasets)
- Pillow (procesamiento de im√°genes)

### Material de Lectura

Antes de comenzar, lee:

- `teoria.md` - Marco te√≥rico completo sobre modelos generativos
- `README.md` - Estructura del laboratorio y conceptos clave
- Papers recomendados:
  - "Auto-Encoding Variational Bayes" (Kingma & Welling, 2013) - VAE
  - "Generative Adversarial Networks" (Goodfellow et al., 2014) - GAN
  - "Denoising Diffusion Probabilistic Models" (Ho et al., 2020) - Diffusion

## üìñ Introducci√≥n

La **Inteligencia Artificial Generativa** representa uno de los avances m√°s revolucionarios y fascinantes del deep learning moderno. A diferencia de los modelos discriminativos que aprendimos en laboratorios anteriores (que clasifican o predicen), los modelos generativos aprenden a **crear** contenido completamente nuevo: im√°genes realistas, texto coherente, m√∫sica original, e incluso dise√±os moleculares para nuevos f√°rmacos.

### Contexto del Problema

En los laboratorios anteriores (Labs 01-08), hemos trabajado con **modelos discriminativos** que aprenden funciones del tipo P(y|X):

- **Clasificaci√≥n de im√°genes**: ¬øEsta imagen contiene un gato o un perro? ‚Üí P(y|imagen)
- **Detecci√≥n de objetos**: ¬øD√≥nde est√°n los objetos en esta imagen? ‚Üí P(ubicaciones|imagen)
- **Predicci√≥n num√©rica**: ¬øCu√°l es el precio de esta casa? ‚Üí P(precio|caracter√≠sticas)

Estos modelos son excelentes para **reconocer** patrones, pero tienen una limitaci√≥n fundamental: **no pueden crear contenido nuevo**. Si quieres generar una imagen de un gato que nunca ha existido, un modelo de clasificaci√≥n no puede ayudarte.

**Los modelos generativos** abordan un problema fundamentalmente diferente: aprender la distribuci√≥n de probabilidad de los datos P(X) para poder:

- **Generar muestras nuevas**: Crear im√°genes, textos o audio completamente originales
- **Completar datos**: Rellenar partes faltantes de una imagen o documento
- **Transformar datos**: Convertir un boceto en una foto realista
- **Interpolar**: Crear transiciones suaves entre dos ejemplos
- **Comprimir**: Encontrar representaciones compactas de datos complejos

### Enfoque con Modelos Generativos

La IA generativa ha evolucionado a trav√©s de varias arquitecturas principales:

**1. Autoencoders (AE) - La Base:**
```
Entrada (X) ‚Üí [Encoder] ‚Üí Espacio Latente (z) ‚Üí [Decoder] ‚Üí Reconstrucci√≥n (X')
    784     ‚Üí   [NN]    ‚Üí      64           ‚Üí    [NN]    ‚Üí       784
```

- **Objetivo**: Comprimir datos a una representaci√≥n compacta y reconstruirlos
- **Limitaci√≥n**: El espacio latente puede tener "agujeros", generaci√≥n limitada
- **Analog√≠a**: Como comprimir una foto JPG - puedes recuperar algo similar, pero no puedes crear fotos nuevas

**2. Variational Autoencoders (VAE) - Probabil√≠sticos:**
```
Entrada (X) ‚Üí [Encoder] ‚Üí Œº, œÉ ‚Üí Sample z ~ N(Œº,œÉ¬≤) ‚Üí [Decoder] ‚Üí X'
```

- **Innovaci√≥n**: En lugar de codificar a un punto fijo, codifica a una **distribuci√≥n de probabilidad**
- **Ventaja**: Espacio latente continuo y completo, puede generar nuevas muestras
- **P√©rdida**: Reconstrucci√≥n + KL Divergence (fuerza el espacio latente a seguir una distribuci√≥n normal)
- **Analog√≠a**: Como aprender el "concepto" de gato en lugar de memorizar gatos espec√≠ficos

**3. Generative Adversarial Networks (GAN) - Competitivos:**
```
Generator:     z (ruido) ‚Üí G(z) ‚Üí imagen falsa
Discriminator: imagen ‚Üí D(imagen) ‚Üí [0=falsa, 1=real]
```

- **Innovaci√≥n**: Dos redes compitiendo - el generador intenta enga√±ar, el discriminador intenta detectar
- **Ventaja**: Genera im√°genes de alt√≠sima calidad y realismo
- **Desaf√≠o**: Entrenamiento inestable, sensible a hiperpar√°metros
- **Analog√≠a**: Como un falsificador (G) compitiendo contra un detective de arte (D) - ambos mejoran constantemente

**4. Diffusion Models - Denoising:**
```
Forward:  Imagen limpia ‚Üí ... ‚Üí Ruido puro (a√±adir ruido gradualmente)
Reverse:  Ruido puro ‚Üí ... ‚Üí Imagen limpia (aprender a eliminar ruido)
```

- **Innovaci√≥n**: Aprender a revertir un proceso de degradaci√≥n gradual
- **Ventaja**: Calidad excepcional, entrenamiento estable
- **Uso**: DALL-E 2, Stable Diffusion, Midjourney
- **Analog√≠a**: Como restaurar una pintura antigua eliminando capas de suciedad

### Conceptos Fundamentales

**1. Espacio Latente (Latent Space):**

El espacio latente es una representaci√≥n comprimida y continua de los datos donde:
- **Cada punto representa una posible muestra** (ej: una imagen de un d√≠gito)
- **Puntos cercanos representan muestras similares** (todos los "3" est√°n juntos)
- **Podemos interpolar** entre puntos para crear transiciones suaves
- **Dimensionalidad reducida**: 784 p√≠xeles ‚Üí 64 dimensiones latentes

```python
# Ejemplo conceptual
punto_A = [0.5, 0.3, ..., 0.8]  # Representa un "3" escrito de cierta forma
punto_B = [0.6, 0.4, ..., 0.7]  # Representa otro "3" ligeramente diferente
intermedio = 0.5 * punto_A + 0.5 * punto_B  # Mezcla de ambos estilos
```

**2. P√©rdida en VAE - Dos Objetivos:**

```python
P√©rdida_Total = P√©rdida_Reconstrucci√≥n + Œ≤ √ó P√©rdida_KL

# Reconstrucci√≥n: ¬øQu√© tan bien reconstruimos la entrada?
P√©rdida_Reconstrucci√≥n = ||X - X'||¬≤  # o Binary Cross-Entropy

# KL Divergence: ¬øQu√© tan "normal" es nuestro espacio latente?
P√©rdida_KL = -0.5 √ó Œ£(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)
```

- **Reconstrucci√≥n**: Asegura que podemos recuperar la entrada original
- **KL Divergence**: Fuerza el espacio latente a seguir N(0,1), haci√©ndolo suave y continuo
- **Trade-off**: Balance entre fidelidad de reconstrucci√≥n y calidad de generaci√≥n

**3. Reparameterization Trick:**

Problema: No podemos hacer backpropagation a trav√©s de sampling aleatorio.

Soluci√≥n elegante:
```python
# ‚ùå No diferenciable
z = sample_from(N(Œº, œÉ¬≤))

# ‚úÖ Diferenciable - separamos la aleatoriedad
Œµ = sample_from(N(0, 1))  # Ruido est√°ndar (sin par√°metros)
z = Œº + œÉ √ó Œµ              # Ahora Œº y œÉ reciben gradientes
```

**4. Entrenamiento Adversarial (GAN):**

```python
# Fase 1: Entrenar Discriminador (D)
p√©rdida_D_real = -log(D(im√°genes_reales))      # Maximizar D(real)
p√©rdida_D_fake = -log(1 - D(G(ruido)))         # Minimizar D(fake)
p√©rdida_D = p√©rdida_D_real + p√©rdida_D_fake

# Fase 2: Entrenar Generador (G)
p√©rdida_G = -log(D(G(ruido)))                  # Enga√±ar a D
```

**Equilibrio de Nash**: Cuando G y D alcanzan un punto donde ninguno puede mejorar sin que el otro se adapte.

### Aplicaciones Pr√°cticas

La IA generativa ha transformado m√∫ltiples industrias:

**1. Arte y Dise√±o:**
- **DALL-E 2, Midjourney**: Creaci√≥n de arte desde texto
- **StyleGAN**: Generaci√≥n de rostros ultrarrealistas
- **Artbreeder**: Mezcla y evoluci√≥n de im√°genes
- **Adobe Firefly**: Herramientas creativas profesionales

**2. Entretenimiento:**
- **Deepfakes**: Efectos especiales en cine (con regulaci√≥n √©tica)
- **Generaci√≥n de m√∫sica**: AIVA, MuseNet
- **Generaci√≥n de niveles**: Videojuegos procedurales
- **Animaci√≥n**: Generaci√≥n de movimientos realistas

**3. Medicina y Ciencia:**
- **Dise√±o de f√°rmacos**: Generar mol√©culas candidatas
- **S√≠ntesis de datos m√©dicos**: Entrenar modelos sin comprometer privacidad
- **Aumento de datos**: Generar im√°genes m√©dicas sint√©ticas
- **Dise√±o de prote√≠nas**: AlphaFold y variantes generativas

**4. Procesamiento de Lenguaje:**
- **ChatGPT, GPT-4**: Conversaci√≥n y escritura
- **GitHub Copilot**: Generaci√≥n de c√≥digo
- **Jasper, Copy.ai**: Escritura creativa y marketing
- **Traducci√≥n avanzada**: M√°s natural y contextual

**5. Industria y Negocios:**
- **Dise√±o de productos**: Generaci√≥n de prototipos
- **Arquitectura**: Dise√±os de edificios y espacios
- **Moda**: Generaci√≥n de patrones y dise√±os textiles
- **Marketing**: Contenido personalizado a escala

### Motivaci√≥n Hist√≥rica

La evoluci√≥n de la IA generativa ha sido fascinante:

**2013 - VAE (Kingma & Welling):**
- Introducci√≥n del espacio latente probabil√≠stico
- Permiti√≥ generaci√≥n controlable por primera vez
- Fundamento matem√°tico riguroso basado en inferencia variacional

**2014 - GAN (Ian Goodfellow):**
- Paradigma revolucionario: entrenamiento adversarial
- Historia: Goodfellow tuvo la idea en un bar durante un debate con colegas
- Demostr√≥ que la competici√≥n puede generar excelencia

**2015-2018 - Evoluci√≥n de GANs:**
- **DCGAN**: Convoluciones para im√°genes
- **ProGAN**: Generaci√≥n progresiva de alta resoluci√≥n
- **StyleGAN**: Control excepcional sobre caracter√≠sticas

**2020 - Diffusion Models:**
- **DDPM**: Fundamentos te√≥ricos de modelos de difusi√≥n
- Convergencia de ideas de f√≠sica (procesos estoc√°sticos) y ML

**2021-2024 - Explosi√≥n Comercial:**
- **DALL-E 2, Midjourney**: Texto a imagen accesible al p√∫blico
- **Stable Diffusion**: Open-source, democratizaci√≥n de la tecnolog√≠a
- **ChatGPT**: IA generativa de texto mainstream
- **Sora**: Video generativo de alta calidad

**Impacto Cultural:**
- De herramienta de investigaci√≥n a fen√≥meno global en menos de una d√©cada
- Debates sobre arte, creatividad y autor√≠a
- Nuevas profesiones: "prompt engineering"
- Preguntas √©ticas sobre desinformaci√≥n y derechos de autor

---

**En este laboratorio**, comenzaremos desde los fundamentos (Autoencoders) y avanzaremos hasta implementar VAE y GAN completos. Aprender√°s no solo a usar estos modelos, sino a **entender profundamente** c√≥mo funcionan, por qu√© funcionan, y cu√°ndo usarlos.

**Advertencia √âtica**: Con gran poder viene gran responsabilidad. La IA generativa puede usarse para crear arte hermoso y resolver problemas cient√≠ficos, pero tambi√©n para desinformaci√≥n y deepfakes maliciosos. En este laboratorio tambi√©n discutiremos las implicaciones √©ticas y c√≥mo usar esta tecnolog√≠a responsablemente.

¬°Prep√°rate para crear tu primera imagen completamente generada por IA! üé®ü§ñ

---

## üî¨ Parte 1: Fundamentos - Autoencoder Simple (45 min)

### 1.1 Introducci√≥n Conceptual

Un **Autoencoder** es como un embudo que comprime informaci√≥n y luego la expande de nuevo.

**Analog√≠a**: Imagina que tienes que enviar una foto de 1 MB a trav√©s de una conexi√≥n lenta:
- **Encoder (Compresor)**: Reduce la foto a 100 KB identificando solo lo esencial
- **Espacio Latente**: Los 100 KB que contienen la esencia de la imagen
- **Decoder (Descompresor)**: Reconstruye una imagen similar desde esos 100 KB

**Arquitectura Visual**:
```
Entrada (784)  ‚Üí  Encoder  ‚Üí  Latent (64)  ‚Üí  Decoder  ‚Üí  Salida (784)
[Imagen MNIST] ‚Üí [Comprimir] ‚Üí [C√≥digo]     ‚Üí [Expandir] ‚Üí [Reconstrucci√≥n]
     28√ó28     ‚Üí    NN       ‚Üí   8√ó8        ‚Üí    NN      ‚Üí     28√ó28
```

**Diferencia con modelos anteriores**:
- **Clasificador**: X ‚Üí [NN] ‚Üí etiqueta (ej: "es un 7")
- **Autoencoder**: X ‚Üí [NN] ‚Üí X' (reconstruye la entrada misma)

### 1.2 Implementaci√≥n del Encoder

El encoder comprime la entrada a una representaci√≥n de menor dimensi√≥n:

```python
import numpy as np
import matplotlib.pyplot as plt

class Encoder:
    """
    Encoder: Comprime datos de alta dimensi√≥n a espacio latente.
    """
    
    def __init__(self, input_dim, hidden_dim, latent_dim):
        """
        Args:
            input_dim: Dimensi√≥n de entrada (ej: 784 para MNIST)
            hidden_dim: Dimensi√≥n de capa oculta (ej: 256)
            latent_dim: Dimensi√≥n del espacio latente (ej: 64)
        """
        # Inicializaci√≥n He para mejor convergencia
        self.W1 = np.random.randn(hidden_dim, input_dim) * np.sqrt(2.0 / input_dim)
        self.b1 = np.zeros((hidden_dim, 1))
        
        self.W2 = np.random.randn(latent_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)
        self.b2 = np.zeros((latent_dim, 1))
        
        print(f"‚úÖ Encoder creado:")
        print(f"   {input_dim} ‚Üí {hidden_dim} ‚Üí {latent_dim}")
    
    def relu(self, x):
        """Activaci√≥n ReLU."""
        return np.maximum(0, x)
    
    def forward(self, X):
        """
        Codifica entradas al espacio latente.
        
        Args:
            X: (input_dim, batch_size)
        
        Returns:
            z: (latent_dim, batch_size) - representaci√≥n latente
        """
        # Capa 1: input ‚Üí hidden
        self.h1 = self.relu(self.W1 @ X + self.b1)
        
        # Capa 2: hidden ‚Üí latent
        self.z = self.relu(self.W2 @ self.h1 + self.b2)
        
        return self.z

# Ejemplo de uso
print("="*70)
print("1. IMPLEMENTANDO EL ENCODER")
print("="*70)

# Datos de prueba (simulamos im√°genes 28√ó28 = 784)
batch_size = 5
input_dim = 784  # MNIST: 28√ó28
X_test = np.random.rand(input_dim, batch_size)

# Crear encoder
encoder = Encoder(input_dim=784, hidden_dim=256, latent_dim=64)

# Forward pass
z = encoder.forward(X_test)

print(f"\nüìä Resultados:")
print(f"   Entrada: {X_test.shape} ‚Üí {X_test.size} valores")
print(f"   Latente: {z.shape} ‚Üí {z.size} valores")
print(f"   Compresi√≥n: {X_test.size / z.size:.1f}x")
```

**Actividad 1.1**: Modifica las dimensiones del encoder (ej: latent_dim=32) y observa c√≥mo cambia la compresi√≥n.

**Pregunta de Reflexi√≥n 1.1**: ¬øQu√© informaci√≥n se podr√≠a perder al comprimir de 784 a 64 dimensiones?

### 1.3 Implementaci√≥n del Decoder

El decoder expande la representaci√≥n latente de vuelta a la dimensi√≥n original:

```python
class Decoder:
    """
    Decoder: Expande representaci√≥n latente a reconstrucci√≥n.
    """
    
    def __init__(self, latent_dim, hidden_dim, output_dim):
        """
        Args:
            latent_dim: Dimensi√≥n del espacio latente
            hidden_dim: Dimensi√≥n de capa oculta
            output_dim: Dimensi√≥n de salida (igual a input original)
        """
        self.W1 = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2.0 / latent_dim)
        self.b1 = np.zeros((hidden_dim, 1))
        
        self.W2 = np.random.randn(output_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)
        self.b2 = np.zeros((output_dim, 1))
        
        print(f"‚úÖ Decoder creado:")
        print(f"   {latent_dim} ‚Üí {hidden_dim} ‚Üí {output_dim}")
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def sigmoid(self, x):
        """Sigmoid para salida [0,1] (im√°genes normalizadas)."""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def forward(self, z):
        """
        Decodifica representaci√≥n latente.
        
        Args:
            z: (latent_dim, batch_size)
        
        Returns:
            X_recon: (output_dim, batch_size) - reconstrucci√≥n
        """
        # Capa 1: latent ‚Üí hidden
        self.h1 = self.relu(self.W1 @ z + self.b1)
        
        # Capa 2: hidden ‚Üí output (con sigmoid para [0,1])
        self.X_recon = self.sigmoid(self.W2 @ self.h1 + self.b2)
        
        return self.X_recon

# Crear decoder
decoder = Decoder(latent_dim=64, hidden_dim=256, output_dim=784)

# Decodificar la representaci√≥n latente anterior
X_recon = decoder.forward(z)

print(f"\nüìä Reconstrucci√≥n:")
print(f"   Latente: {z.shape}")
print(f"   Reconstrucci√≥n: {X_recon.shape}")
print(f"   Rango de valores: [{X_recon.min():.3f}, {X_recon.max():.3f}]")
```

**Actividad 1.2**: Verifica que la forma de `X_recon` sea igual a la de `X_test`.

### 1.4 Autoencoder Completo

Ahora combinamos encoder y decoder en un autoencoder completo:

```python
class SimpleAutoencoder:
    """
    Autoencoder simple: Encoder + Decoder.
    """
    
    def __init__(self, input_dim, hidden_dim, latent_dim):
        """
        Args:
            input_dim: Dimensi√≥n de entrada
            hidden_dim: Dimensi√≥n de capas ocultas
            latent_dim: Dimensi√≥n del espacio latente
        """
        print("\n" + "="*70)
        print("CREANDO AUTOENCODER")
        print("="*70)
        
        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)
        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)
        
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        
        print(f"\nüìä Arquitectura completa:")
        print(f"   {input_dim} ‚Üí {hidden_dim} ‚Üí {latent_dim} ‚Üí {hidden_dim} ‚Üí {input_dim}")
        print(f"   Ratio de compresi√≥n: {input_dim / latent_dim:.1f}x")
    
    def forward(self, X):
        """
        Forward pass completo: X ‚Üí z ‚Üí X'
        
        Args:
            X: (input_dim, batch_size)
        
        Returns:
            X_recon: Reconstrucci√≥n
            z: Representaci√≥n latente
        """
        z = self.encoder.forward(X)
        X_recon = self.decoder.forward(z)
        return X_recon, z
    
    def compute_loss(self, X, X_recon):
        """
        Calcula p√©rdida de reconstrucci√≥n (MSE).
        
        Args:
            X: Entrada original
            X_recon: Reconstrucci√≥n
        
        Returns:
            loss: Mean Squared Error
        """
        mse = np.mean((X - X_recon) ** 2)
        return mse

# Crear autoencoder
ae = SimpleAutoencoder(input_dim=784, hidden_dim=256, latent_dim=64)

# Forward pass
X_recon, z = ae.forward(X_test)
loss = ae.compute_loss(X_test, X_recon)

print(f"\nüîÑ Forward pass completo:")
print(f"   Entrada: {X_test.shape}")
print(f"   Latente: {z.shape}")
print(f"   Reconstrucci√≥n: {X_recon.shape}")
print(f"   P√©rdida (MSE): {loss:.6f}")
```

**Actividad 1.3**: Calcula la p√©rdida para diferentes tama√±os de espacio latente (32, 64, 128). ¬øC√≥mo afecta al error?

**Pregunta de Reflexi√≥n 1.2**: ¬øPor qu√© usamos MSE como funci√≥n de p√©rdida en lugar de cross-entropy?

### 1.5 Visualizaci√≥n con Datos Reales

Probemos el autoencoder con d√≠gitos MNIST reales:

```python
from sklearn.datasets import load_digits

def visualizar_autoencoder():
    """
    Visualiza reconstrucciones del autoencoder.
    """
    print("\n" + "="*70)
    print("PROBANDO CON D√çGITOS REALES")
    print("="*70)
    
    # Cargar datos de d√≠gitos (8√ó8 = 64)
    digits = load_digits()
    X = digits.data / 16.0  # Normalizar a [0, 1]
    X = X.T  # (64, n_samples)
    
    print(f"\nüì¶ Dataset cargado:")
    print(f"   Forma: {X.shape}")
    print(f"   Rango: [{X.min():.2f}, {X.max():.2f}]")
    
    # Crear autoencoder (ajustado para 8√ó8)
    ae_small = SimpleAutoencoder(input_dim=64, hidden_dim=32, latent_dim=16)
    
    # Reconstruir primeras muestras
    n_samples = 10
    X_batch = X[:, :n_samples]
    X_recon, z = ae_small.forward(X_batch)
    loss = ae_small.compute_loss(X_batch, X_recon)
    
    print(f"\nüìä Resultados (sin entrenar):")
    print(f"   P√©rdida: {loss:.6f}")
    
    # Visualizar originales vs reconstrucciones
    fig, axes = plt.subplots(2, n_samples, figsize=(15, 3))
    
    for i in range(n_samples):
        # Original
        img_orig = X[:, i].reshape(8, 8)
        axes[0, i].imshow(img_orig, cmap='gray', vmin=0, vmax=1)
        axes[0, i].axis('off')
        if i == 0:
            axes[0, i].set_title('Originales', fontsize=10, loc='left')
        
        # Reconstrucci√≥n
        img_recon = X_recon[:, i].reshape(8, 8)
        axes[1, i].imshow(img_recon, cmap='gray', vmin=0, vmax=1)
        axes[1, i].axis('off')
        if i == 0:
            axes[1, i].set_title('Reconstrucciones', fontsize=10, loc='left')
    
    plt.suptitle('Autoencoder Simple (Sin Entrenar)', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('autoencoder_reconstruccion.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"\nüí° Observaci√≥n:")
    print(f"   Las reconstrucciones son borrosas porque el autoencoder")
    print(f"   no est√° entrenado. En Lab 06 aprendimos a entrenar redes.")
    print(f"   Con entrenamiento, las reconstrucciones mejorar√≠an mucho.")

visualizar_autoencoder()
```

**Actividad 1.4**: Cambia `latent_dim` a 8 y a 32. ¬øC√≥mo afecta visualmente a las reconstrucciones?

### Actividades

**Actividad 1.5**: Implementa una funci√≥n `compression_ratio()` que calcule y retorne el ratio de compresi√≥n del autoencoder.

**Actividad 1.6**: Crea un autoencoder con 3 capas en el encoder y 3 en el decoder. ¬øMejora la reconstrucci√≥n?

**Actividad 1.7**: A√±ade un m√©todo `info()` al `SimpleAutoencoder` que imprima el n√∫mero total de par√°metros.

### Preguntas de Reflexi√≥n

**Pregunta 1.3 (Concebir)**: ¬øQu√© aplicaciones pr√°cticas tiene comprimir im√°genes a un espacio latente peque√±o?

**Pregunta 1.4 (Dise√±ar)**: Si quisieras comprimir 10x m√°s, ¬øcambiar√≠as el `latent_dim` o la arquitectura completa? ¬øPor qu√©?

**Pregunta 1.5 (Implementar)**: ¬øPor qu√© usamos sigmoid en la √∫ltima capa del decoder?

**Pregunta 1.6 (Operar)**: En un sistema de almacenamiento masivo de im√°genes, ¬øqu√© trade-offs considerar√≠as entre compresi√≥n y calidad?

---

## üî¨ Parte 2: Variational Autoencoder (VAE) (60 min)

### 2.1 De Autoencoder a VAE - El Salto Conceptual

**Problema con Autoencoders Simples**:
```
Autoencoder tradicional:
  X ‚Üí [Encoder] ‚Üí z (punto fijo) ‚Üí [Decoder] ‚Üí X'
  
  Problema: El espacio latente tiene "agujeros"
  - z‚ÇÅ = [0.5, 0.3] ‚Üí d√≠gito "3"
  - z‚ÇÇ = [0.5, 0.4] ‚Üí ¬ø? (podr√≠a ser basura)
  - No podemos generar muestras nuevas confiablemente
```

**Soluci√≥n VAE**:
```
VAE:
  X ‚Üí [Encoder] ‚Üí (Œº, œÉ) ‚Üí Sample z ~ N(Œº,œÉ¬≤) ‚Üí [Decoder] ‚Üí X'
  
  Ventaja: El espacio latente es continuo y completo
  - Cualquier z muestreado de N(0,1) ‚Üí imagen v√°lida
  - Podemos generar infinitas muestras nuevas
  - Interpolaci√≥n suave entre puntos
```

**Analog√≠a Detallada**:

Imagina que quieres comprimir la forma de escribir d√≠gitos:

- **Autoencoder**: Guarda la posici√≥n exacta de cada trazo
  - Problema: Solo puedes reproducir los d√≠gitos exactos que guardaste
  - Como tomar fotos de cada d√≠gito

- **VAE**: Aprende la "distribuci√≥n" de c√≥mo se escriben los d√≠gitos
  - Codifica: "Este '3' tiene un bucle superior de tama√±o medio (Œº=0.5) con variaci√≥n (œÉ=0.1)"
  - Puede generar infinitos "3" diferentes pero todos v√°lidos
  - Como aprender el "concepto" de c√≥mo escribir un "3"

### 2.2 Encoder Probabil√≠stico - Œº y œÉ

En un VAE, el encoder no produce un punto fijo z, sino par√°metros de una distribuci√≥n:

```python
class VAEEncoder:
    """
    Encoder para VAE: produce Œº (media) y log(œÉ¬≤) (log-varianza).
    """
    
    def __init__(self, input_dim, hidden_dim, latent_dim):
        """
        Args:
            input_dim: Dimensi√≥n de entrada
            hidden_dim: Dimensi√≥n de capa oculta compartida
            latent_dim: Dimensi√≥n del espacio latente
        """
        # Capa compartida
        self.W_shared = np.random.randn(hidden_dim, input_dim) * np.sqrt(2.0 / input_dim)
        self.b_shared = np.zeros((hidden_dim, 1))
        
        # Rama para Œº (media)
        self.W_mu = np.random.randn(latent_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)
        self.b_mu = np.zeros((latent_dim, 1))
        
        # Rama para log(œÉ¬≤) (log-varianza)
        self.W_logvar = np.random.randn(latent_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)
        self.b_logvar = np.zeros((latent_dim, 1))
        
        print(f"‚úÖ VAE Encoder creado:")
        print(f"   Entrada: {input_dim}")
        print(f"   Hidden: {hidden_dim}")
        print(f"   Latente: {latent_dim} (Œº y log-œÉ¬≤)")
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def forward(self, X):
        """
        Codifica entrada a distribuci√≥n latente.
        
        Args:
            X: (input_dim, batch_size)
        
        Returns:
            mu: (latent_dim, batch_size) - media
            logvar: (latent_dim, batch_size) - log-varianza
        """
        # Capa compartida
        h = self.relu(self.W_shared @ X + self.b_shared)
        
        # Ramificaci√≥n
        mu = self.W_mu @ h + self.b_mu
        logvar = self.W_logvar @ h + self.b_logvar
        
        return mu, logvar

# Ejemplo
print("\n" + "="*70)
print("2. VAE ENCODER - CODIFICACI√ìN PROBABIL√çSTICA")
print("="*70)

X_test = np.random.rand(64, 5)  # 5 im√°genes de 8√ó8
vae_encoder = VAEEncoder(input_dim=64, hidden_dim=32, latent_dim=16)

mu, logvar = vae_encoder.forward(X_test)

print(f"\nüìä Salidas del encoder:")
print(f"   Œº shape: {mu.shape}")
print(f"   log(œÉ¬≤) shape: {logvar.shape}")
print(f"\n   Ejemplo para muestra 0:")
print(f"   Œº‚ÇÄ = {mu[:5, 0]}...")  # Primeras 5 dims
print(f"   log(œÉ¬≤)‚ÇÄ = {logvar[:5, 0]}...")
```

**Actividad 2.1**: Calcula œÉ = exp(0.5 √ó logvar) manualmente para la primera muestra. ¬øQu√© valores obtienes?

**Pregunta de Reflexi√≥n 2.1**: ¬øPor qu√© parametrizamos log(œÉ¬≤) en lugar de œÉ directamente?

### 2.3 Reparameterization Trick - La Clave del VAE

**El Problema**:
```python
# ‚ùå NO FUNCIONA - No podemos hacer backpropagation
z = np.random.normal(mu, sigma)  # Sampling rompe el flujo de gradientes
```

**La Soluci√≥n Elegante**:
```python
# ‚úÖ FUNCIONA - Reparameterization trick
epsilon = np.random.normal(0, 1, size=mu.shape)  # Ruido est√°ndar
z = mu + sigma * epsilon  # Ahora Œº y œÉ reciben gradientes
```

Implementaci√≥n:

```python
def reparameterize(mu, logvar):
    """
    Reparameterization trick: permite backpropagation a trav√©s de sampling.
    
    z = Œº + œÉ √ó Œµ, donde Œµ ~ N(0,1)
    
    Args:
        mu: (latent_dim, batch_size) - media
        logvar: (latent_dim, batch_size) - log-varianza
    
    Returns:
        z: (latent_dim, batch_size) - muestras latentes
    """
    # Calcular desviaci√≥n est√°ndar desde log-varianza
    std = np.exp(0.5 * logvar)
    
    # Muestrear ruido de distribuci√≥n normal est√°ndar
    epsilon = np.random.randn(*mu.shape)
    
    # Reparameterizar: z = Œº + œÉ √ó Œµ
    z = mu + std * epsilon
    
    return z

# Demostraci√≥n
print("\n" + "="*70)
print("3. REPARAMETERIZATION TRICK")
print("="*70)

# Usar Œº y logvar del ejemplo anterior
z = reparameterize(mu, logvar)

print(f"\nüìä Muestreo latente:")
print(f"   Input: Œº={mu.shape}, log(œÉ¬≤)={logvar.shape}")
print(f"   Output: z={z.shape}")

print(f"\n   Estad√≠sticas de z:")
print(f"   Media: {z.mean():.4f} (deber√≠a estar cerca de 0)")
print(f"   Std: {z.std():.4f} (deber√≠a estar cerca de 1)")

# Visualizar distribuci√≥n
print(f"\nüí° Explicaci√≥n:")
print(f"   1. Calculamos œÉ = exp(0.5 √ó log(œÉ¬≤))")
print(f"   2. Muestreamos Œµ ~ N(0,1)")
print(f"   3. Calculamos z = Œº + œÉ √ó Œµ")
print(f"   4. Ahora los gradientes fluyen a trav√©s de Œº y œÉ!")
```

**Actividad 2.2**: Genera 1000 muestras de z usando el mismo Œº y logvar. Grafica su distribuci√≥n.

**Pregunta de Reflexi√≥n 2.2**: ¬øPor qu√© este "truco" permite hacer backpropagation?

### 2.4 VAE Decoder - Igual que Antes

El decoder del VAE es id√©ntico al del autoencoder simple:

```python
class VAEDecoder:
    """
    Decoder para VAE: igual que autoencoder normal.
    """
    
    def __init__(self, latent_dim, hidden_dim, output_dim):
        self.W1 = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2.0 / latent_dim)
        self.b1 = np.zeros((hidden_dim, 1))
        
        self.W2 = np.random.randn(output_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)
        self.b2 = np.zeros((output_dim, 1))
        
        print(f"‚úÖ VAE Decoder creado:")
        print(f"   {latent_dim} ‚Üí {hidden_dim} ‚Üí {output_dim}")
    
    def relu(self, x):
        return np.maximum(0, x)
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def forward(self, z):
        """
        Decodifica z a reconstrucci√≥n.
        
        Args:
            z: (latent_dim, batch_size)
        
        Returns:
            X_recon: (output_dim, batch_size)
        """
        h = self.relu(self.W1 @ z + self.b1)
        X_recon = self.sigmoid(self.W2 @ h + self.b2)
        return X_recon

vae_decoder = VAEDecoder(latent_dim=16, hidden_dim=32, output_dim=64)
X_recon = vae_decoder.forward(z)

print(f"\nüìä Reconstrucci√≥n:")
print(f"   z: {z.shape} ‚Üí X_recon: {X_recon.shape}")
```

### 2.5 Funci√≥n de P√©rdida VAE - Dos Componentes

**La p√©rdida del VAE combina dos objetivos**:

```python
P√©rdida_Total = Reconstrucci√≥n + Œ≤ √ó KL_Divergence

1. Reconstrucci√≥n: ¬øQu√© tan bien reconstruimos X?
2. KL Divergence: ¬øQu√© tan "normal" es nuestro espacio latente?
```

Implementaci√≥n detallada:

```python
def vae_loss(X, X_recon, mu, logvar, beta=1.0):
    """
    Calcula la p√©rdida completa del VAE.
    
    Loss = Reconstruction + Œ≤ √ó KL_Divergence
    
    Args:
        X: (input_dim, batch_size) - entrada original
        X_recon: (input_dim, batch_size) - reconstrucci√≥n
        mu: (latent_dim, batch_size) - media latente
        logvar: (latent_dim, batch_size) - log-varianza latente
        beta: peso de KL divergence (t√≠picamente 1.0)
    
    Returns:
        total_loss: p√©rdida total
        recon_loss: p√©rdida de reconstrucci√≥n
        kl_loss: KL divergence
    """
    batch_size = X.shape[1]
    
    # 1. P√©rdida de Reconstrucci√≥n (Binary Cross-Entropy)
    epsilon = 1e-10  # Para estabilidad num√©rica
    recon_loss = -np.sum(
        X * np.log(X_recon + epsilon) + 
        (1 - X) * np.log(1 - X_recon + epsilon)
    ) / batch_size
    
    # 2. KL Divergence
    # KL(N(Œº,œÉ¬≤) || N(0,1)) = -0.5 √ó Œ£(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)
    kl_loss = -0.5 * np.sum(1 + logvar - mu**2 - np.exp(logvar)) / batch_size
    
    # 3. P√©rdida Total
    total_loss = recon_loss + beta * kl_loss
    
    return total_loss, recon_loss, kl_loss

# Calcular p√©rdida
print("\n" + "="*70)
print("4. FUNCI√ìN DE P√âRDIDA VAE")
print("="*70)

total_loss, recon_loss, kl_loss = vae_loss(X_test, X_recon, mu, logvar)

print(f"\nüìä Componentes de p√©rdida (sin entrenar):")
print(f"   Reconstrucci√≥n: {recon_loss:.4f}")
print(f"   KL Divergence:  {kl_loss:.4f}")
print(f"   TOTAL:          {total_loss:.4f}")

print(f"\nüí° Interpretaci√≥n:")
print(f"   - Reconstrucci√≥n: qu√© tan bien recuperamos la entrada")
print(f"   - KL Divergence: qu√© tan cerca est√° q(z|X) de N(0,1)")
print(f"   - Balance: trade-off entre fidelidad y regularizaci√≥n")
```

**Actividad 2.3**: Var√≠a Œ≤ de 0.1 a 10. ¬øC√≥mo cambia el balance entre las dos p√©rdidas?

**Pregunta de Reflexi√≥n 2.3**: ¬øQu√© pasar√≠a si Œ≤=0? ¬øY si Œ≤=1000?

### 2.6 VAE Completo - Todo Junto

Ahora juntamos todas las piezas:

```python
class VariationalAutoencoder:
    """
    Variational Autoencoder completo.
    """
    
    def __init__(self, input_dim, hidden_dim, latent_dim):
        """
        Args:
            input_dim: Dimensi√≥n de entrada
            hidden_dim: Dimensi√≥n de capas ocultas
            latent_dim: Dimensi√≥n del espacio latente
        """
        print("\n" + "="*70)
        print("CREANDO VARIATIONAL AUTOENCODER (VAE)")
        print("="*70)
        
        self.encoder = VAEEncoder(input_dim, hidden_dim, latent_dim)
        self.decoder = VAEDecoder(latent_dim, hidden_dim, input_dim)
        
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        
        print(f"\nüìä Arquitectura VAE:")
        print(f"   Input: {input_dim}")
        print(f"   ‚Üí Encoder ‚Üí (Œº, log-œÉ¬≤): {latent_dim}")
        print(f"   ‚Üí Reparameterize ‚Üí z: {latent_dim}")
        print(f"   ‚Üí Decoder ‚Üí Output: {input_dim}")
    
    def forward(self, X):
        """
        Forward pass completo del VAE.
        
        Args:
            X: (input_dim, batch_size)
        
        Returns:
            X_recon: reconstrucci√≥n
            mu: media latente
            logvar: log-varianza latente
        """
        # Encoder: X ‚Üí (Œº, log-œÉ¬≤)
        mu, logvar = self.encoder.forward(X)
        
        # Reparameterization trick: (Œº, log-œÉ¬≤) ‚Üí z
        z = reparameterize(mu, logvar)
        
        # Decoder: z ‚Üí X'
        X_recon = self.decoder.forward(z)
        
        return X_recon, mu, logvar
    
    def compute_loss(self, X, X_recon, mu, logvar, beta=1.0):
        """Calcula p√©rdida VAE."""
        return vae_loss(X, X_recon, mu, logvar, beta)
    
    def generate(self, num_samples):
        """
        Genera nuevas muestras muestreando z ~ N(0,1).
        
        Args:
            num_samples: n√∫mero de muestras a generar
        
        Returns:
            X_generated: muestras generadas
        """
        # Muestrear z desde prior N(0,1)
        z = np.random.randn(self.latent_dim, num_samples)
        
        # Decodificar
        X_generated = self.decoder.forward(z)
        
        return X_generated

# Crear VAE completo
vae = VariationalAutoencoder(input_dim=64, hidden_dim=32, latent_dim=16)

# Forward pass
X_recon_vae, mu_vae, logvar_vae = vae.forward(X_test)
total, recon, kl = vae.compute_loss(X_test, X_recon_vae, mu_vae, logvar_vae)

print(f"\nüîÑ Test forward pass:")
print(f"   Input: {X_test.shape}")
print(f"   Output: {X_recon_vae.shape}")
print(f"   P√©rdida total: {total:.4f}")
```

**Actividad 2.4**: Crea un VAE con `latent_dim=8`. ¬øEs m√°s dif√≠cil reconstruir con menos dimensiones?

### 2.7 Exploraci√≥n del Espacio Latente

Una de las caracter√≠sticas m√°s poderosas del VAE es su espacio latente continuo:

```python
def explorar_espacio_latente():
    """
    Explora el espacio latente del VAE.
    """
    from sklearn.datasets import load_digits
    
    print("\n" + "="*70)
    print("5. EXPLORACI√ìN DEL ESPACIO LATENTE")
    print("="*70)
    
    # Cargar datos
    digits = load_digits()
    X = digits.data / 16.0
    X = X.T  # (64, n_samples)
    y = digits.target
    
    # Crear VAE con latent_dim=2 para visualizaci√≥n
    vae_viz = VariationalAutoencoder(input_dim=64, hidden_dim=32, latent_dim=2)
    
    # Encodear todos los d√≠gitos
    mu_all, logvar_all = vae_viz.encoder.forward(X)
    
    # Visualizar espacio latente
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(mu_all[0, :], mu_all[1, :], 
                         c=y, cmap='tab10', 
                         alpha=0.6, s=30, edgecolors='black', linewidth=0.5)
    plt.colorbar(scatter, label='D√≠gito')
    plt.xlabel('Dimensi√≥n Latente 1', fontsize=12)
    plt.ylabel('Dimensi√≥n Latente 2', fontsize=12)
    plt.title('Espacio Latente del VAE (2D)', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)
    plt.axvline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)
    plt.tight_layout()
    plt.savefig('vae_latent_space.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"\nüí° Observaciones:")
    print(f"   - Cada color representa un d√≠gito diferente")
    print(f"   - D√≠gitos similares est√°n cercanos en el espacio latente")
    print(f"   - El espacio es continuo (no hay saltos bruscos)")
    print(f"   - Con entrenamiento, las separaciones ser√≠an m√°s claras")

explorar_espacio_latente()
```

**Actividad 2.5**: Modifica para usar `latent_dim=3` y haz una visualizaci√≥n 3D.

### 2.8 Generaci√≥n de Nuevas Muestras

La ventaja del VAE: podemos generar muestras completamente nuevas:

```python
def generar_nuevas_muestras():
    """
    Genera d√≠gitos completamente nuevos.
    """
    print("\n" + "="*70)
    print("6. GENERACI√ìN DE NUEVAS MUESTRAS")
    print("="*70)
    
    # Usar el VAE creado anteriormente
    num_samples = 16
    
    print(f"\nüé® Generando {num_samples} d√≠gitos nuevos...")
    X_generated = vae.generate(num_samples)
    
    # Visualizar
    fig, axes = plt.subplots(4, 4, figsize=(10, 10))
    axes = axes.ravel()
    
    for i in range(num_samples):
        img = X_generated[:, i].reshape(8, 8)
        axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)
        axes[i].axis('off')
        axes[i].set_title(f'Muestra {i+1}', fontsize=9)
    
    plt.suptitle('D√≠gitos Generados por VAE (sin entrenar)', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('vae_generated_samples.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"\nüí° Nota importante:")
    print(f"   Estas muestras se ven aleatorias porque el VAE no est√° entrenado.")
    print(f"   Con entrenamiento (usando backpropagation del Lab 05),")
    print(f"   generar√≠a d√≠gitos realistas y variados.")

generar_nuevas_muestras()
```

**Actividad 2.6**: Genera 100 muestras. ¬øHay alguna que se parezca a un d√≠gito por casualidad?

### 2.9 Interpolaci√≥n en el Espacio Latente

Podemos crear transiciones suaves entre dos d√≠gitos:

```python
def interpolar_digitos():
    """
    Interpola entre dos d√≠gitos en el espacio latente.
    """
    from sklearn.datasets import load_digits
    
    print("\n" + "="*70)
    print("7. INTERPOLACI√ìN EN ESPACIO LATENTE")
    print("="*70)
    
    # Cargar datos
    digits = load_digits()
    X = digits.data / 16.0
    X = X.T
    
    # Elegir dos d√≠gitos (ej: √≠ndices 0 y 10)
    idx1, idx2 = 0, 100
    x1 = X[:, idx1:idx1+1]
    x2 = X[:, idx2:idx2+1]
    
    # Encodear a espacio latente
    mu1, logvar1 = vae.encoder.forward(x1)
    mu2, logvar2 = vae.encoder.forward(x2)
    
    # Interpolar (10 pasos)
    n_steps = 10
    alphas = np.linspace(0, 1, n_steps)
    
    interpolated = []
    for alpha in alphas:
        # Interpolaci√≥n lineal en espacio latente
        z_interp = (1 - alpha) * mu1 + alpha * mu2
        
        # Decodificar
        x_interp = vae.decoder.forward(z_interp)
        interpolated.append(x_interp)
    
    # Visualizar
    fig, axes = plt.subplots(1, n_steps, figsize=(15, 2))
    
    for i, x in enumerate(interpolated):
        img = x.reshape(8, 8)
        axes[i].imshow(img, cmap='gray', vmin=0, vmax=1)
        axes[i].axis('off')
        axes[i].set_title(f'Œ±={alphas[i]:.1f}', fontsize=9)
    
    plt.suptitle(f'Interpolaci√≥n: D√≠gito {digits.target[idx1]} ‚Üí D√≠gito {digits.target[idx2]}', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('vae_interpolation.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"\nüí° Interpretaci√≥n:")
    print(f"   Cada paso muestra una mezcla gradual entre los dos d√≠gitos.")
    print(f"   Con VAE entrenado, la transici√≥n ser√≠a suave y realista.")
    print(f"   Esto demuestra la continuidad del espacio latente.")

interpolar_digitos()
```

**Actividad 2.7**: Interpola entre tres d√≠gitos diferentes usando interpolaci√≥n esf√©rica en lugar de lineal.

### Actividades Integradoras

**Actividad 2.8**: Implementa una funci√≥n `encode_decode_test()` que codifique y decodifique 100 im√°genes y mida el error promedio.

**Actividad 2.9**: Crea una visualizaci√≥n 2D del espacio latente mostrando reconstrucciones en una cuadr√≠cula (latent space walk).

**Actividad 2.10**: Implementa `beta_vae()` que permita variar Œ≤ y observa c√≥mo afecta a las reconstrucciones vs regularizaci√≥n.

**Actividad 2.11**: Compara VAE con Autoencoder simple: ¬øcu√°l genera mejores muestras nuevas?

### Preguntas de Reflexi√≥n

**Pregunta 2.4 (Concebir)**: ¬øC√≥mo podr√≠as usar un VAE para detectar im√°genes an√≥malas o "fuera de distribuci√≥n"?

**Pregunta 2.5 (Dise√±ar)**: Si quisieras generar d√≠gitos de un n√∫mero espec√≠fico (ej: solo "7"), ¬øc√≥mo modificar√≠as la arquitectura?

**Pregunta 2.6 (Implementar)**: ¬øPor qu√© KL divergence fuerza el espacio latente a ser N(0,1)? ¬øQu√© pasar√≠a sin esta regularizaci√≥n?

**Pregunta 2.7 (Operar)**: En una aplicaci√≥n de generaci√≥n de rostros, ¬øqu√© dimensi√≥n de espacio latente recomendar√≠as y por qu√©?

---

## üî¨ Parte 3: Generative Adversarial Networks (GAN) (45 min)

### 3.1 Introducci√≥n al Entrenamiento Adversarial

**GAN: Un Juego de Dos Jugadores**

Imagina dos redes neuronales compitiendo:

- **Generador (G)**: Intenta crear im√°genes falsas que parezcan reales
  - Como un falsificador de billetes
  - Input: ruido aleatorio z
  - Output: imagen falsa

- **Discriminador (D)**: Intenta distinguir im√°genes reales de falsas
  - Como un detective de billetes falsos
  - Input: imagen (real o falsa)
  - Output: probabilidad de ser real [0, 1]

**El Proceso de Competici√≥n**:

```
Ronda 1:
  G crea billetes falsos malos ‚Üí D detecta f√°cilmente
  
Ronda 2:
  D mejora su detecci√≥n ‚Üí G ajusta su t√©cnica
  
Ronda 3:
  G crea billetes mejores ‚Üí D se vuelve m√°s experto
  
...

Ronda N:
  G crea billetes casi perfectos ‚Üî D apenas puede distinguir
  
¬°EQUILIBRIO! ‚Üí G genera im√°genes realistas
```

**Arquitectura Visual**:
```
Generador:
  z (ruido) ‚Üí [NN] ‚Üí imagen falsa
  (100,)    ‚Üí Dense ‚Üí (784,)
  
Discriminador:
  imagen ‚Üí [NN] ‚Üí probabilidad
  (784,) ‚Üí Dense ‚Üí (1,)  # 0=falso, 1=real
  
Entrenamiento:
  1. Entrenar D: maximizar D(real), minimizar D(fake)
  2. Entrenar G: maximizar D(fake) (enga√±ar a D)
  3. Repetir alternadamente
```

### 3.2 Implementaci√≥n del Generador

El generador transforma ruido aleatorio en im√°genes:

```python
class Generator:
    """
    Generador de GAN: ruido ‚Üí imagen falsa.
    """
    
    def __init__(self, latent_dim, hidden_dim, output_dim):
        """
        Args:
            latent_dim: Dimensi√≥n del ruido de entrada (ej: 100)
            hidden_dim: Dimensi√≥n de capas ocultas (ej: 128)
            output_dim: Dimensi√≥n de salida (ej: 784 para MNIST)
        """
        # Capa 1
        self.W1 = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2.0 / latent_dim)
        self.b1 = np.zeros((hidden_dim, 1))
        
        # Capa 2
        self.W2 = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)
        self.b2 = np.zeros((hidden_dim, 1))
        
        # Capa de salida
        self.W3 = np.random.randn(output_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)
        self.b3 = np.zeros((output_dim, 1))
        
        print(f"‚úÖ Generador creado:")
        print(f"   {latent_dim} ‚Üí {hidden_dim} ‚Üí {hidden_dim} ‚Üí {output_dim}")
        
        self.latent_dim = latent_dim
    
    def leaky_relu(self, x, alpha=0.2):
        """LeakyReLU: permite gradientes negativos peque√±os."""
        return np.where(x > 0, x, alpha * x)
    
    def tanh(self, x):
        """Tanh: salida en [-1, 1]."""
        return np.tanh(x)
    
    def forward(self, z):
        """
        Genera im√°genes falsas desde ruido.
        
        Args:
            z: (latent_dim, batch_size) - ruido
        
        Returns:
            fake_images: (output_dim, batch_size) - en rango [-1, 1]
        """
        # Capa 1
        h1 = self.leaky_relu(self.W1 @ z + self.b1)
        
        # Capa 2
        h2 = self.leaky_relu(self.W2 @ h1 + self.b2)
        
        # Capa de salida con tanh para [-1, 1]
        fake_images = self.tanh(self.W3 @ h2 + self.b3)
        
        return fake_images
    
    def generate_noise(self, batch_size):
        """
        Genera ruido aleatorio para el generador.
        
        Args:
            batch_size: n√∫mero de muestras de ruido
        
        Returns:
            z: (latent_dim, batch_size)
        """
        return np.random.randn(self.latent_dim, batch_size)

# Crear generador
print("\n" + "="*70)
print("1. GENERADOR DE GAN")
print("="*70)

generator = Generator(latent_dim=100, hidden_dim=128, output_dim=64)

# Generar muestras falsas
z = generator.generate_noise(batch_size=5)
fake_images = generator.forward(z)

print(f"\nüìä Generaci√≥n:")
print(f"   Ruido z: {z.shape}")
print(f"   Im√°genes falsas: {fake_images.shape}")
print(f"   Rango: [{fake_images.min():.3f}, {fake_images.max():.3f}]")
```

**Actividad 3.1**: Genera 100 im√°genes falsas y visualiza algunas. ¬øSe parecen a d√≠gitos (sin entrenar)?

**Pregunta de Reflexi√≥n 3.1**: ¬øPor qu√© usamos tanh en la √∫ltima capa en lugar de sigmoid?

### 3.3 Implementaci√≥n del Discriminador

El discriminador clasifica im√°genes como reales o falsas:

```python
class Discriminator:
    """
    Discriminador de GAN: imagen ‚Üí probabilidad de ser real.
    """
    
    def __init__(self, input_dim, hidden_dim):
        """
        Args:
            input_dim: Dimensi√≥n de entrada (ej: 784)
            hidden_dim: Dimensi√≥n de capas ocultas (ej: 128)
        """
        # Capa 1
        self.W1 = np.random.randn(hidden_dim, input_dim) * np.sqrt(2.0 / input_dim)
        self.b1 = np.zeros((hidden_dim, 1))
        
        # Capa 2
        self.W2 = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)
        self.b2 = np.zeros((hidden_dim, 1))
        
        # Capa de salida (clasificaci√≥n binaria)
        self.W3 = np.random.randn(1, hidden_dim) * np.sqrt(2.0 / hidden_dim)
        self.b3 = np.zeros((1, 1))
        
        print(f"‚úÖ Discriminador creado:")
        print(f"   {input_dim} ‚Üí {hidden_dim} ‚Üí {hidden_dim} ‚Üí 1")
    
    def leaky_relu(self, x, alpha=0.2):
        return np.where(x > 0, x, alpha * x)
    
    def sigmoid(self, x):
        """Sigmoid: salida en [0, 1] (probabilidad)."""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def forward(self, images):
        """
        Discrimina si las im√°genes son reales o falsas.
        
        Args:
            images: (input_dim, batch_size)
        
        Returns:
            probs: (1, batch_size) - probabilidad de ser real
        """
        # Capa 1
        h1 = self.leaky_relu(self.W1 @ images + self.b1)
        
        # Capa 2
        h2 = self.leaky_relu(self.W2 @ h1 + self.b2)
        
        # Capa de salida con sigmoid
        probs = self.sigmoid(self.W3 @ h2 + self.b3)
        
        return probs

# Crear discriminador
print("\n" + "="*70)
print("2. DISCRIMINADOR DE GAN")
print("="*70)

discriminator = Discriminator(input_dim=64, hidden_dim=128)

# Probar con im√°genes falsas
probs_fake = discriminator.forward(fake_images)

print(f"\nüìä Discriminaci√≥n (im√°genes falsas):")
print(f"   Input: {fake_images.shape}")
print(f"   Output probabilities: {probs_fake.shape}")
print(f"   Probabilidades: {probs_fake.ravel()}")
print(f"\nüí° Interpretaci√≥n:")
print(f"   Valores cercanos a 0 = imagen falsa")
print(f"   Valores cercanos a 1 = imagen real")
print(f"   Sin entrenar, las probabilidades son aleatorias (~0.5)")
```

**Actividad 3.2**: Crea im√°genes "reales" (de MNIST) y compara las probabilidades del discriminador con las de im√°genes falsas.

### 3.4 Funciones de P√©rdida GAN

**P√©rdida del Discriminador**:
```python
Objetivo: Maximizar D(real) y minimizar D(fake)
P√©rdida_D = -[log(D(real)) + log(1 - D(fake))]
```

**P√©rdida del Generador**:
```python
Objetivo: Maximizar D(fake) para enga√±ar a D
P√©rdida_G = -log(D(fake))
```

Implementaci√≥n:

```python
def discriminator_loss(D_real, D_fake):
    """
    P√©rdida del discriminador (Binary Cross-Entropy).
    
    Args:
        D_real: probabilidades para im√°genes reales
        D_fake: probabilidades para im√°genes falsas
    
    Returns:
        loss: p√©rdida del discriminador
    """
    epsilon = 1e-10  # Estabilidad num√©rica
    
    # Maximizar D(real) ‚Üí minimizar -log(D(real))
    loss_real = -np.mean(np.log(D_real + epsilon))
    
    # Minimizar D(fake) ‚Üí minimizar -log(1 - D(fake))
    loss_fake = -np.mean(np.log(1 - D_fake + epsilon))
    
    # P√©rdida total
    loss = loss_real + loss_fake
    
    return loss, loss_real, loss_fake

def generator_loss(D_fake):
    """
    P√©rdida del generador.
    
    Args:
        D_fake: probabilidades del discriminador para im√°genes falsas
    
    Returns:
        loss: p√©rdida del generador
    """
    epsilon = 1e-10
    
    # Maximizar D(fake) ‚Üí minimizar -log(D(fake))
    loss = -np.mean(np.log(D_fake + epsilon))
    
    return loss

# Ejemplo de c√°lculo
print("\n" + "="*70)
print("3. FUNCIONES DE P√âRDIDA GAN")
print("="*70)

from sklearn.datasets import load_digits

# Cargar im√°genes reales
digits = load_digits()
real_images = (digits.data / 8.0 - 1.0).T[:, :5]  # Normalizar a [-1, 1]

# Discriminar reales y falsas
D_real = discriminator.forward(real_images)
D_fake = discriminator.forward(fake_images)

# Calcular p√©rdidas
d_loss, d_loss_real, d_loss_fake = discriminator_loss(D_real, D_fake)
g_loss = generator_loss(D_fake)

print(f"\nüìä P√©rdidas (sin entrenar):")
print(f"   Discriminador:")
print(f"     - P√©rdida real:  {d_loss_real:.4f}")
print(f"     - P√©rdida fake:  {d_loss_fake:.4f}")
print(f"     - TOTAL:         {d_loss:.4f}")
print(f"\n   Generador:")
print(f"     - P√©rdida:       {g_loss:.4f}")

print(f"\nüí° Objetivo del entrenamiento:")
print(f"   - D intenta minimizar d_loss")
print(f"   - G intenta minimizar g_loss")
print(f"   - Convergen cuando D(real)‚âà1 y D(fake)‚âà0.5 (equilibrio)")
```

**Actividad 3.3**: Calcula las p√©rdidas con diferentes probabilidades manualmente para entender c√≥mo funcionan.

**Pregunta de Reflexi√≥n 3.2**: ¬øPor qu√© la meta de G es que D(fake)‚âà0.5 y no D(fake)‚âà1?

### 3.5 Loop de Entrenamiento GAN (Conceptual)

Aunque no implementaremos backpropagation completo aqu√≠ (eso fue Lab 05), veamos la estructura del entrenamiento:

```python
def entrenar_gan_conceptual(generator, discriminator, real_images, 
                            n_epochs=100, batch_size=32):
    """
    Esquema conceptual del entrenamiento de GAN.
    
    NOTA: Esta es una versi√≥n simplificada sin backpropagation.
    Para implementaci√≥n completa, usar PyTorch/TensorFlow.
    """
    print("\n" + "="*70)
    print("4. ESQUEMA DE ENTRENAMIENTO GAN")
    print("="*70)
    
    print(f"\nüìö Proceso de entrenamiento:")
    print(f"   Total √©pocas: {n_epochs}")
    print(f"   Batch size: {batch_size}")
    
    # Simulaci√≥n del proceso
    for epoch in range(5):  # Solo 5 para demo
        print(f"\n--- √âpoca {epoch+1} ---")
        
        # Paso 1: Entrenar Discriminador
        print("  üîµ Entrenando Discriminador:")
        
        # 1a. Forward pass en im√°genes reales
        batch_real = real_images[:, :batch_size]
        D_real = discriminator.forward(batch_real)
        
        # 1b. Generar im√°genes falsas
        z = generator.generate_noise(batch_size)
        batch_fake = generator.forward(z)
        D_fake = discriminator.forward(batch_fake)
        
        # 1c. Calcular p√©rdida de D
        d_loss, d_real, d_fake = discriminator_loss(D_real, D_fake)
        
        print(f"     D(real): {D_real.mean():.3f}, D(fake): {D_fake.mean():.3f}")
        print(f"     P√©rdida D: {d_loss:.4f}")
        
        # 1d. [BACKPROP AQU√ç] Actualizar pesos de D
        # discriminator.pesos -= learning_rate * gradientes
        
        # Paso 2: Entrenar Generador
        print("  üî¥ Entrenando Generador:")
        
        # 2a. Generar nuevas im√°genes falsas
        z = generator.generate_noise(batch_size)
        batch_fake = generator.forward(z)
        
        # 2b. Obtener predicci√≥n de D (sin actualizar D)
        D_fake_for_G = discriminator.forward(batch_fake)
        
        # 2c. Calcular p√©rdida de G
        g_loss = generator_loss(D_fake_for_G)
        
        print(f"     D(fake): {D_fake_for_G.mean():.3f}")
        print(f"     P√©rdida G: {g_loss:.4f}")
        
        # 2d. [BACKPROP AQU√ç] Actualizar pesos de G
        # generator.pesos -= learning_rate * gradientes
    
    print(f"\nüí° Notas importantes:")
    print(f"   1. D y G se entrenan alternadamente")
    print(f"   2. D entrena primero (necesita ser un buen juez)")
    print(f"   3. G se entrena manteniendo D fijo")
    print(f"   4. El balance entre D y G es crucial")
    print(f"   5. En pr√°ctica, usar frameworks con autograd (PyTorch/TF)")

# Ejecutar demo
entrenar_gan_conceptual(generator, discriminator, real_images)
```

**Actividad 3.4**: Dibuja un diagrama del flujo de entrenamiento GAN mostrando cu√°ndo se actualizan D y G.

### 3.6 Problemas Comunes en GANs

```python
def demostrar_problemas_gan():
    """
    Ilustra problemas comunes en el entrenamiento de GANs.
    """
    print("\n" + "="*70)
    print("5. PROBLEMAS COMUNES EN GANS")
    print("="*70)
    
    print(f"\n‚ö†Ô∏è  1. MODE COLLAPSE")
    print(f"   S√≠ntoma: G genera solo unas pocas variaciones")
    print(f"   Ejemplo: Solo genera el d√≠gito '1' repetidamente")
    print(f"   Causa: G encuentra un 'truco' para enga√±ar a D")
    print(f"   Soluci√≥n: Minibatch discrimination, Unrolled GAN")
    
    print(f"\n‚ö†Ô∏è  2. VANISHING GRADIENTS")
    print(f"   S√≠ntoma: D se vuelve demasiado bueno, G no aprende")
    print(f"   Ejemplo: D siempre dice 0 para fake ‚Üí log(0) = -‚àû")
    print(f"   Causa: D discrimina perfectamente muy r√°pido")
    print(f"   Soluci√≥n: Gradient penalty (WGAN-GP), ajustar learning rates")
    
    print(f"\n‚ö†Ô∏è  3. INESTABILIDAD")
    print(f"   S√≠ntoma: P√©rdidas oscilan violentamente")
    print(f"   Ejemplo: D loss oscila entre 0.1 y 5.0")
    print(f"   Causa: Balance incorrecto entre D y G")
    print(f"   Soluci√≥n: Ajustar learning rates, arquitecturas (DCGAN)")
    
    print(f"\n‚ö†Ô∏è  4. CONVERGENCIA LENTA")
    print(f"   S√≠ntoma: Requiere miles de √©pocas")
    print(f"   Ejemplo: Im√°genes siguen borrosas despu√©s de 100 √©pocas")
    print(f"   Causa: Problema dif√≠cil, espacio de b√∫squeda enorme")
    print(f"   Soluci√≥n: Paciencia, mejor arquitectura, pre-training")
    
    # Visualizar mode collapse
    print(f"\nüé® Simulaci√≥n de Mode Collapse:")
    
    # Generar m√∫ltiples muestras
    z = generator.generate_noise(batch_size=20)
    fakes = generator.forward(z)
    
    # Calcular varianza (baja varianza = mode collapse)
    varianza = np.var(fakes, axis=1).mean()
    
    print(f"   Varianza promedio: {varianza:.6f}")
    print(f"   {'‚úÖ Diversidad saludable' if varianza > 0.01 else '‚ùå Posible mode collapse'}")

demostrar_problemas_gan()
```

**Actividad 3.5**: Investiga qu√© es WGAN-GP y c√≥mo mejora la estabilidad del entrenamiento.

### 3.7 Visualizaci√≥n de Progreso

```python
def visualizar_progreso_gan():
    """
    Visualiza el progreso del GAN durante entrenamiento (simulado).
    """
    print("\n" + "="*70)
    print("6. VISUALIZACI√ìN DE PROGRESO")
    print("="*70)
    
    # Simular √©pocas de entrenamiento
    epocas = [0, 10, 50, 100]
    
    fig, axes = plt.subplots(len(epocas), 5, figsize=(12, 10))
    
    for i, epoca in enumerate(epocas):
        # Simular mejora progresiva (solo para demo)
        # En realidad, necesitar√≠as guardar checkpoints durante entrenamiento
        
        z = generator.generate_noise(batch_size=5)
        fakes = generator.forward(z)
        
        # A√±adir ruido que disminuye con las √©pocas (simula mejora)
        noise_level = 1.0 - (epoca / 100)
        fakes_noisy = fakes + np.random.randn(*fakes.shape) * noise_level * 0.3
        fakes_noisy = np.clip(fakes_noisy, -1, 1)
        
        for j in range(5):
            img = (fakes_noisy[:, j] + 1) / 2  # Convertir [-1,1] a [0,1]
            img = img.reshape(8, 8)
            
            axes[i, j].imshow(img, cmap='gray', vmin=0, vmax=1)
            axes[i, j].axis('off')
            
            if j == 0:
                axes[i, j].set_ylabel(f'√âpoca {epoca}', fontsize=10)
    
    plt.suptitle('Progreso del Generador (Simulado)', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('gan_progress.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"\nüí° En entrenamiento real:")
    print(f"   - √âpoca 0: Ruido aleatorio")
    print(f"   - √âpoca 10: Formas borrosas")
    print(f"   - √âpoca 50: D√≠gitos reconocibles")
    print(f"   - √âpoca 100: D√≠gitos claros y variados")

visualizar_progreso_gan()
```

### 3.8 Estrategias de Entrenamiento

```python
def estrategias_entrenamiento_gan():
    """
    Mejores pr√°cticas para entrenar GANs.
    """
    print("\n" + "="*70)
    print("7. ESTRATEGIAS DE ENTRENAMIENTO")
    print("="*70)
    
    estrategias = {
        "1. Label Smoothing": {
            "desc": "Usar etiquetas 0.9 en vez de 1.0 para reales",
            "ventaja": "Previene overconfidence del discriminador",
            "codigo": "labels_real = 0.9, labels_fake = 0.0"
        },
        "2. Entrenar D m√°s veces": {
            "desc": "Entrenar D k veces por cada entrenamiento de G",
            "ventaja": "D se mantiene m√°s fuerte, da mejor se√±al a G",
            "codigo": "for _ in range(k): train_discriminator()"
        },
        "3. Learning Rate Diferente": {
            "desc": "lr_D = 0.0002, lr_G = 0.0001",
            "ventaja": "Controla el balance entre D y G",
            "codigo": "optimizer_D = Adam(lr=0.0002), optimizer_G = Adam(lr=0.0001)"
        },
        "4. Batch Normalization": {
            "desc": "Normalizar activaciones en cada capa",
            "ventaja": "Estabiliza entrenamiento, acelera convergencia",
            "codigo": "En frameworks: nn.BatchNorm2d() en PyTorch"
        },
        "5. LeakyReLU": {
            "desc": "Usar LeakyReLU en lugar de ReLU",
            "ventaja": "Evita neuronas muertas, mejora gradientes",
            "codigo": "activation = LeakyReLU(0.2)"
        },
        "6. Progressive Growing": {
            "desc": "Empezar con im√°genes peque√±as, crecer gradualmente",
            "ventaja": "Entrenamiento m√°s estable, mejor calidad final",
            "codigo": "T√©cnica avanzada (ProGAN, StyleGAN)"
        }
    }
    
    for nombre, info in estrategias.items():
        print(f"\n{nombre}:")
        print(f"   üìù {info['desc']}")
        print(f"   ‚úÖ {info['ventaja']}")
        print(f"   üíª {info['codigo']}")
    
    print(f"\nüéØ Recomendaci√≥n para principiantes:")
    print(f"   1. Empezar con arquitectura DCGAN (probada y estable)")
    print(f"   2. Usar Adam optimizer con lr=0.0002, beta1=0.5")
    print(f"   3. Normalizar im√°genes a [-1, 1]")
    print(f"   4. A√±adir ruido a las etiquetas (label smoothing)")
    print(f"   5. Monitorear D_loss y G_loss constantemente")

estrategias_entrenamiento_gan()
```

### Actividades

**Actividad 3.6**: Implementa una funci√≥n que calcule el "equilibrio" entre D y G bas√°ndose en sus p√©rdidas.

**Actividad 3.7**: Crea una visualizaci√≥n que muestre c√≥mo D y G compiten a lo largo del tiempo (gr√°fico de p√©rdidas).

**Actividad 3.8**: Implementa `conditional_gan_sketch()` que muestre c√≥mo a√±adir condicionamiento (ej: generar un d√≠gito espec√≠fico).

**Actividad 3.9**: Compara la arquitectura de tu GAN con DCGAN. ¬øQu√© diferencias encuentras?

### Preguntas de Reflexi√≥n

**Pregunta 3.3 (Concebir)**: ¬øEn qu√© aplicaciones reales ser√≠a m√°s √∫til un GAN que un VAE? ¬øY viceversa?

**Pregunta 3.4 (Dise√±ar)**: Si D llega a 100% de precisi√≥n muy r√°pido, ¬øqu√© ajustes har√≠as para balancear el entrenamiento?

**Pregunta 3.5 (Implementar)**: ¬øPor qu√© usamos LeakyReLU en GANs en lugar de ReLU est√°ndar?

**Pregunta 3.6 (Operar)**: En un sistema de generaci√≥n de rostros en producci√≥n, ¬øc√≥mo detectar√≠as y manejar√≠as mode collapse?


---

## üöÄ Desaf√≠os Avanzados

### Desaf√≠o 1: VAE con Framework Moderno (PyTorch/TensorFlow)

**Objetivo**: Implementar y entrenar un VAE completo usando un framework moderno.

**Requisitos**:
- Implementar VAE completo con backpropagation autom√°tico
- Entrenar en MNIST durante 20 √©pocas
- Visualizar reconstrucciones a lo largo del entrenamiento
- Generar 100 nuevas muestras
- Crear un latent space walk (visualizaci√≥n 2D)

**Criterios de √©xito**:
- P√©rdida de reconstrucci√≥n < 100
- D√≠gitos generados reconocibles
- Espacio latente con clusters claros

**Pista**:
```python
# En PyTorch
import torch
import torch.nn as nn

class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        # Encoder
        self.encoder = nn.Sequential(...)
        self.fc_mu = nn.Linear(128, latent_dim)
        self.fc_logvar = nn.Linear(128, latent_dim)
        # Decoder
        self.decoder = nn.Sequential(...)
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    # ... forward, loss, etc.
```

### Desaf√≠o 2: GAN Simple Funcional

**Objetivo**: Entrenar un GAN simple que genere d√≠gitos MNIST reconocibles.

**Requisitos**:
- Implementar Generador y Discriminador en PyTorch/TensorFlow
- Entrenar durante 50 √©pocas
- Guardar im√°genes generadas cada 10 √©pocas
- Implementar al menos 2 estrategias de estabilizaci√≥n
- Graficar p√©rdidas de D y G a lo largo del tiempo

**Criterios de √©xito**:
- Al menos 50% de d√≠gitos generados son reconocibles
- P√©rdidas de D y G se estabilizan (no divergen)
- Diversidad: genera m√∫ltiples tipos de d√≠gitos

### Desaf√≠o 3: Comparaci√≥n VAE vs GAN

**Objetivo**: Comparar objetivamente VAE y GAN en la misma tarea.

**Requisitos**:
- Entrenar ambos modelos en MNIST
- Generar 100 muestras de cada modelo
- Comparar m√©tricas:
  - Calidad visual (evaluaci√≥n humana)
  - Diversidad (varianza de p√≠xeles)
  - Tiempo de entrenamiento
  - Estabilidad del entrenamiento
- Crear informe con visualizaciones

**Criterios de √©xito**:
- An√°lisis cuantitativo con al menos 3 m√©tricas
- Conclusiones claras sobre cu√°ndo usar cada modelo
- Recomendaciones basadas en evidencia

### Desaf√≠o 4: Conditional VAE (CVAE)

**Objetivo**: Extender VAE para generar d√≠gitos espec√≠ficos bajo demanda.

**Requisitos**:
- Modificar arquitectura para incorporar etiquetas
- Entrenar CVAE en MNIST
- Generar d√≠gitos espec√≠ficos (ej: 10 muestras del "7")
- Demostrar control sobre la generaci√≥n

**Criterios de √©xito**:
- Puede generar d√≠gitos espec√≠ficos con >80% precisi√≥n
- Mantiene diversidad dentro de cada clase
- Latent space organizado por clases

**Pista**:
```python
# En el encoder
encoded = encoder(torch.cat([x, one_hot_label], dim=1))

# En el decoder
decoded = decoder(torch.cat([z, one_hot_label], dim=1))
```

### Desaf√≠o 5: Interpolaci√≥n Avanzada

**Objetivo**: Crear interpolaciones suaves y creativas en el espacio latente.

**Requisitos**:
- Implementar interpolaci√≥n lineal
- Implementar interpolaci√≥n esf√©rica (SLERP)
- Crear un "video" de interpolaci√≥n (50 frames)
- Interpolar entre 3+ puntos (no solo 2)

**Criterios de √©xito**:
- Transiciones suaves entre d√≠gitos
- No hay saltos bruscos o artefactos
- Video guardado como GIF o MP4

**F√≥rmula SLERP**:
```python
def slerp(z1, z2, alpha):
    """Spherical linear interpolation."""
    omega = np.arccos(np.clip(np.dot(z1/np.linalg.norm(z1), 
                                     z2/np.linalg.norm(z2)), -1, 1))
    sin_omega = np.sin(omega)
    return (np.sin((1-alpha)*omega) / sin_omega * z1 + 
            np.sin(alpha*omega) / sin_omega * z2)
```

### Desaf√≠o 6: Autoencoder Denoising

**Objetivo**: Usar autoencoder para eliminar ruido de im√°genes.

**Requisitos**:
- A√±adir ruido gaussiano a im√°genes MNIST
- Entrenar autoencoder para reconstruir originales
- Evaluar en diferentes niveles de ruido (œÉ = 0.1, 0.3, 0.5)
- Visualizar antes/despu√©s

**Criterios de √©xito**:
- Mejora visual clara en im√°genes ruidosas
- MSE de im√°genes denoised < MSE de im√°genes ruidosas
- Funciona con m√∫ltiples niveles de ruido

### Desaf√≠o 7: Implementaci√≥n de DCGAN

**Objetivo**: Implementar la arquitectura DCGAN (Deep Convolutional GAN).

**Requisitos**:
- Usar capas convolucionales en G y D
- Seguir gu√≠as de arquitectura de DCGAN paper
- Entrenar en MNIST o Fashion-MNIST
- Generar im√°genes de calidad superior a GAN simple

**Arquitectura DCGAN**:
```python
Generador:
- FC: latent_dim ‚Üí 7√ó7√ó256
- ConvTranspose2d: 7√ó7√ó256 ‚Üí 14√ó14√ó128
- ConvTranspose2d: 14√ó14√ó128 ‚Üí 28√ó28√ó64
- ConvTranspose2d: 28√ó28√ó64 ‚Üí 28√ó28√ó1

Discriminador:
- Conv2d: 28√ó28√ó1 ‚Üí 14√ó14√ó64
- Conv2d: 14√ó14√ó64 ‚Üí 7√ó7√ó128
- Conv2d: 7√ó7√ó128 ‚Üí 4√ó4√ó256
- FC: 4√ó4√ó256 ‚Üí 1
```

---

## üìä An√°lisis de Resultados y M√©tricas

### M√©tricas para Modelos Generativos

**1. Inception Score (IS)**:
```python
def inception_score(generated_images, n_splits=10):
    """
    Mide calidad y diversidad de im√°genes generadas.
    Requiere: modelo Inception pre-entrenado
    
    Interpretaci√≥n:
    - Mayor es mejor
    - IS > 5: Buena calidad para MNIST
    - IS > 10: Excelente para ImageNet
    """
    # Implementaci√≥n requiere modelo Inception
    pass
```

**2. Fr√©chet Inception Distance (FID)**:
```python
def frechet_inception_distance(real_images, fake_images):
    """
    Compara distribuciones de caracter√≠sticas.
    Requiere: modelo Inception pre-entrenado
    
    Interpretaci√≥n:
    - Menor es mejor
    - FID < 50: Buena calidad
    - FID < 10: Excelente calidad
    """
    # Implementaci√≥n requiere modelo Inception
    pass
```

**3. Reconstruction Error (VAE)**:
```python
def evaluate_reconstruction(vae, test_images):
    """
    Mide qu√© tan bien reconstruye el VAE.
    """
    reconstructed, mu, logvar = vae.forward(test_images)
    mse = np.mean((test_images - reconstructed) ** 2)
    return mse

# Uso
mse = evaluate_reconstruction(vae, X_test)
print(f"MSE de reconstrucci√≥n: {mse:.4f}")
```

**4. Latent Space Quality**:
```python
def evaluate_latent_space(vae, X, y):
    """
    Eval√∫a organizaci√≥n del espacio latente.
    """
    from sklearn.metrics import silhouette_score
    
    # Encodear al espacio latente
    mu, _ = vae.encoder.forward(X)
    
    # Calcular silhouette score (clustering quality)
    score = silhouette_score(mu.T, y)
    
    print(f"Silhouette Score: {score:.4f}")
    print(f"Interpretaci√≥n: {score > 0.5 and 'Buena separaci√≥n' or 'Pobre separaci√≥n'}")
    
    return score
```

**5. Mode Coverage (GAN)**:
```python
def evaluate_mode_coverage(gan, n_samples=1000):
    """
    Eval√∫a si el GAN genera todas las clases (0-9).
    """
    # Generar muestras
    z = gan.generate_noise(n_samples)
    generated = gan.forward(z)
    
    # Clasificar con un clasificador pre-entrenado
    # (requiere tener un clasificador MNIST entrenado)
    predictions = classifier.predict(generated)
    
    # Contar clases √∫nicas
    unique_classes = len(np.unique(predictions))
    
    print(f"Clases generadas: {unique_classes}/10")
    print(f"Mode Collapse: {'S√≠' if unique_classes < 8 else 'No'}")
    
    return unique_classes
```

### Visualizaciones de An√°lisis

```python
def analisis_completo_vae(vae, X_test, y_test):
    """
    An√°lisis completo del VAE.
    """
    print("="*70)
    print("AN√ÅLISIS COMPLETO DEL VAE")
    print("="*70)
    
    # 1. Reconstrucciones
    X_recon, mu, logvar = vae.forward(X_test[:, :10])
    
    fig, axes = plt.subplots(2, 10, figsize=(15, 3))
    for i in range(10):
        axes[0, i].imshow(X_test[:, i].reshape(8, 8), cmap='gray')
        axes[0, i].axis('off')
        axes[1, i].imshow(X_recon[:, i].reshape(8, 8), cmap='gray')
        axes[1, i].axis('off')
    axes[0, 0].set_ylabel('Original', fontsize=10)
    axes[1, 0].set_ylabel('Reconstrucci√≥n', fontsize=10)
    plt.suptitle('Reconstrucciones del VAE')
    plt.tight_layout()
    plt.savefig('vae_analysis_reconstruction.png', dpi=300)
    plt.show()
    
    # 2. Espacio latente
    mu_all, _ = vae.encoder.forward(X_test)
    
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(mu_all[0, :], mu_all[1, :], 
                         c=y_test, cmap='tab10', alpha=0.6)
    plt.colorbar(scatter, label='Clase')
    plt.xlabel('Dimensi√≥n Latente 1')
    plt.ylabel('Dimensi√≥n Latente 2')
    plt.title('Espacio Latente del VAE')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('vae_analysis_latent.png', dpi=300)
    plt.show()
    
    # 3. Generaci√≥n
    generated = vae.generate(16)
    
    fig, axes = plt.subplots(4, 4, figsize=(8, 8))
    for i, ax in enumerate(axes.flat):
        ax.imshow(generated[:, i].reshape(8, 8), cmap='gray')
        ax.axis('off')
    plt.suptitle('Muestras Generadas')
    plt.tight_layout()
    plt.savefig('vae_analysis_generated.png', dpi=300)
    plt.show()
    
    # 4. M√©tricas
    mse = evaluate_reconstruction(vae, X_test)
    silhouette = evaluate_latent_space(vae, X_test, y_test)
    
    print(f"\nüìä M√©tricas:")
    print(f"   MSE Reconstrucci√≥n: {mse:.4f}")
    print(f"   Silhouette Score: {silhouette:.4f}")
    print(f"   Conclusi√≥n: {'Buen modelo' if mse < 0.1 and silhouette > 0.3 else 'Necesita mejoras'}")
```

---

## üéì Conclusiones y Reflexi√≥n Final

### Resumen de Conceptos Aprendidos

En este laboratorio has aprendido:

**1. Fundamentos de Modelos Generativos**:
- ‚úÖ Diferencia entre modelos discriminativos y generativos
- ‚úÖ Concepto de espacio latente y su importancia
- ‚úÖ Compresi√≥n y reconstrucci√≥n de informaci√≥n

**2. Variational Autoencoders (VAE)**:
- ‚úÖ Encoder probabil√≠stico (Œº, œÉ)
- ‚úÖ Reparameterization trick para backpropagation
- ‚úÖ P√©rdida combinada: Reconstrucci√≥n + KL Divergence
- ‚úÖ Generaci√≥n de nuevas muestras desde prior N(0,1)
- ‚úÖ Interpolaci√≥n en espacio latente

**3. Generative Adversarial Networks (GAN)**:
- ‚úÖ Arquitectura adversarial: Generador vs Discriminador
- ‚úÖ Entrenamiento alternado y equilibrio de Nash
- ‚úÖ Problemas comunes: mode collapse, vanishing gradients
- ‚úÖ Estrategias de estabilizaci√≥n del entrenamiento
- ‚úÖ Diferencias arquitect√≥nicas y casos de uso

**4. Implementaci√≥n Pr√°ctica**:
- ‚úÖ Implementaci√≥n desde cero de autoencoders
- ‚úÖ Construcci√≥n de VAE completo con todas sus componentes
- ‚úÖ Creaci√≥n de GAN con generador y discriminador
- ‚úÖ Visualizaci√≥n y an√°lisis de resultados
- ‚úÖ Evaluaci√≥n de calidad de modelos generativos

### Comparaci√≥n Final: VAE vs GAN

| Aspecto | VAE | GAN |
|---------|-----|-----|
| **Calidad** | Media-Alta | Muy Alta |
| **Diversidad** | Alta (sin mode collapse) | Media (riesgo de mode collapse) |
| **Estabilidad** | Alta (entrenamiento predecible) | Baja (sensible a hiperpar√°metros) |
| **Velocidad** | R√°pida | Media |
| **Interpretabilidad** | Alta (espacio latente estructurado) | Baja (espacio latente menos estructurado) |
| **Control** | Alto (interpolaci√≥n, aritm√©tica) | Medio |
| **Dificultad** | Media | Alta |
| **Mejor para** | Compresi√≥n, interpolaci√≥n, densidad | Generaci√≥n de alta calidad |

**Cu√°ndo usar VAE**:
- Necesitas espacio latente interpretable
- Quieres estabilidad de entrenamiento
- Necesitas calcular likelihood de datos
- Interpolaci√≥n suave es importante
- Aplicaciones: compresi√≥n, denoising, anomaly detection

**Cu√°ndo usar GAN**:
- Calidad visual es prioridad #1
- Tienes recursos para experimentaci√≥n
- Puedes tolerar inestabilidad
- No necesitas likelihood
- Aplicaciones: generaci√≥n de im√°genes realistas, style transfer, super-resoluci√≥n

### Evoluci√≥n Hist√≥rica y Estado del Arte

**2013-2014: Nacimiento**
- VAE (Kingma & Welling, 2013)
- GAN (Goodfellow et al., 2014)
- Primeras generaciones borrosas

**2015-2017: Mejoras Arquitect√≥nicas**
- DCGAN: Convoluciones para GANs
- ProGAN: Crecimiento progresivo
- Œ≤-VAE: Control sobre disentanglement

**2018-2020: Salto de Calidad**
- StyleGAN: Control fino sobre caracter√≠sticas
- BigGAN: Escala masiva
- VQ-VAE: Representaciones discretas

**2020-2024: Era Moderna**
- **Diffusion Models**: Nueva familia dominante
  - DALL-E 2, Stable Diffusion, Midjourney
  - Mejor calidad que GANs, m√°s estable que GANs
- **Transformers Generativos**: GPT, DALL-E
- **Modelos Multimodales**: Texto + Imagen
- **Aplicaciones Comerciales**: Accesibles al p√∫blico

**Tendencias Futuras**:
- üîÆ Generaci√≥n 3D y video de alta calidad
- üîÆ Control m√°s fino y preciso
- üîÆ Modelos m√°s eficientes (menos par√°metros)
- üîÆ Generaci√≥n personalizada y adaptativa
- üîÆ Integraci√≥n con otras modalidades (audio, texto, 3D)

### Consideraciones √âticas - Responsabilidad

**Potencial Positivo**:
- üé® **Arte y Creatividad**: Democratizaci√≥n de creaci√≥n art√≠stica
- üî¨ **Ciencia**: Dise√±o de f√°rmacos, simulaciones
- üè• **Medicina**: Generaci√≥n de datos sint√©ticos (privacidad)
- üéì **Educaci√≥n**: Contenido personalizado
- ‚ôø **Accesibilidad**: Generaci√≥n de descripciones, traducciones

**Riesgos y Preocupaciones**:
- ‚ö†Ô∏è **Deepfakes**: Desinformaci√≥n, manipulaci√≥n
- ‚ö†Ô∏è **Sesgos**: Reproducci√≥n de sesgos en datos
- ‚ö†Ô∏è **Derechos de Autor**: ¬øDe qui√©n es el arte generado?
- ‚ö†Ô∏è **Trabajo**: Impacto en artistas, dise√±adores
- ‚ö†Ô∏è **Privacidad**: Generaci√≥n de rostros sin consentimiento

**Principios de Uso Responsable**:

1. **Transparencia**: Siempre revelar que el contenido es generado por IA
2. **Consentimiento**: No generar contenido de personas sin permiso
3. **Verificaci√≥n**: Implementar watermarking y detecci√≥n
4. **Regulaci√≥n**: Seguir leyes y regulaciones locales
5. **Evaluaci√≥n de Impacto**: Considerar consecuencias sociales
6. **Equidad**: Detectar y mitigar sesgos
7. **Educaci√≥n**: Educar al p√∫blico sobre IA generativa

**Recursos sobre √âtica**:
- Partnership on AI: [www.partnershiponai.org](https://www.partnershiponai.org)
- Montreal Declaration for Responsible AI
- EU AI Act
- Adobe Content Authenticity Initiative

### Pr√≥ximos Pasos en tu Aprendizaje

**1. Profundizar en Teor√≠a**:
- üìö Leer papers originales (VAE, GAN, Diffusion)
- üìö Estudiar matem√°ticas avanzadas (teor√≠a de informaci√≥n, inferencia variacional)
- üìö Explorar arquitecturas modernas (StyleGAN3, DALL-E 2)

**2. Pr√°ctica con Frameworks**:
- üíª Implementar VAE y GAN completos en PyTorch/TensorFlow
- üíª Entrenar en datasets complejos (CelebA, ImageNet)
- üíª Experimentar con Stable Diffusion y Hugging Face

**3. Proyectos Aplicados**:
- üöÄ Crear generador de arte personalizado
- üöÄ Implementar style transfer avanzado
- üöÄ Desarrollar herramienta de aumento de datos
- üöÄ Contribuir a proyectos open-source

**4. Especializaci√≥n**:
- üéØ Diffusion Models (DDPM, DDIM, Score-based)
- üéØ Transformers Generativos (GPT, DALL-E)
- üéØ Generaci√≥n 3D (NeRF, 3D-aware GANs)
- üéØ Audio/M√∫sica generativa (WaveNet, Jukebox)

### Recursos Adicionales

**Papers Fundamentales**:
- [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) - Kingma & Welling (2013)
- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) - Goodfellow et al. (2014)
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) - Ho et al. (2020)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Vaswani et al. (2017)

**Tutoriales y Cursos**:
- [PyTorch VAE Tutorial](https://pytorch.org/tutorials/)
- [TensorFlow GAN Guide](https://www.tensorflow.org/tutorials/generative/dcgan)
- [Hugging Face Diffusers](https://huggingface.co/docs/diffusers/)
- [Fast.ai Practical Deep Learning](https://course.fast.ai/)
- [Stanford CS236: Deep Generative Models](https://deepgenerativemodels.github.io/)

**Implementaciones de Referencia**:
- [PyTorch Examples: VAE](https://github.com/pytorch/examples/tree/master/vae)
- [PyTorch Examples: DCGAN](https://github.com/pytorch/examples/tree/master/dcgan)
- [Stable Diffusion](https://github.com/CompVis/stable-diffusion)
- [StyleGAN3](https://github.com/NVlabs/stylegan3)

**Comunidades y Foros**:
- r/MachineLearning (Reddit)
- Papers with Code
- Weights & Biases (wandb.ai)
- Hugging Face Forums

**Herramientas Pr√°cticas**:
- [Google Colab](https://colab.research.google.com/) - GPUs gratuitas
- [Weights & Biases](https://wandb.ai/) - Tracking de experimentos
- [TensorBoard](https://www.tensorflow.org/tensorboard) - Visualizaci√≥n
- [Gradio](https://gradio.app/) - Demos interactivas

---

## ‚úÖ Checklist de Verificaci√≥n

Antes de dar por completado este laboratorio, verifica que puedes:

### Conceptos Te√≥ricos
- [ ] Explicar la diferencia entre modelos discriminativos y generativos
- [ ] Describir qu√© es el espacio latente y por qu√© es importante
- [ ] Explicar el reparameterization trick y por qu√© es necesario
- [ ] Entender la funci√≥n de p√©rdida del VAE (reconstrucci√≥n + KL)
- [ ] Describir c√≥mo funciona el entrenamiento adversarial de GANs
- [ ] Identificar problemas comunes (mode collapse, vanishing gradients)

### Implementaci√≥n Pr√°ctica
- [ ] Implementar un autoencoder simple desde cero
- [ ] Construir un encoder probabil√≠stico (Œº, œÉ)
- [ ] Implementar el reparameterization trick
- [ ] Crear un VAE completo funcional
- [ ] Implementar generador y discriminador de GAN
- [ ] Calcular p√©rdidas de VAE y GAN correctamente

### Experimentaci√≥n
- [ ] Entrenar (o intentar entrenar) un VAE en MNIST
- [ ] Visualizar reconstrucciones y compararlas con originales
- [ ] Explorar el espacio latente en 2D
- [ ] Generar nuevas muestras desde el prior
- [ ] Realizar interpolaciones en el espacio latente
- [ ] Experimentar con hiperpar√°metros (latent_dim, Œ≤, etc.)

### An√°lisis y Evaluaci√≥n
- [ ] Evaluar calidad de reconstrucciones (MSE, visual)
- [ ] Analizar organizaci√≥n del espacio latente
- [ ] Comparar VAE y GAN en la misma tarea
- [ ] Identificar mode collapse o problemas de entrenamiento
- [ ] Documentar experimentos y resultados

### Aplicaciones y √âtica
- [ ] Identificar casos de uso apropiados para VAE vs GAN
- [ ] Entender aplicaciones reales de IA generativa
- [ ] Reconocer implicaciones √©ticas (deepfakes, sesgos)
- [ ] Conocer principios de uso responsable
- [ ] Saber c√≥mo detectar contenido generado por IA

### Pr√≥ximos Pasos
- [ ] Tener plan para implementar con frameworks modernos
- [ ] Conocer recursos para aprendizaje continuo
- [ ] Identificar √°rea de especializaci√≥n de inter√©s
- [ ] Saber d√≥nde buscar papers y c√≥digo de referencia

---

## üéâ ¬°Felicitaciones!

Has completado el **Laboratorio 09: Inteligencia Artificial Generativa**.

### Lo que has logrado:

üéØ **Dominaste los fundamentos** de modelos generativos desde cero  
üß† **Implementaste VAE completo** con todas sus componentes  
‚öîÔ∏è **Construiste GAN** con entrenamiento adversarial  
üé® **Generaste contenido nuevo** (¬°aunque sea simple!)  
üìä **Analizaste y evaluaste** modelos generativos  
ü§î **Reflexionaste sobre √©tica** y uso responsable  

### El viaje contin√∫a:

Este laboratorio es solo el comienzo de tu aventura en IA generativa. El campo evoluciona r√°pidamente:

- **2013**: VAEs generan d√≠gitos borrosos
- **2014**: GANs prometen revolucionar generaci√≥n
- **2020**: Diffusion models superan a GANs
- **2023**: ChatGPT y DALL-E son mainstream
- **2024**: Modelos multimodales, video, 3D...
- **¬ø2025+?**: ¬°T√∫ puedes ser parte de la innovaci√≥n!

### Mensaje final:

> "La creatividad no es dominio exclusivo de los humanos. Con IA generativa, hemos creado herramientas que pueden sorprendernos, inspirarnos y amplificar nuestra creatividad. Pero con gran poder viene gran responsabilidad. Usa estas t√©cnicas sabiamente, √©ticamente, y para hacer del mundo un lugar mejor."

**¬°Ahora tienes las bases para crear cosas asombrosas! üöÄ**

---

**Laboratorio dise√±ado con üíô para aprender Deep Learning desde cero**  
**Serie completa**: Labs 01-09 | De Neuronas a IA Generativa

**¬øDudas o feedback?** Comparte tus experimentos, proyectos y preguntas con la comunidad.

**Pr√≥ximo desaf√≠o**: Implementa tu primer modelo generativo completo y entr√©nalo hasta que genere algo que te sorprenda. ¬°El l√≠mite es tu imaginaci√≥n! üé®ü§ñ

---

**#DeepLearning #IAGenerativa #VAE #GAN #MachineLearning #AI**
