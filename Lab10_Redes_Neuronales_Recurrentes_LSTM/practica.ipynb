{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Redes Neuronales Recurrentes y LSTM - Pr√°ctica\n",
    "\n",
    "## Objetivos\n",
    "1. Implementar RNN desde cero\n",
    "2. Entender la arquitectura LSTM y sus puertas\n",
    "3. Comparar RNN, LSTM y GRU\n",
    "4. Aplicar a series temporales\n",
    "5. Generar secuencias de texto\n",
    "6. Visualizar hidden states y puertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from codigo.rnn_lstm import *\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: RNN Vanilla - Conceptos B√°sicos\n",
    "\n",
    "### 1.1 Crear una RNN Simple\n",
    "\n",
    "Empezamos con una RNN b√°sica que procesa secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros\n",
    "input_size = 1\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "\n",
    "# Crear RNN\n",
    "rnn = RNNNumPy(input_size, hidden_size, output_size, seed=42)\n",
    "\n",
    "print(f\"RNN creada:\")\n",
    "print(f\"  Input size: {input_size}\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Output size: {output_size}\")\n",
    "print(f\"\\nForma de pesos:\")\n",
    "print(f\"  W_xh (input->hidden): {rnn.cell.W_xh.shape}\")\n",
    "print(f\"  W_hh (hidden->hidden): {rnn.cell.W_hh.shape}\")\n",
    "print(f\"  W_hy (hidden->output): {rnn.W_hy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Procesar una Secuencia Simple\n",
    "\n",
    "Vamos a procesar una secuencia sinusoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar secuencia de seno\n",
    "seq_len = 20\n",
    "t = np.linspace(0, 4 * np.pi, seq_len)\n",
    "X = np.sin(t).reshape(seq_len, input_size, 1)\n",
    "\n",
    "print(f\"Secuencia de entrada:\")\n",
    "print(f\"  Shape: {X.shape}\")\n",
    "print(f\"  (seq_len, input_size, batch_size)\")\n",
    "\n",
    "# Forward pass\n",
    "outputs, hidden_states = rnn.forward(X)\n",
    "\n",
    "print(f\"\\nSalidas:\")\n",
    "print(f\"  Shape: {outputs.shape}\")\n",
    "print(f\"  Hidden states guardados: {len(hidden_states)}\")\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "axes[0].plot(t, X[:, 0, 0], 'b-', label='Input (seno)', linewidth=2)\n",
    "axes[0].set_ylabel('Valor')\n",
    "axes[0].set_title('Entrada: Onda Sinusoidal')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(t, outputs[:, 0, 0], 'r-', label='Output RNN', linewidth=2)\n",
    "axes[1].set_xlabel('Tiempo')\n",
    "axes[1].set_ylabel('Valor')\n",
    "axes[1].set_title('Salida de RNN (sin entrenar)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualizar Hidden States\n",
    "\n",
    "Los hidden states contienen la \"memoria\" de la RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir hidden states a array\n",
    "hidden_array = np.array([h[:, 0] for h in hidden_states])\n",
    "\n",
    "print(f\"Hidden states shape: {hidden_array.shape}\")\n",
    "print(f\"(seq_len={seq_len}, hidden_size={hidden_size})\")\n",
    "\n",
    "# Visualizar evoluci√≥n de hidden states\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(hidden_array.T, aspect='auto', cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar(label='Activaci√≥n')\n",
    "plt.xlabel('Paso Temporal')\n",
    "plt.ylabel('Neurona Oculta')\n",
    "plt.title('Evoluci√≥n de Hidden States en RNN')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Cada columna representa el estado de todas las neuronas en un momento.\")\n",
    "print(\"   Las neuronas aprenden patrones temporales espec√≠ficos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: LSTM - Arquitectura Completa\n",
    "\n",
    "### 2.1 Crear LSTM y Procesar Secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear LSTM con misma configuraci√≥n\n",
    "lstm = LSTMNumPy(input_size, hidden_size, output_size, seed=42)\n",
    "\n",
    "print(\"LSTM creada con 3 puertas:\")\n",
    "print(\"  1. Forget Gate (f_t): Decide qu√© olvidar\")\n",
    "print(\"  2. Input Gate (i_t): Decide qu√© recordar\")\n",
    "print(\"  3. Output Gate (o_t): Decide qu√© exponer\\n\")\n",
    "\n",
    "print(f\"Pesos de la celda LSTM:\")\n",
    "print(f\"  W_f (forget): {lstm.cell.W_f.shape}\")\n",
    "print(f\"  W_i (input): {lstm.cell.W_i.shape}\")\n",
    "print(f\"  W_C (candidato): {lstm.cell.W_C.shape}\")\n",
    "print(f\"  W_o (output): {lstm.cell.W_o.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "outputs_lstm, hidden_lstm, cell_lstm = lstm.forward(X)\n",
    "\n",
    "print(f\"\\nSalidas LSTM:\")\n",
    "print(f\"  Outputs: {outputs_lstm.shape}\")\n",
    "print(f\"  Hidden states: {len(hidden_lstm)}\")\n",
    "print(f\"  Cell states: {len(cell_lstm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizar Puertas LSTM\n",
    "\n",
    "Veamos c√≥mo las puertas se activan durante el procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar secuencia y guardar activaciones de puertas\n",
    "h_t = np.zeros((hidden_size, 1))\n",
    "C_t = np.zeros((hidden_size, 1))\n",
    "\n",
    "forget_gates = []\n",
    "input_gates = []\n",
    "output_gates = []\n",
    "cell_states = []\n",
    "\n",
    "for t in range(seq_len):\n",
    "    h_t, C_t = lstm.cell.forward(X[t], h_t, C_t)\n",
    "    forget_gates.append(lstm.cell.cache['f_t'][:, 0])\n",
    "    input_gates.append(lstm.cell.cache['i_t'][:, 0])\n",
    "    output_gates.append(lstm.cell.cache['o_t'][:, 0])\n",
    "    cell_states.append(C_t[:, 0])\n",
    "\n",
    "# Convertir a arrays\n",
    "forget_gates = np.array(forget_gates)\n",
    "input_gates = np.array(input_gates)\n",
    "output_gates = np.array(output_gates)\n",
    "cell_states = np.array(cell_states)\n",
    "\n",
    "# Visualizar las 3 puertas\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "\n",
    "im1 = axes[0].imshow(forget_gates.T, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[0].set_title('Forget Gate (f_t): Verde = Recordar, Rojo = Olvidar', fontsize=12)\n",
    "axes[0].set_ylabel('Neurona')\n",
    "plt.colorbar(im1, ax=axes[0], label='Activaci√≥n [0-1]')\n",
    "\n",
    "im2 = axes[1].imshow(input_gates.T, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[1].set_title('Input Gate (i_t): Verde = Aceptar nueva info, Rojo = Rechazar', fontsize=12)\n",
    "axes[1].set_ylabel('Neurona')\n",
    "plt.colorbar(im2, ax=axes[1], label='Activaci√≥n [0-1]')\n",
    "\n",
    "im3 = axes[2].imshow(output_gates.T, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[2].set_title('Output Gate (o_t): Verde = Exponer, Rojo = Ocultar', fontsize=12)\n",
    "axes[2].set_ylabel('Neurona')\n",
    "plt.colorbar(im3, ax=axes[2], label='Activaci√≥n [0-1]')\n",
    "\n",
    "im4 = axes[3].imshow(cell_states.T, aspect='auto', cmap='coolwarm')\n",
    "axes[3].set_title('Cell State (C_t): Memoria a largo plazo', fontsize=12)\n",
    "axes[3].set_xlabel('Paso Temporal')\n",
    "axes[3].set_ylabel('Neurona')\n",
    "plt.colorbar(im4, ax=axes[3], label='Valor')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretaci√≥n:\")\n",
    "print(\"   ‚Ä¢ Forget Gate (verde): La neurona retiene informaci√≥n\")\n",
    "print(\"   ‚Ä¢ Input Gate (verde): La neurona acepta nueva informaci√≥n\")\n",
    "print(\"   ‚Ä¢ Output Gate (verde): La neurona expone su estado\")\n",
    "print(\"   ‚Ä¢ Cell State: Memoria que persiste a trav√©s del tiempo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparar RNN vs LSTM\n",
    "\n",
    "Visualicemos las diferencias en los hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar hidden states\n",
    "hidden_rnn = np.array([h[:, 0] for h in hidden_states])\n",
    "hidden_lstm_array = np.array([h[:, 0] for h in hidden_lstm])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "im1 = axes[0].imshow(hidden_rnn.T, aspect='auto', cmap='coolwarm')\n",
    "axes[0].set_title('Hidden States - RNN Vanilla', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Paso Temporal')\n",
    "axes[0].set_ylabel('Neurona Oculta')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(hidden_lstm_array.T, aspect='auto', cmap='coolwarm')\n",
    "axes[1].set_title('Hidden States - LSTM', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Paso Temporal')\n",
    "axes[1].set_ylabel('Neurona Oculta')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Diferencias:\")\n",
    "print(\"   ‚Ä¢ RNN: Estados m√°s vol√°tiles, cambian m√°s bruscamente\")\n",
    "print(\"   ‚Ä¢ LSTM: Estados m√°s estables, gracias al cell state\")\n",
    "print(\"   ‚Ä¢ LSTM puede mantener informaci√≥n por m√°s tiempo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: GRU - Alternativa Simplificada\n",
    "\n",
    "### 3.1 Crear y Comparar GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear GRU\n",
    "gru = GRUNumPy(input_size, hidden_size, output_size, seed=42)\n",
    "\n",
    "print(\"GRU creada con 2 puertas:\")\n",
    "print(\"  1. Reset Gate (r_t): Controla cu√°nto pasado usar\")\n",
    "print(\"  2. Update Gate (z_t): Balance entre pasado y presente\\n\")\n",
    "\n",
    "print(f\"Pesos de la celda GRU:\")\n",
    "print(f\"  W_r (reset): {gru.cell.W_r.shape}\")\n",
    "print(f\"  W_z (update): {gru.cell.W_z.shape}\")\n",
    "print(f\"  W_h (candidato): {gru.cell.W_h.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "outputs_gru, hidden_gru = gru.forward(X)\n",
    "\n",
    "print(f\"\\nSalidas GRU:\")\n",
    "print(f\"  Outputs: {outputs_gru.shape}\")\n",
    "print(f\"  Hidden states: {len(hidden_gru)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparaci√≥n de Complejidad\n",
    "\n",
    "Calculemos el n√∫mero de par√°metros de cada arquitectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(input_size, hidden_size, output_size, model_type):\n",
    "    \"\"\"Calcular n√∫mero de par√°metros.\"\"\"\n",
    "    if model_type == 'RNN':\n",
    "        # W_xh + W_hh + b_h + W_hy + b_y\n",
    "        params = (hidden_size * input_size +  # W_xh\n",
    "                 hidden_size * hidden_size +  # W_hh\n",
    "                 hidden_size +                # b_h\n",
    "                 output_size * hidden_size +  # W_hy\n",
    "                 output_size)                 # b_y\n",
    "    \n",
    "    elif model_type == 'LSTM':\n",
    "        # 4 gates: f, i, C, o\n",
    "        combined_size = hidden_size + input_size\n",
    "        params = (4 * (hidden_size * combined_size + hidden_size) +  # 4 gates\n",
    "                 output_size * hidden_size + output_size)            # output\n",
    "    \n",
    "    elif model_type == 'GRU':\n",
    "        # 3 transformations: r, z, h\n",
    "        combined_size = hidden_size + input_size\n",
    "        params = (3 * (hidden_size * combined_size + hidden_size) +  # 3 gates\n",
    "                 output_size * hidden_size + output_size)            # output\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Calcular par√°metros\n",
    "params_rnn = count_parameters(input_size, hidden_size, output_size, 'RNN')\n",
    "params_lstm = count_parameters(input_size, hidden_size, output_size, 'LSTM')\n",
    "params_gru = count_parameters(input_size, hidden_size, output_size, 'GRU')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACI√ìN DE ARQUITECTURAS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Modelo':<15} {'Par√°metros':<15} {'Ratio vs RNN':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'RNN':<15} {params_rnn:<15} {1.0:<15.2f}\")\n",
    "print(f\"{'LSTM':<15} {params_lstm:<15} {params_lstm/params_rnn:<15.2f}\")\n",
    "print(f\"{'GRU':<15} {params_gru:<15} {params_gru/params_rnn:<15.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualizar\n",
    "models = ['RNN', 'LSTM', 'GRU']\n",
    "params = [params_rnn, params_lstm, params_gru]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, params, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# A√±adir valores sobre las barras\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.ylabel('N√∫mero de Par√°metros', fontsize=12)\n",
    "plt.title(f'Comparaci√≥n de Complejidad\\n(input={input_size}, hidden={hidden_size}, output={output_size})',\n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Conclusiones:\")\n",
    "print(f\"   ‚Ä¢ LSTM tiene ~{params_lstm/params_rnn:.1f}x m√°s par√°metros que RNN\")\n",
    "print(f\"   ‚Ä¢ GRU tiene ~{params_gru/params_rnn:.1f}x m√°s par√°metros que RNN\")\n",
    "print(f\"   ‚Ä¢ GRU es ~{params_lstm/params_gru:.0f}% m√°s ligero que LSTM\")\n",
    "print(\"   ‚Ä¢ M√°s par√°metros = Mayor capacidad pero m√°s lento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Series Temporales - Predicci√≥n\n",
    "\n",
    "### 4.1 Generar Serie Temporal Sint√©tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar serie temporal compleja\n",
    "n_points = 200\n",
    "t = np.linspace(0, 10 * np.pi, n_points)\n",
    "\n",
    "# Combinar m√∫ltiples componentes\n",
    "trend = 0.05 * t  # Tendencia lineal\n",
    "seasonal = np.sin(t) + 0.5 * np.sin(2 * t)  # Componentes estacionales\n",
    "noise = np.random.normal(0, 0.1, n_points)  # Ruido\n",
    "\n",
    "time_series = trend + seasonal + noise\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(t, time_series, 'b-', linewidth=1.5, label='Serie Temporal')\n",
    "plt.plot(t, trend, 'r--', linewidth=2, label='Tendencia', alpha=0.7)\n",
    "plt.plot(t, seasonal, 'g--', linewidth=2, label='Estacionalidad', alpha=0.7)\n",
    "plt.xlabel('Tiempo', fontsize=12)\n",
    "plt.ylabel('Valor', fontsize=12)\n",
    "plt.title('Serie Temporal Sint√©tica (Tendencia + Estacionalidad + Ruido)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Serie temporal generada: {len(time_series)} puntos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Preparar Datos para Entrenamiento\n",
    "\n",
    "Dividimos la serie en secuencias de entrada/salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar datos\n",
    "data_mean = time_series.mean()\n",
    "data_std = time_series.std()\n",
    "time_series_norm = (time_series - data_mean) / data_std\n",
    "\n",
    "# Crear secuencias\n",
    "seq_length = 10  # Usar 10 pasos para predecir el siguiente\n",
    "pred_length = 1  # Predecir 1 paso adelante\n",
    "\n",
    "X_seq, y_seq = create_sequences(time_series_norm, seq_length, pred_length)\n",
    "\n",
    "print(f\"Secuencias creadas:\")\n",
    "print(f\"  X shape: {X_seq.shape} (num_sequences, seq_length)\")\n",
    "print(f\"  y shape: {y_seq.shape} (num_sequences, pred_length)\")\n",
    "\n",
    "# Dividir en train/test\n",
    "split = int(0.8 * len(X_seq))\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "print(f\"\\nDivisi√≥n train/test:\")\n",
    "print(f\"  Train: {len(X_train)} secuencias\")\n",
    "print(f\"  Test:  {len(X_test)} secuencias\")\n",
    "\n",
    "# Mostrar ejemplo de secuencia\n",
    "print(f\"\\nEjemplo de secuencia:\")\n",
    "print(f\"  Input (10 pasos):  {X_train[0]}\")\n",
    "print(f\"  Target (1 paso):   {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Entrenar con PyTorch (si est√° disponible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    \n",
    "    # Preparar datos para PyTorch\n",
    "    X_train_t = torch.FloatTensor(X_train).unsqueeze(-1)  # (batch, seq, 1)\n",
    "    y_train_t = torch.FloatTensor(y_train).squeeze()\n",
    "    X_test_t = torch.FloatTensor(X_test).unsqueeze(-1)\n",
    "    y_test_t = torch.FloatTensor(y_test).squeeze()\n",
    "    \n",
    "    # Transponer para (seq, batch, features)\n",
    "    X_train_t = X_train_t.transpose(0, 1)\n",
    "    X_test_t = X_test_t.transpose(0, 1)\n",
    "    \n",
    "    print(f\"Datos preparados para PyTorch:\")\n",
    "    print(f\"  X_train: {X_train_t.shape} (seq, batch, features)\")\n",
    "    print(f\"  y_train: {y_train_t.shape} (batch,)\")\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = SimpleLSTM(\n",
    "        input_size=1,\n",
    "        hidden_size=32,\n",
    "        output_size=1,\n",
    "        num_layers=1,\n",
    "        bidirectional=False\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    print(f\"\\nModelo LSTM creado:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Entrenar\n",
    "    num_epochs = 50\n",
    "    losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(X_train_t)\n",
    "        loss = criterion(output.squeeze(), y_train_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}')\n",
    "    \n",
    "    # Visualizar curva de aprendizaje\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, linewidth=2)\n",
    "    plt.xlabel('√âpoca', fontsize=12)\n",
    "    plt.ylabel('MSE Loss', fontsize=12)\n",
    "    plt.title('Curva de Aprendizaje - LSTM', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluar\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred, _ = model(X_train_t)\n",
    "        test_pred, _ = model(X_test_t)\n",
    "    \n",
    "    train_pred = train_pred.squeeze().numpy()\n",
    "    test_pred = test_pred.squeeze().numpy()\n",
    "    \n",
    "    # Visualizar predicciones\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Train\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_train, train_pred, alpha=0.5, s=20)\n",
    "    plt.plot([y_train.min(), y_train.max()], \n",
    "            [y_train.min(), y_train.max()], 'r--', linewidth=2)\n",
    "    plt.xlabel('Valor Real', fontsize=11)\n",
    "    plt.ylabel('Predicci√≥n', fontsize=11)\n",
    "    plt.title('Train Set', fontsize=12, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, test_pred, alpha=0.5, s=20, color='orange')\n",
    "    plt.plot([y_test.min(), y_test.max()], \n",
    "            [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "    plt.xlabel('Valor Real', fontsize=11)\n",
    "    plt.ylabel('Predicci√≥n', fontsize=11)\n",
    "    plt.title('Test Set', fontsize=12, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    train_mse = np.mean((y_train - train_pred) ** 2)\n",
    "    test_mse = np.mean((y_test - test_pred) ** 2)\n",
    "    \n",
    "    print(f\"\\nüìä Resultados:\")\n",
    "    print(f\"   Train MSE: {train_mse:.6f}\")\n",
    "    print(f\"   Test MSE:  {test_mse:.6f}\")\n",
    "    print(f\"   ‚úì Modelo entrenado exitosamente!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PyTorch no disponible. Saltando entrenamiento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Generaci√≥n de Texto Simple\n",
    "\n",
    "### 5.1 Crear Vocabulario Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto simple para demostraci√≥n\n",
    "text = \"hola mundo hola python hola rnn lstm gru deep learning\"\n",
    "\n",
    "# Crear vocabulario\n",
    "words = text.split()\n",
    "vocab = sorted(set(words))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "print(f\"Texto: '{text}'\")\n",
    "print(f\"\\nVocabulario ({len(vocab)} palabras):\")\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"  {word:12} -> {idx}\")\n",
    "\n",
    "# Convertir a √≠ndices\n",
    "text_indices = [word_to_idx[word] for word in words]\n",
    "print(f\"\\nTexto como √≠ndices: {text_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Modelo Char-Level (Caracteres)\n",
    "\n",
    "Un modelo m√°s interesante: predecir el siguiente car√°cter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    # Texto m√°s largo para char-level\n",
    "    text_char = \"\"\"El deep learning es fascinante. \n",
    "    Las redes neuronales recurrentes pueden aprender patrones en secuencias. \n",
    "    LSTM y GRU son arquitecturas poderosas.\"\"\"\n",
    "    \n",
    "    # Crear vocabulario de caracteres\n",
    "    chars = sorted(set(text_char))\n",
    "    char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "    idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "    vocab_size = len(chars)\n",
    "    \n",
    "    print(f\"Texto length: {len(text_char)} caracteres\")\n",
    "    print(f\"Vocabulario: {vocab_size} caracteres √∫nicos\")\n",
    "    print(f\"Caracteres: {''.join(chars)}\")\n",
    "    \n",
    "    # Convertir texto a √≠ndices\n",
    "    text_encoded = [char_to_idx[ch] for ch in text_char]\n",
    "    \n",
    "    # Crear secuencias\n",
    "    seq_len_char = 20\n",
    "    X_char, y_char = [], []\n",
    "    \n",
    "    for i in range(len(text_encoded) - seq_len_char):\n",
    "        X_char.append(text_encoded[i:i+seq_len_char])\n",
    "        y_char.append(text_encoded[i+seq_len_char])\n",
    "    \n",
    "    X_char = torch.LongTensor(X_char)  # (batch, seq_len)\n",
    "    y_char = torch.LongTensor(y_char)  # (batch,)\n",
    "    \n",
    "    print(f\"\\nSecuencias creadas: {len(X_char)}\")\n",
    "    print(f\"Ejemplo:\")\n",
    "    print(f\"  Input:  '{text_char[0:seq_len_char]}'\")\n",
    "    print(f\"  Target: '{text_char[seq_len_char]}'\")\n",
    "    \n",
    "    # Crear modelo\n",
    "    char_model = CharRNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_size=32,\n",
    "        hidden_size=64,\n",
    "        num_layers=1,\n",
    "        rnn_type='lstm'\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(char_model.parameters(), lr=0.005)\n",
    "    \n",
    "    print(f\"\\nModelo CharRNN creado:\")\n",
    "    print(f\"  Vocab size: {vocab_size}\")\n",
    "    print(f\"  Embed size: 32\")\n",
    "    print(f\"  Hidden size: 64\")\n",
    "    \n",
    "    # Entrenar\n",
    "    num_epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    dataset = TensorDataset(X_char, y_char)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    char_losses = []\n",
    "    char_model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = char_model(batch_x)\n",
    "            # Tomar √∫ltimo output\n",
    "            output_last = output[:, -1, :]  # (batch, vocab_size)\n",
    "            loss = criterion(output_last, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(char_model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        char_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(char_losses, linewidth=2, color='purple')\n",
    "    plt.xlabel('√âpoca', fontsize=12)\n",
    "    plt.ylabel('Cross Entropy Loss', fontsize=12)\n",
    "    plt.title('Entrenamiento CharRNN', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Modelo CharRNN entrenado!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PyTorch no disponible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Generar Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYTORCH_AVAILABLE:\n",
    "    def generate_text(model, start_text, length=100, temperature=1.0):\n",
    "        \"\"\"Generar texto car√°cter por car√°cter.\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Inicializar con texto de inicio\n",
    "        current_text = start_text\n",
    "        input_seq = [char_to_idx.get(ch, 0) for ch in start_text[-seq_len_char:]]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(length):\n",
    "                # Preparar input\n",
    "                x = torch.LongTensor([input_seq]).to('cpu')\n",
    "                \n",
    "                # Predecir\n",
    "                output, _ = model(x)\n",
    "                logits = output[0, -1, :] / temperature\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                \n",
    "                # Samplear siguiente car√°cter\n",
    "                next_idx = torch.multinomial(probs, 1).item()\n",
    "                next_char = idx_to_char[next_idx]\n",
    "                \n",
    "                # Actualizar\n",
    "                current_text += next_char\n",
    "                input_seq = input_seq[1:] + [next_idx]\n",
    "        \n",
    "        return current_text\n",
    "    \n",
    "    # Generar con diferentes temperaturas\n",
    "    start = \"El deep learning\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERACI√ìN DE TEXTO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for temp in [0.5, 1.0, 1.5]:\n",
    "        generated = generate_text(char_model, start, length=100, temperature=temp)\n",
    "        print(f\"\\nTemperatura {temp}:\")\n",
    "        print(f\"{generated}\")\n",
    "        print(\"-\"*70)\n",
    "    \n",
    "    print(\"\\nüí° Temperatura:\")\n",
    "    print(\"   ‚Ä¢ Baja (0.5): M√°s determinista, repite patrones\")\n",
    "    print(\"   ‚Ä¢ Media (1.0): Balance entre creatividad y coherencia\")\n",
    "    print(\"   ‚Ä¢ Alta (1.5): M√°s aleatorio, m√°s creativo pero menos coherente\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PyTorch no disponible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Implementar Backward Pass para RNN\n",
    "\n",
    "Completa la implementaci√≥n del backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO: Implementa backpropagation through time para RNN\n",
    "\n",
    "def bptt_simple(X, y_true, rnn_cell, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Backpropagation Through Time simple.\n",
    "    \n",
    "    Pasos:\n",
    "    1. Forward pass guardando todos los estados\n",
    "    2. Calcular gradiente de la p√©rdida\n",
    "    3. Backward pass acumulando gradientes\n",
    "    4. Actualizar pesos\n",
    "    \"\"\"\n",
    "    seq_len = X.shape[0]\n",
    "    \n",
    "    # TODO: Implementar BPTT\n",
    "    # 1. Forward pass\n",
    "    # 2. Calcular loss\n",
    "    # 3. Backward pass\n",
    "    # 4. Actualizar pesos con gradient clipping\n",
    "    \n",
    "    pass\n",
    "\n",
    "print(\"üí™ Ejercicio 1: Implementa BPTT arriba\")\n",
    "print(\"   Pista: Usa gradient clipping para estabilidad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Comparar RNN/LSTM/GRU en una Tarea\n",
    "\n",
    "Compara los tres modelos en predicci√≥n de series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO: Compara RNN, LSTM y GRU\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"üí™ Ejercicio 2:\")\n",
    "    print(\"   1. Crea 3 modelos (RNN, LSTM, GRU) con PyTorch\")\n",
    "    print(\"   2. Entrena cada uno en la serie temporal\")\n",
    "    print(\"   3. Compara MSE de test\")\n",
    "    print(\"   4. Compara tiempo de entrenamiento\")\n",
    "    print(\"   5. Visualiza resultados\")\n",
    "    \n",
    "    # TODO: Tu c√≥digo aqu√≠\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PyTorch no disponible para este ejercicio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Bidirectional LSTM\n",
    "\n",
    "Implementa y prueba un LSTM bidireccional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO: LSTM Bidireccional\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    print(\"üí™ Ejercicio 3:\")\n",
    "    print(\"   1. Crea SimpleLSTM con bidirectional=True\")\n",
    "    print(\"   2. Entrena en clasificaci√≥n (ejemplo: sentimiento)\")\n",
    "    print(\"   3. Compara con LSTM unidireccional\")\n",
    "    print(\"   4. Analiza cu√°ndo bidireccional es mejor\")\n",
    "    \n",
    "    # TODO: Tu c√≥digo aqu√≠\n",
    "    # Pista: Bidireccional duplica el tama√±o de hidden state\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PyTorch no disponible para este ejercicio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Resumen y Conclusiones\n",
    "\n",
    "### 7.1 Conceptos Clave Aprendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN: Redes Neuronales Recurrentes y LSTM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. ARQUITECTURAS:\")\n",
    "print(\"   ‚Ä¢ RNN Vanilla: Simple, problemas con gradiente\")\n",
    "print(\"   ‚Ä¢ LSTM: 3 puertas, resuelve gradiente desvaneciente\")\n",
    "print(\"   ‚Ä¢ GRU: 2 puertas, m√°s eficiente que LSTM\")\n",
    "\n",
    "print(\"\\n2. COMPONENTES LSTM:\")\n",
    "print(\"   ‚Ä¢ Forget Gate: Controla qu√© olvidar\")\n",
    "print(\"   ‚Ä¢ Input Gate: Controla qu√© recordar\")\n",
    "print(\"   ‚Ä¢ Output Gate: Controla qu√© exponer\")\n",
    "print(\"   ‚Ä¢ Cell State: Memoria a largo plazo\")\n",
    "\n",
    "print(\"\\n3. APLICACIONES:\")\n",
    "print(\"   ‚Ä¢ Series Temporales: Predicci√≥n, forecasting\")\n",
    "print(\"   ‚Ä¢ NLP: Clasificaci√≥n, generaci√≥n de texto\")\n",
    "print(\"   ‚Ä¢ Secuencias: Cualquier dato con orden temporal\")\n",
    "\n",
    "print(\"\\n4. T√âCNICAS IMPORTANTES:\")\n",
    "print(\"   ‚Ä¢ Gradient Clipping: Prevenir explosi√≥n\")\n",
    "print(\"   ‚Ä¢ Teacher Forcing: Acelerar entrenamiento\")\n",
    "print(\"   ‚Ä¢ Bidirectional: Usar contexto futuro\")\n",
    "print(\"   ‚Ä¢ Stacked: M√∫ltiples capas para abstracci√≥n\")\n",
    "\n",
    "print(\"\\n5. LIMITACIONES:\")\n",
    "print(\"   ‚Ä¢ Procesamiento secuencial (no paralelizable)\")\n",
    "print(\"   ‚Ä¢ Lento comparado con Transformers\")\n",
    "print(\"   ‚Ä¢ L√≠mite en dependencias muy largas\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì Lab 10 completado!\")\n",
    "print(\"  Siguiente: Lab 11 - Transformers\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Recursos Adicionales\n",
    "\n",
    "**Papers Importantes:**\n",
    "- [LSTM Original (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) - Hochreiter & Schmidhuber\n",
    "- [GRU (2014)](https://arxiv.org/abs/1406.1078) - Cho et al.\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) - Chris Olah\n",
    "\n",
    "**Recursos Online:**\n",
    "- PyTorch RNN Tutorial: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "- The Unreasonable Effectiveness of RNNs: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "**Datasets para Pr√°ctica:**\n",
    "- Time Series: Stock prices, weather data\n",
    "- Text: Penn Treebank, WikiText-2, Shakespeare\n",
    "- Sequences: Human activity, sensor data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
