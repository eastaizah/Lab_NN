# Lab 10: Redes Neuronales Recurrentes y LSTM

## Descripci√≥n

Este laboratorio introduce las Redes Neuronales Recurrentes (RNN) y Long Short-Term Memory (LSTM), arquitecturas especializadas para procesar datos secuenciales como texto, series de tiempo y audio. Implementaremos desde cero los componentes fundamentales y exploraremos aplicaciones pr√°cticas.

## Objetivos de Aprendizaje

Al completar este laboratorio, podr√°s:

1. ‚úÖ Comprender c√≥mo las RNNs procesan secuencias
2. ‚úÖ Implementar una RNN simple desde cero
3. ‚úÖ Entender el problema del gradiente que desaparece/explota
4. ‚úÖ Comprender la arquitectura LSTM y sus componentes (gates)
5. ‚úÖ Implementar LSTM desde cero
6. ‚úÖ Aplicar RNNs/LSTMs a problemas de clasificaci√≥n de texto
7. ‚úÖ Entender bidirectional RNNs y stacked RNNs
8. ‚úÖ Conocer variantes: GRU, Bidirectional LSTM

## Contenido

### üìñ Teor√≠a (`teoria.md`)

Documento completo con los fundamentos te√≥ricos:
- ¬øPor qu√© RNNs para secuencias?
- Arquitectura de RNN vanilla
- Backpropagation Through Time (BPTT)
- Problema del gradiente que desaparece
- LSTM: arquitectura y gates
- GRU: alternativa m√°s simple
- Aplicaciones de RNNs

### üíª Pr√°ctica (`practica.ipynb`)

Jupyter Notebook interactivo con:
- Implementaci√≥n de RNN desde cero
- Construcci√≥n de LSTM paso a paso
- Entrenamiento en datos secuenciales
- Predicci√≥n de series de tiempo
- Clasificaci√≥n de sentimientos en texto
- Generaci√≥n de texto
- Ejercicios progresivos

### üîß C√≥digo de Ejemplo (`codigo/rnn_lstm.py`)

Script Python con implementaciones completas:
- Clase `RNNCell`: Celda RNN b√°sica
- Clase `RNN`: Red recurrente completa
- Clase `LSTMCell`: Celda LSTM con gates
- Clase `LSTM`: LSTM completa
- Clase `GRU`: Gated Recurrent Unit
- Ejemplos con PyTorch

## C√≥mo Usar Este Laboratorio

### Opci√≥n 1: Jupyter Notebook (Recomendado)

```bash
# Desde el directorio del repositorio
cd Lab10_Redes_Neuronales_Recurrentes_LSTM
jupyter notebook practica.ipynb
```

### Opci√≥n 2: Script Python

```bash
# Ejecutar el c√≥digo de ejemplo
python codigo/rnn_lstm.py
```

### Opci√≥n 3: Lectura y Experimentaci√≥n

1. Lee `teoria.md` para entender los conceptos
2. Abre `practica.ipynb` en Jupyter
3. Ejecuta cada celda y experimenta con las secuencias
4. Completa los ejercicios propuestos
5. Revisa `codigo/rnn_lstm.py` como referencia

## Requisitos

```bash
pip install numpy matplotlib jupyter torch
```

## Conceptos Clave

- **Secuencia**: Datos ordenados en el tiempo (texto, series temporales)
- **Estado Oculto (Hidden State)**: Memoria de la RNN sobre el pasado
- **Celda Recurrente**: Unidad que procesa un paso de tiempo
- **BPTT**: Backpropagation Through Time - entrenamiento de RNNs
- **Gates**: Mecanismos que controlan flujo de informaci√≥n (forget, input, output)
- **Cell State**: Memoria a largo plazo en LSTM
- **GRU**: Versi√≥n simplificada de LSTM con menos par√°metros

## Ejercicios

### Ejercicio 10.1: RNN para Suma
Implementa una RNN que sume una secuencia de n√∫meros.

### Ejercicio 10.2: LSTM desde Cero
Completa la implementaci√≥n de LSTM con todos los gates.

### Ejercicio 10.3: Predicci√≥n de Series
Entrena LSTM para predecir valores futuros de una serie de tiempo.

### Ejercicio 10.4: Clasificaci√≥n de Sentimientos
Usa RNN/LSTM para clasificar rese√±as como positivas o negativas.

### Ejercicio 10.5: Generaci√≥n de Texto (Desaf√≠o)
Genera texto caracter por caracter usando LSTM.

## Ventajas de RNNs/LSTMs

1. **Procesan Secuencias de Longitud Variable**: Flexible para diferentes inputs
2. **Memoria Temporal**: Mantienen contexto del pasado
3. **Compartici√≥n de Par√°metros**: Mismos pesos en cada paso temporal
4. **Capturan Dependencias Temporales**: Entienden orden y contexto

## Diferencias RNN vs LSTM

### RNN Vanilla

**Ventajas:**
- Simple, f√°cil de entender
- Menos par√°metros

**Desventajas:**
- Gradientes que desaparecen/explotan
- No captura dependencias largas
- Dif√≠cil de entrenar

### LSTM

**Ventajas:**
- Captura dependencias a largo plazo
- Resuelve gradientes que desaparecen
- M√°s estable en entrenamiento

**Desventajas:**
- M√°s par√°metros (4√ó que RNN)
- M√°s lento de entrenar
- M√°s complejo

### GRU

**Ventajas:**
- Menos par√°metros que LSTM (3√ó que RNN)
- M√°s r√°pido que LSTM
- Rendimiento similar a LSTM

**Desventajas:**
- M√°s complejo que RNN vanilla

## Aplicaciones

### Procesamiento de Lenguaje Natural (NLP)
- **Clasificaci√≥n de Texto**: Sentimientos, spam, categor√≠as
- **Traducci√≥n Autom√°tica**: seq2seq con encoder-decoder
- **Generaci√≥n de Texto**: Completar oraciones, escribir historias
- **Named Entity Recognition**: Identificar personas, lugares
- **Question Answering**: Responder preguntas sobre texto

### Series de Tiempo
- **Predicci√≥n de Stock**: Precios de acciones
- **Predicci√≥n de Clima**: Temperatura, lluvia
- **Predicci√≥n de Demanda**: Ventas, tr√°fico
- **Detecci√≥n de Anomal√≠as**: En se√±ales temporales

### Audio y M√∫sica
- **Reconocimiento de Voz**: Speech-to-text
- **Generaci√≥n de M√∫sica**: Componer melod√≠as
- **Clasificaci√≥n de Audio**: G√©neros musicales

### Video
- **Descripci√≥n de Video**: Generar captions
- **Reconocimiento de Acciones**: Detectar actividades

## Arquitecturas Avanzadas

### Bidirectional RNN/LSTM
```
‚Üí ‚Üí ‚Üí ‚Üí  (forward)
‚ÜêÂ∫èÂàóÊï∞ÊçÆ ‚Üê  (backward)
```
- Procesa secuencia en ambas direcciones
- Mejor contexto para cada elemento
- √ötil cuando toda la secuencia est√° disponible

### Stacked (Deep) RNN/LSTM
```
LSTM Layer 3
    ‚Üë
LSTM Layer 2
    ‚Üë
LSTM Layer 1
    ‚Üë
  Input
```
- M√∫ltiples capas RNN/LSTM apiladas
- Captura jerarqu√≠a de caracter√≠sticas
- Primera capa: caracter√≠sticas simples
- Capas superiores: caracter√≠sticas complejas

### Encoder-Decoder
```
Encoder RNN ‚Üí Context Vector ‚Üí Decoder RNN
   input           |              output
 sequence          |            sequence
```
- Usado en traducci√≥n autom√°tica
- Encoder comprime input a vector
- Decoder genera output desde vector

### Attention Mechanism
- Permite enfocarse en partes relevantes del input
- Mejora significativa sobre encoder-decoder b√°sico
- Base para Transformers (Lab 11)

## Notas Importantes

‚ö†Ô∏è **Gradient Clipping**: Esencial para evitar explosi√≥n de gradientes. Limita norma de gradientes (ej: clip a 5.0).

üí° **Secuencia de Tama√±o**: LSTMs funcionan bien hasta ~200-300 pasos. Para m√°s largo, considera Transformers.

üöÄ **Embeddings**: Para texto, usa embeddings (Word2Vec, GloVe) antes de RNN/LSTM.

‚ö° **Bidireccional**: √ötil para tareas donde el futuro importa (clasificaci√≥n), no para predicci√≥n en tiempo real.

## F√≥rmulas Importantes

### RNN Vanilla
```python
h_t = tanh(W_hh @ h_{t-1} + W_xh @ x_t + b_h)
y_t = W_hy @ h_t + b_y
```

### LSTM
```python
f_t = œÉ(W_f @ [h_{t-1}, x_t] + b_f)  # forget gate
i_t = œÉ(W_i @ [h_{t-1}, x_t] + b_i)  # input gate
CÃÉ_t = tanh(W_C @ [h_{t-1}, x_t] + b_C)  # candidate
C_t = f_t * C_{t-1} + i_t * CÃÉ_t  # cell state
o_t = œÉ(W_o @ [h_{t-1}, x_t] + b_o)  # output gate
h_t = o_t * tanh(C_t)  # hidden state
```

### GRU
```python
r_t = œÉ(W_r @ [h_{t-1}, x_t])  # reset gate
z_t = œÉ(W_z @ [h_{t-1}, x_t])  # update gate
hÃÉ_t = tanh(W @ [r_t * h_{t-1}, x_t])  # candidate
h_t = (1 - z_t) * h_{t-1} + z_t * hÃÉ_t  # hidden state
```

## N√∫mero de Par√°metros

Para hidden_size=h, input_size=x, output_size=y:

- **RNN**: 4 matrices ‚Üí (h√óh + x√óh + h + h√óy + y)
- **LSTM**: 4√ó RNN ‚Üí 4(h√óh + x√óh + h) + (h√óy + y)
- **GRU**: 3√ó RNN ‚Üí 3(h√óh + x√óh + h) + (h√óy + y)

## Pr√≥ximo Paso

Una vez completes este laboratorio, contin√∫a con:

üëâ **[Lab 11: Transformers](../Lab11_Transformers/)**

Exploraremos la arquitectura que revolucion√≥ el NLP y est√° transformando todo el deep learning.

## Recursos Adicionales

- [Understanding LSTM Networks - colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [The Unreasonable Effectiveness of RNNs - Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [PyTorch RNN Tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)
- [Sequence Models - Coursera](https://www.coursera.org/learn/nlp-sequence-models)
- [RNN Cheatsheet - Stanford CS230](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)

## Preguntas Frecuentes

**P: ¬øCu√°ndo usar RNN vs LSTM vs GRU?**  
R: RNN para secuencias cortas y simples. LSTM cuando necesitas memoria a largo plazo. GRU como alternativa m√°s r√°pida a LSTM con rendimiento similar.

**P: ¬øPor qu√© los gradientes desaparecen en RNNs?**  
R: Al hacer backprop a trav√©s de muchos pasos temporales, multiplicamos derivadas <1, haciendo que el gradiente ‚Üí 0 exponencialmente.

**P: ¬øLSTM siempre es mejor que RNN?**  
R: No siempre. Para tareas simples, RNN puede ser suficiente y m√°s r√°pido. LSTM brilla en dependencias largas.

**P: ¬øBidirectional LSTM para generaci√≥n de texto?**  
R: No. Bidirectional requiere toda la secuencia. Para generaci√≥n (predicci√≥n del futuro), usa LSTM unidireccional.

**P: ¬øCu√°ntas capas usar en Stacked LSTM?**  
R: T√≠picamente 2-3 capas. M√°s de 4 raramente ayuda y aumenta overfitting.

## Verificaci√≥n de Conocimientos

- [ ] Entiendo c√≥mo las RNNs procesan secuencias paso a paso
- [ ] Puedo explicar el problema del gradiente que desaparece
- [ ] Entiendo los 3 gates de LSTM y su prop√≥sito
- [ ] S√© implementar RNN y LSTM desde cero
- [ ] Conozco la diferencia entre RNN, LSTM y GRU
- [ ] Puedo aplicar RNNs/LSTMs a problemas de NLP y series de tiempo
- [ ] Entiendo cu√°ndo usar bidirectional vs unidirectional
