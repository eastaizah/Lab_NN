# Laboratorio de Redes Neuronales y Deep Learning

Conjunto de gu铆as y pr谩cticas de laboratorio en Python sobre redes neuronales, deep learning e inteligencia artificial generativa. Este curso est谩 dise帽ado para iniciarse desde cero, con un enfoque muy did谩ctico basado en el libro "Neural Networks from Scratch in Python".

##  Contenido del Curso

### [Lab 01: Introducci贸n a las Neuronas](Lab01_Introduccion_Neuronas/)
Fundamentos de las redes neuronales. Implementaci贸n de una neurona desde cero.
- Teor铆a: Conceptos b谩sicos de neuronas artificiales
- Pr谩ctica: Implementaci贸n de una neurona simple
- C贸digo: Neurona con pesos y bias

### [Lab 02: Primera Red Neuronal](Lab02_Primera_Red_Neuronal/)
Construcci贸n de la primera red neuronal completa desde cero.
- Teor铆a: Arquitectura de redes neuronales
- Pr谩ctica: Capas de neuronas
- C贸digo: Red neuronal multicapa

### [Lab 03: Funciones de Activaci贸n](Lab03_Funciones_Activacion/)
Exploraci贸n de diferentes funciones de activaci贸n.
- Teor铆a: Prop贸sito y tipos de funciones de activaci贸n
- Pr谩ctica: ReLU, Sigmoid, Softmax, Tanh
- C贸digo: Implementaci贸n desde cero

### [Lab 04: Funciones de P茅rdida y Optimizaci贸n](Lab04_Funciones_Perdida/)
Medici贸n del error y optimizaci贸n de redes neuronales.
- Teor铆a: Funciones de costo y optimizaci贸n
- Pr谩ctica: Cross-Entropy, MSE, MAE
- C贸digo: C谩lculo de p茅rdida

### [Lab 05: Backpropagation](Lab05_Backpropagation/)
Algoritmo de retropropagaci贸n para entrenar redes neuronales.
- Teor铆a: Derivadas y regla de la cadena
- Pr谩ctica: C谩lculo de gradientes
- C贸digo: Backpropagation desde cero

### [Lab 06: Entrenamiento de Redes Neuronales](Lab06_Entrenamiento/)
Proceso completo de entrenamiento de una red neuronal.
- Teor铆a: Descenso de gradiente, learning rate, epochs
- Pr谩ctica: Entrenamiento con datos reales
- C贸digo: Loop de entrenamiento completo

### [Lab 07: M茅tricas de Evaluaci贸n y Matriz de Confusi贸n](Lab07_Metricas_Evaluacion/)
Evaluaci贸n rigurosa de modelos de clasificaci贸n.
- Teor铆a: Matriz de confusi贸n, Accuracy, Precision, Recall, F1-Score
- Pr谩ctica: Validaci贸n cruzada, datasets balanceados y desbalanceados
- C贸digo: Implementaci贸n de m茅tricas desde cero, optimizaci贸n de umbrales

### [Lab 08: Frameworks de Deep Learning](Lab08_Frameworks_DeepLearning/)
Introducci贸n a PyTorch y TensorFlow.
- Teor铆a: Ventajas de usar frameworks
- Pr谩ctica: Comparaci贸n de implementaciones
- C贸digo: Redes neuronales con PyTorch y TensorFlow

### [Lab 09: Inteligencia Artificial Generativa](Lab09_IA_Generativa/)
Introducci贸n a modelos generativos modernos.
- Teor铆a: VAE, GAN, Diffusion Models, aplicaciones con Transformers
- Pr谩ctica: Tipos de modelos generativos, generaci贸n de contenido
- C贸digo: Modelo generativo simple, integraci贸n con arquitecturas modernas

### [Lab 10: Redes Neuronales Convolucionales (CNN)](Lab10_Redes_Neuronales_Convolucionales/)
Arquitecturas especializadas para procesamiento de im谩genes y visi贸n computacional.
- Teor铆a: Convoluci贸n, pooling, arquitecturas famosas (LeNet, ResNet, VGG)
- Pr谩ctica: Implementaci贸n de CNN desde cero, clasificaci贸n de im谩genes
- C贸digo: Capas convolucionales, filtros, CNN completa en PyTorch

### [Lab 11: Redes Neuronales Recurrentes y LSTM](Lab11_Redes_Neuronales_Recurrentes_LSTM/)
Arquitecturas para datos secuenciales como texto y series de tiempo.
- Teor铆a: RNN, LSTM, GRU, problema del gradiente que desaparece
- Pr谩ctica: Procesamiento de secuencias, predicci贸n de series temporales
- C贸digo: RNN y LSTM desde cero, clasificaci贸n de texto, generaci贸n

### [Lab 12: Transformers](Lab12_Transformers/)
Arquitectura revolucionaria basada en atenci贸n para NLP y m谩s.
- Teor铆a: Self-Attention, Multi-Head Attention, BERT, GPT, Vision Transformers
- Pr谩ctica: Implementaci贸n de Transformers, fine-tuning de modelos
- C贸digo: Attention desde cero, Hugging Face, aplicaciones modernas

##  C贸mo Empezar

### Requisitos Previos
- Python 3.8 o superior
- Conocimientos b谩sicos de programaci贸n en Python
- Conocimientos b谩sicos de matem谩ticas (谩lgebra lineal, c谩lculo)

### Instalaci贸n

1. Clonar el repositorio:
```bash
git clone https://github.com/eastaizah/Lab_NN.git
cd Lab_NN
```

2. Crear un entorno virtual (recomendado):
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. Instalar dependencias:
```bash
pip install -r requirements.txt
```

### Uso

Cada laboratorio contiene:
- `teoria.md`: Documento con fundamentos te贸ricos
- `practica.ipynb`: Jupyter notebook con ejercicios pr谩cticos
- `codigo/`: Directorio con implementaciones de ejemplo

Se recomienda seguir los laboratorios en orden, ya que cada uno construye sobre los conceptos del anterior.

##  Metodolog铆a

Este curso sigue una filosof铆a did谩ctica basada en "Neural Networks from Scratch in Python" y expandida a arquitecturas modernas:
1. **Entender los fundamentos**: Implementar todo desde cero antes de usar librer铆as
2. **Aprendizaje pr谩ctico**: C贸digo ejecutable en cada laboratorio
3. **Progresi贸n gradual**: De conceptos simples a arquitecturas complejas
4. **Visualizaci贸n**: Gr谩ficos y ejemplos visuales en cada tema
5. **Del fundamento a la pr谩ctica**: Desde implementaciones NumPy hasta modelos de producci贸n

##  Ruta de Aprendizaje

El curso est谩 organizado en **tres m贸dulos pedag贸gicos**:

### M贸dulo 1: Fundamentos (Labs 01-07)
Construcci贸n de redes neuronales desde cero con NumPy
- Neuronas y arquitecturas b谩sicas
- Funciones de activaci贸n y p茅rdida
- Backpropagation y optimizaci贸n
- Entrenamiento completo
- **Evaluaci贸n y m茅tricas de clasificaci贸n**

### M贸dulo 2: Frameworks y Arquitecturas Modernas (Labs 08, 10-12)
Arquitecturas especializadas y herramientas profesionales
- PyTorch y TensorFlow
- **CNNs** para visi贸n computacional
- **RNNs/LSTMs** para secuencias y texto
- **Transformers** para NLP y aplicaciones multimodales

### M贸dulo 3: IA Generativa (Lab 09)
Modelos generativos modernos
- VAEs y GANs
- Diffusion Models
- Integraci贸n con Transformers (GPT, DALL-E)

##  Contribuir

Las contribuciones son bienvenidas. Por favor, abre un issue o pull request para sugerencias o mejoras.

##  Licencia

Este proyecto es de c贸digo abierto y est谩 disponible bajo la licencia MIT.

##  Referencias

- Harrison Kinsley & Daniel Kukiea. "Neural Networks from Scratch in Python"
- Ian Goodfellow, Yoshua Bengio, Aaron Courville. "Deep Learning"
- Michael Nielsen. "Neural Networks and Deep Learning"
