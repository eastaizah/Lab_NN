# Lab 10: Redes Neuronales Convolucionales (CNN)

## Descripci√≥n

Este laboratorio introduce las Redes Neuronales Convolucionales (CNN), arquitecturas especializadas para procesamiento de datos con estructura de cuadr√≠cula como im√°genes. Implementaremos desde cero los componentes fundamentales de una CNN y exploraremos sus aplicaciones en visi√≥n computacional.

## Objetivos de Aprendizaje

Al completar este laboratorio, podr√°s:

1. ‚úÖ Comprender la arquitectura de una CNN y sus componentes
2. ‚úÖ Implementar capas convolucionales desde cero
3. ‚úÖ Entender pooling y sus variantes (max pooling, average pooling)
4. ‚úÖ Construir arquitecturas CNN completas
5. ‚úÖ Aplicar CNNs a problemas de clasificaci√≥n de im√°genes
6. ‚úÖ Entender conceptos de padding, stride, y receptive field
7. ‚úÖ Conocer arquitecturas CNN famosas (LeNet, AlexNet, VGG, ResNet)

## Contenido

### üìñ Teor√≠a (`teoria.md`)

Documento completo con los fundamentos te√≥ricos:
- ¬øPor qu√© CNNs para im√°genes?
- Operaci√≥n de convoluci√≥n
- Filtros y feature maps
- Capas de pooling
- Arquitecturas CNN completas
- CNNs vs Redes totalmente conectadas
- Arquitecturas CNN famosas

### üíª Pr√°ctica (`practica.ipynb`)

Jupyter Notebook interactivo con:
- Implementaci√≥n de convoluci√≥n 2D desde cero
- Construcci√≥n de capas CNN
- Visualizaci√≥n de filtros y activaciones
- Entrenamiento de CNN en MNIST
- Comparaci√≥n con redes densas
- Ejercicios progresivos

### üîß C√≥digo de Ejemplo (`codigo/cnn.py`)

Script Python con implementaciones completas:
- Funci√≥n `convolve2d()`: Operaci√≥n de convoluci√≥n
- Clase `CapaConvolucional`: Capa CNN completa
- Clase `CapaPooling`: Max y average pooling
- Clase `CNN`: Red convolucional completa
- Ejemplos de arquitecturas

## C√≥mo Usar Este Laboratorio

### Opci√≥n 1: Jupyter Notebook (Recomendado)

```bash
# Desde el directorio del repositorio
cd Lab09_Redes_Neuronales_Convolucionales
jupyter notebook practica.ipynb
```

### Opci√≥n 2: Script Python

```bash
# Ejecutar el c√≥digo de ejemplo
python codigo/cnn.py
```

### Opci√≥n 3: Lectura y Experimentaci√≥n

1. Lee `teoria.md` para entender los conceptos
2. Abre `practica.ipynb` en Jupyter
3. Ejecuta cada celda y experimenta con los par√°metros
4. Completa los ejercicios propuestos
5. Revisa `codigo/cnn.py` como referencia

## Requisitos

```bash
pip install numpy matplotlib jupyter torch torchvision
```

## Conceptos Clave

- **Convoluci√≥n**: Operaci√≥n que aplica filtros para detectar caracter√≠sticas
- **Filtro/Kernel**: Matriz de pesos que se desliza sobre la entrada
- **Feature Map**: Resultado de aplicar un filtro a la entrada
- **Pooling**: Reducci√≥n de dimensionalidad espacial
- **Stride**: Paso del desplazamiento del filtro
- **Padding**: Relleno de bordes para controlar tama√±o de salida
- **Receptive Field**: Regi√≥n de la entrada que afecta a una neurona

## Ejercicios

### Ejercicio 9.1: Convoluci√≥n Manual
Implementa una convoluci√≥n 2D sin usar bucles, solo operaciones NumPy.

### Ejercicio 9.2: Filtros Personalizados
Crea filtros para detectar bordes horizontales, verticales y diagonales.

### Ejercicio 9.3: CNN en MNIST
Construye y entrena una CNN simple en el dataset MNIST.

### Ejercicio 9.4: Visualizaci√≥n de Activaciones
Visualiza qu√© caracter√≠sticas aprende cada capa de la CNN.

### Ejercicio 9.5: Arquitectura Personalizada (Desaf√≠o)
Dise√±a tu propia arquitectura CNN para clasificar CIFAR-10.

## Ventajas de las CNNs

1. **Invariancia a Traslaci√≥n**: Detectan caracter√≠sticas sin importar posici√≥n
2. **Compartici√≥n de Par√°metros**: Menos par√°metros que redes densas
3. **Jerarqu√≠a de Caracter√≠sticas**: Aprenden desde bordes hasta objetos
4. **Eficiencia Computacional**: Aprovechan estructura local de im√°genes

## Arquitecturas CNN Famosas

### LeNet-5 (1998)
- Primera CNN exitosa
- MNIST: 99%+ precisi√≥n
- Arquitectura: CONV ‚Üí POOL ‚Üí CONV ‚Üí POOL ‚Üí FC

### AlexNet (2012)
- Ganadora ImageNet 2012
- Populariz√≥ deep learning
- 8 capas, ReLU, Dropout

### VGG (2014)
- Capas convolucionales 3x3 apiladas
- Arquitectura muy profunda (16-19 capas)
- Simple pero efectiva

### ResNet (2015)
- Conexiones residuales (skip connections)
- Permite entrenar redes muy profundas (>100 capas)
- Soluciona problema de gradientes que desaparecen

## Aplicaciones

- **Clasificaci√≥n de Im√°genes**: Reconocer objetos en fotos
- **Detecci√≥n de Objetos**: YOLO, Faster R-CNN
- **Segmentaci√≥n Sem√°ntica**: U-Net, Mask R-CNN
- **Reconocimiento Facial**: FaceNet, DeepFace
- **Diagn√≥stico M√©dico**: Detecci√≥n de tumores en radiograf√≠as
- **Veh√≠culos Aut√≥nomos**: Detecci√≥n de se√±ales, peatones
- **Arte y Estilo**: Neural Style Transfer

## Notas Importantes

‚ö†Ô∏è **Dimensiones**: Presta atenci√≥n a las dimensiones de entrada/salida en cada capa.

üí° **Visualizaci√≥n**: Visualizar filtros y activaciones ayuda a entender qu√© aprende la red.

üöÄ **Transfer Learning**: En la pr√°ctica, se suelen usar redes pre-entrenadas y hacer fine-tuning.

## F√≥rmulas Importantes

### Tama√±o de salida de convoluci√≥n:
```
Output_size = (Input_size - Kernel_size + 2*Padding) / Stride + 1
```

### N√∫mero de par√°metros en capa convolucional:
```
Params = (Kernel_height * Kernel_width * Input_channels + 1) * Num_filters
```

## Pr√≥ximo Paso

Una vez completes este laboratorio, contin√∫a con:

üëâ **[Lab 11: Redes Neuronales Recurrentes y LSTM](../Lab11_Redes_Neuronales_Recurrentes_LSTM/)**

Exploraremos arquitecturas especializadas para datos secuenciales como texto y series de tiempo.

## Recursos Adicionales

- [CS231n: Convolutional Neural Networks](http://cs231n.stanford.edu/)
- [Visualizaci√≥n de CNNs](https://poloclub.github.io/cnn-explainer/)
- [PyTorch CNN Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)
- [Distill.pub - Feature Visualization](https://distill.pub/2017/feature-visualization/)
- [Neural Style Transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)

## Preguntas Frecuentes

**P: ¬øPor qu√© las CNNs funcionan mejor que redes densas para im√°genes?**  
R: Aprovechan la estructura espacial de las im√°genes, usan menos par√°metros gracias a la compartici√≥n de pesos, y son invariantes a la traslaci√≥n.

**P: ¬øQu√© tama√±o de kernel es mejor?**  
R: Kernels 3x3 son los m√°s comunes por su balance entre campo receptivo y par√°metros. A veces se usan 1x1 para cambiar dimensionalidad.

**P: ¬øCu√°ndo usar padding?**  
R: "same" padding mantiene tama√±o espacial, √∫til en redes profundas. "valid" (sin padding) reduce tama√±o, √∫til para reducir dimensionalidad.

**P: ¬øMax pooling o average pooling?**  
R: Max pooling es m√°s com√∫n porque preserva caracter√≠sticas m√°s fuertes. Average pooling suaviza pero pierde informaci√≥n.

## Verificaci√≥n de Conocimientos

- [ ] Entiendo c√≥mo funciona la operaci√≥n de convoluci√≥n
- [ ] Puedo calcular dimensiones de salida de capas CNN
- [ ] S√© implementar convoluci√≥n y pooling desde cero
- [ ] Entiendo la diferencia entre CNNs y redes densas
- [ ] Conozco arquitecturas CNN famosas y sus innovaciones
- [ ] Puedo construir y entrenar una CNN con PyTorch/TensorFlow
