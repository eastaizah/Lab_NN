# Gu√≠a de Laboratorio: Redes Neuronales Convolucionales (CNN)

## üìã Informaci√≥n del Laboratorio

**T√≠tulo:** Redes Neuronales Convolucionales ‚Äî Visi√≥n Computacional con Deep Learning  
**C√≥digo:** Lab 10  
**Duraci√≥n:** 3-4 horas  
**Nivel:** Avanzado  

---

## üéØ Objetivos Espec√≠ficos

Al completar este laboratorio, ser√°s capaz de:

1. Explicar por qu√© las redes densas son ineficientes para im√°genes y c√≥mo las CNNs resuelven ese problema
2. Implementar la operaci√≥n de convoluci√≥n 2D desde cero usando NumPy
3. Aplicar filtros cl√°sicos de visi√≥n computacional (bordes, blur, Sobel)
4. Calcular dimensiones de salida usando la f√≥rmula: `(Input - Kernel + 2¬∑Padding) / Stride + 1`
5. Implementar capas de Max Pooling y Average Pooling desde cero
6. Construir una arquitectura CNN completa con PyTorch
7. Entrenar una CNN en el dataset MNIST y alcanzar >98% de precisi√≥n
8. Visualizar filtros aprendidos y feature maps de activaci√≥n
9. Comparar cuantitativamente el rendimiento de CNN vs red densa
10. Entender el concepto de campo receptivo y jerarqu√≠a de caracter√≠sticas
11. Describir las innovaciones de LeNet, AlexNet, VGG y ResNet
12. Implementar skip connections al estilo ResNet
13. Aplicar Transfer Learning con modelos pre-entrenados
14. Utilizar t√©cnicas de regularizaci√≥n espec√≠ficas para CNNs (Dropout, Batch Normalization, Data Augmentation)

---

## üìö Prerrequisitos

### Conocimientos

- Python intermedio-avanzado y NumPy (Labs 01‚Äì09)
- Redes neuronales densas y backpropagation (Labs 02, 05)
- Funciones de activaci√≥n, p√©rdida y optimizadores (Labs 03, 04, 06)
- PyTorch b√°sico: tensores, autograd, `nn.Module` (Lab 08)
- √Ålgebra lineal: multiplicaci√≥n de matrices, suma elemento a elemento

### Software

- Python 3.8+
- PyTorch 1.10+ (`pip install torch torchvision`)
- NumPy, Matplotlib, Scipy
- Jupyter Notebook (opcional pero recomendado)

```bash
pip install torch torchvision numpy matplotlib scipy pillow
```

### Material de Lectura

Antes de comenzar, lee:
- `teoria.md` ‚Äî Fundamentos te√≥ricos completos de CNNs
- `README.md` ‚Äî Estructura y recursos del laboratorio
- [CS231n Lecture Notes ‚Äî CNNs](http://cs231n.github.io/convolutional-networks/)
- LeCun et al. (1998): *Gradient-Based Learning Applied to Document Recognition*

---

## üìñ Introducci√≥n

### El Problema con las Redes Densas para Im√°genes

En los laboratorios anteriores construiste redes neuronales completamente conectadas (dense networks). Funcionan bien para datos tabulares, pero presentan serios problemas cuando se aplican a im√°genes:

**Problema 1 ‚Äî Explosi√≥n de par√°metros:**
```
Imagen 224√ó224 RGB = 224 √ó 224 √ó 3 = 150,528 p√≠xeles
Primera capa densa con 1,000 neuronas:
  150,528 √ó 1,000 = 150,528,000 par√°metros ‚Üê ¬°s√≥lo en la primera capa!
```

**Problema 2 ‚Äî Ignorar estructura espacial:**  
Una red densa trata el p√≠xel en la esquina superior izquierda y el del centro como totalmente independientes. Sin embargo, los p√≠xeles cercanos est√°n correlacionados: forman bordes, texturas y formas.

**Problema 3 ‚Äî No hay invariancia:**  
Si el mismo objeto aparece desplazado 5 p√≠xeles a la derecha, la red densa lo trata como una entrada completamente diferente.

### La Soluci√≥n: Redes Neuronales Convolucionales

Las CNNs resuelven los tres problemas anteriores mediante tres principios:

| Principio | Descripci√≥n | Beneficio |
|---|---|---|
| **Conectividad local** | Cada neurona se conecta s√≥lo a una peque√±a regi√≥n de la entrada | Menos par√°metros |
| **Compartici√≥n de pesos** | El mismo filtro se aplica a toda la imagen | Invariancia a traslaci√≥n |
| **Jerarqu√≠a de caracter√≠sticas** | Capas tempranas: bordes ‚Üí medias: texturas ‚Üí tard√≠as: objetos | Representaciones ricas |

### Motivaci√≥n Hist√≥rica

- **1998 ‚Äî LeNet-5** (Yann LeCun): Primera CNN exitosa, reconoc√≠a d√≠gitos manuscritos con >99% de precisi√≥n.
- **2012 ‚Äî AlexNet** (Krizhevsky, Hinton): Gan√≥ ImageNet con 15.3% de error, 10 puntos por debajo del segundo lugar. Marc√≥ el inicio del auge del Deep Learning.
- **2014 ‚Äî VGGNet**: Simplific√≥ el dise√±o usando s√≥lo kernels 3√ó3, llegando a 16-19 capas.
- **2015 ‚Äî ResNet**: Introdujo skip connections y permiti√≥ entrenar redes de +100 capas.
- **Hoy**: CNNs en todos lados ‚Äî diagn√≥stico m√©dico, veh√≠culos aut√≥nomos, reconocimiento facial, arte generativo.

### Aplicaciones Pr√°cticas

- üè• Diagn√≥stico m√©dico: detecci√≥n de tumores en radiograf√≠as y tomograf√≠as
- üöó Veh√≠culos aut√≥nomos: detecci√≥n de se√±ales, peatones y carriles
- üì± Filtros de c√°mara: efectos de arte y realidad aumentada
- üîç B√∫squeda visual: encontrar im√°genes similares
- üè≠ Control de calidad industrial: detecci√≥n de defectos
- üåç An√°lisis de im√°genes satelitales: mapas, deforestaci√≥n, etc.

---

## ü§î Preguntas de Reflexi√≥n Iniciales

Antes de comenzar a programar, reflexiona sobre las siguientes preguntas:

1. **¬øPor qu√© dos p√≠xeles vecinos en una imagen suelen tener valores similares?** ¬øC√≥mo aprovecha la convoluci√≥n esta propiedad?

2. **Si tienes una imagen de 28√ó28 y aplicas un filtro de 5√ó5 con stride=1 y sin padding, ¬øcu√°l es el tama√±o de la salida?** ¬øCu√°ntos p√≠xeles "pierdes" en cada borde?

3. **¬øPor qu√© el Max Pooling 2√ó2 reduce el tama√±o espacial a la mitad?** ¬øQu√© informaci√≥n se pierde y qu√© se conserva?

4. **Una CNN entrenada para reconocer gatos aprende filtros de bordes en las primeras capas.** ¬øPor qu√© crees que eso es as√≠? ¬øQu√© aprender√°n las √∫ltimas capas?

5. **Skip connections en ResNet suman la entrada con la salida de un bloque.** ¬øPor qu√© esto ayuda al flujo de gradientes durante el entrenamiento?

6. **¬øCu√°l es la diferencia entre Transfer Learning y entrenar desde cero?** ¬øCu√°ndo usar√≠as cada estrategia?

7. **La misma operaci√≥n de convoluci√≥n que detecta bordes en im√°genes se usa en audio y texto.** ¬øQu√© caracter√≠sticas detectar√≠a en esos dominios?

---

## üî¨ Parte 1: Operaciones de Convoluci√≥n (45 min)

### 1.1 Convoluci√≥n 2D desde Cero

La convoluci√≥n es el coraz√≥n de las CNNs. Antes de usar frameworks, es fundamental entender la operaci√≥n matem√°ticamente.

**Definici√≥n matem√°tica:**
```
Output[i, j] = Œ£_m Œ£_n Input[i+m, j+n] √ó Kernel[m, n]
```

```python
import numpy as np
import matplotlib.pyplot as plt

# ============================================================
# IMPLEMENTACI√ìN DE CONVOLUCI√ìN 2D DESDE CERO
# ============================================================

def convolve2d_manual(input_matrix, kernel, stride=1, padding=0):
    """
    Implementaci√≥n de convoluci√≥n 2D sin librer√≠as de deep learning.
    
    Args:
        input_matrix: np.array de forma (H, W)
        kernel:       np.array de forma (Kh, Kw)
        stride:       paso del desplazamiento
        padding:      relleno de ceros en los bordes
    
    Returns:
        output: np.array de forma (Oh, Ow)
    """
    H, W = input_matrix.shape
    Kh, Kw = kernel.shape
    
    # Aplicar padding si es necesario
    if padding > 0:
        input_padded = np.pad(input_matrix, padding, mode='constant', constant_values=0)
    else:
        input_padded = input_matrix
    
    # Calcular tama√±o de salida
    Oh = (H - Kh + 2 * padding) // stride + 1
    Ow = (W - Kw + 2 * padding) // stride + 1
    
    output = np.zeros((Oh, Ow))
    
    for i in range(Oh):
        for j in range(Ow):
            # Extraer ventana de la entrada
            region = input_padded[i*stride : i*stride + Kh,
                                  j*stride : j*stride + Kw]
            # Producto elemento a elemento y suma
            output[i, j] = np.sum(region * kernel)
    
    return output


# --- Ejemplo con una imagen simple ---
imagen = np.array([
    [1, 2, 3, 4, 5],
    [1, 2, 3, 4, 5],
    [1, 2, 3, 4, 5],
    [1, 2, 3, 4, 5],
    [1, 2, 3, 4, 5]
], dtype=float)

kernel_bordes = np.array([
    [-1,  0,  1],
    [-1,  0,  1],
    [-1,  0,  1]
], dtype=float)

resultado = convolve2d_manual(imagen, kernel_bordes)
print("Imagen original (5√ó5):")
print(imagen)
print("\nKernel de bordes verticales (3√ó3):")
print(kernel_bordes)
print("\nResultado de la convoluci√≥n (3√ó3):")
print(resultado)

# --- Verificar f√≥rmula de dimensiones ---
H, W = 5, 5
Kh, Kw = 3, 3
stride, padding = 1, 0

Oh = (H - Kh + 2 * padding) // stride + 1
Ow = (W - Kw + 2 * padding) // stride + 1
print(f"\nF√≥rmula: ({H} - {Kh} + 2√ó{padding}) / {stride} + 1 = {Oh}")
print(f"Salida esperada: {Oh}√ó{Ow}")
print(f"Salida obtenida: {resultado.shape[0]}√ó{resultado.shape[1]}")
```

**Salida esperada:**
```
Resultado de la convoluci√≥n (3√ó3):
[[-6. -6. -6.]
 [-6. -6. -6.]
 [-6. -6. -6.]]
F√≥rmula: (5 - 3 + 2√ó0) / 1 + 1 = 3
Salida esperada: 3√ó3
```

> üí° **¬øPor qu√© los valores son negativos?** El kernel detecta transiciones de oscuro a claro (izquierda ‚Üí derecha). Un valor negativo indica borde de claro a oscuro.

---

### 1.2 Filtros Cl√°sicos

La visi√≥n computacional cl√°sica define filtros a mano. Las CNNs los **aprenden autom√°ticamente**, pero entender los filtros cl√°sicos da intuici√≥n sobre lo que la red descubre.

```python
# ============================================================
# FILTROS CL√ÅSICOS DE VISI√ìN COMPUTACIONAL
# ============================================================

from scipy.ndimage import convolve
from PIL import Image

# Crear imagen sint√©tica de prueba (gradiente + bordes)
def crear_imagen_prueba(size=64):
    img = np.zeros((size, size), dtype=float)
    # Cuadrado blanco en el centro
    img[16:48, 16:48] = 255.0
    # Gradiente horizontal
    for col in range(size):
        img[:, col] += col * (255 / size) * 0.3
    return np.clip(img, 0, 255)

imagen_prueba = crear_imagen_prueba(64)

# --- Definici√≥n de filtros cl√°sicos ---
filtros = {
    "Bordes Verticales": np.array([
        [-1,  0,  1],
        [-1,  0,  1],
        [-1,  0,  1]
    ], dtype=float),
    
    "Bordes Horizontales": np.array([
        [-1, -1, -1],
        [ 0,  0,  0],
        [ 1,  1,  1]
    ], dtype=float),
    
    "Sobel X": np.array([
        [-1,  0,  1],
        [-2,  0,  2],
        [-1,  0,  1]
    ], dtype=float),
    
    "Sobel Y": np.array([
        [-1, -2, -1],
        [ 0,  0,  0],
        [ 1,  2,  1]
    ], dtype=float),
    
    "Desenfoque (Blur)": np.ones((3, 3), dtype=float) / 9,
    
    "Laplaciano": np.array([
        [ 0, -1,  0],
        [-1,  4, -1],
        [ 0, -1,  0]
    ], dtype=float),
    
    "Realce de Nitidez": np.array([
        [ 0, -1,  0],
        [-1,  5, -1],
        [ 0, -1,  0]
    ], dtype=float),
}

# --- Aplicar y visualizar cada filtro ---
fig, axes = plt.subplots(2, 4, figsize=(18, 9))
axes[0, 0].imshow(imagen_prueba, cmap='gray', vmin=0, vmax=255)
axes[0, 0].set_title("Imagen Original", fontsize=12, fontweight='bold')
axes[0, 0].axis('off')

for idx, (nombre, filtro) in enumerate(filtros.items()):
    row, col = (idx + 1) // 4, (idx + 1) % 4
    resultado = convolve(imagen_prueba, filtro)
    resultado_vis = np.abs(resultado)
    axes[row, col].imshow(resultado_vis, cmap='gray')
    axes[row, col].set_title(nombre, fontsize=10)
    axes[row, col].axis('off')
    
    # Mostrar el filtro como texto peque√±o
    filtro_str = '\n'.join([' '.join([f'{v:+.1f}' for v in row]) for row in filtro])
    axes[row, col].text(0.02, 0.02, filtro_str, transform=axes[row, col].transAxes,
                        fontsize=6, color='white', family='monospace',
                        bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))

plt.suptitle("Filtros Cl√°sicos de Visi√≥n Computacional", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('filtros_clasicos.png', dpi=100, bbox_inches='tight')
plt.show()
print("‚úÖ Figura guardada: filtros_clasicos.png")

# --- Gradiente de Sobel (magnitud) ---
sobel_x = convolve(imagen_prueba, filtros["Sobel X"])
sobel_y = convolve(imagen_prueba, filtros["Sobel Y"])
magnitud = np.sqrt(sobel_x**2 + sobel_y**2)
direccion = np.arctan2(sobel_y, sobel_x) * (180 / np.pi)

print(f"\nMagnitud Sobel ‚Äî min: {magnitud.min():.1f}, max: {magnitud.max():.1f}")
print(f"Direcci√≥n Sobel ‚Äî min: {direccion.min():.1f}¬∞, max: {direccion.max():.1f}¬∞")
```

---

### 1.3 Padding y Stride

Estos dos hiperpar√°metros controlan el tama√±o de salida y qu√© informaci√≥n se preserva.

```python
# ============================================================
# PADDING Y STRIDE ‚Äî AN√ÅLISIS DETALLADO
# ============================================================

def calcular_dimensiones_salida(H, W, Kh, Kw, stride, padding):
    """Calcula dimensiones de salida y las imprime con detalle."""
    Oh = (H - Kh + 2 * padding) // stride + 1
    Ow = (W - Kw + 2 * padding) // stride + 1
    return Oh, Ow

print("=" * 60)
print("TABLA DE DIMENSIONES DE SALIDA")
print("=" * 60)
print(f"{'Config':<35} {'Entrada':<12} {'Salida'}")
print("-" * 60)

configs = [
    # (H, W, Kh, Kw, stride, padding, descripci√≥n)
    (32, 32, 3, 3, 1, 0, "Valid (sin padding)"),
    (32, 32, 3, 3, 1, 1, "Same (padding=1)"),
    (32, 32, 3, 3, 2, 0, "Stride=2 (sin padding)"),
    (32, 32, 3, 3, 2, 1, "Stride=2 (padding=1)"),
    (28, 28, 5, 5, 1, 0, "Kernel 5√ó5 Valid"),
    (28, 28, 5, 5, 1, 2, "Kernel 5√ó5 Same"),
    (64, 64, 7, 7, 2, 3, "AlexNet Conv1 style"),
    (14, 14, 3, 3, 1, 1, "VGG style"),
]

for H, W, Kh, Kw, s, p, desc in configs:
    Oh, Ow = calcular_dimensiones_salida(H, W, Kh, Kw, s, p)
    print(f"{desc:<35} {H}√ó{W}{'':5} ‚Üí {Oh}√ó{Ow}")

print()
print("TIPOS DE PADDING:")
print("  - 'valid' (p=0): sin relleno, salida m√°s peque√±a")
print("  - 'same'  (p=(K-1)/2): mantiene tama√±o con stride=1")
print()

# --- Demostraci√≥n visual de stride ---
print("EFECTO DEL STRIDE EN UNA IMAGEN 6√ó6 CON KERNEL 2√ó2:")
print()
imagen_6x6 = np.arange(1, 37).reshape(6, 6)
print("Imagen:")
print(imagen_6x6)

for stride_val in [1, 2, 3]:
    resultado = convolve2d_manual(imagen_6x6.astype(float),
                                  np.ones((2, 2)) / 4,
                                  stride=stride_val, padding=0)
    Oh = (6 - 2 + 0) // stride_val + 1
    print(f"\nStride={stride_val} ‚Üí salida {Oh}√ó{Oh}:")
    print(resultado.astype(int))

# --- Padding 'same' exacto ---
def padding_para_same(kernel_size):
    """Calcula el padding necesario para mantener tama√±o (stride=1)."""
    return (kernel_size - 1) // 2

print("\nPADDING NECESARIO PARA 'SAME' (stride=1):")
for k in [1, 3, 5, 7, 9]:
    p = padding_para_same(k)
    print(f"  Kernel {k}√ó{k} ‚Üí padding = {p}")
```

---

## üî¨ Parte 2: Capas CNN (45 min)

### 2.1 Capa Convolucional

Una capa convolucional aprende sus filtros durante el entrenamiento. Aqu√≠ implementamos la estructura completa con m√∫ltiples filtros y canales.

```python
# ============================================================
# CAPA CONVOLUCIONAL COMPLETA
# ============================================================

class CapaConvolucional:
    """
    Capa convolucional con N filtros, soporte para m√∫ltiples canales.
    Implementaci√≥n educativa con NumPy.
    """
    
    def __init__(self, num_filtros, kernel_size, num_canales=1,
                 stride=1, padding=0, seed=42):
        """
        Args:
            num_filtros:  N√∫mero de filtros (feature maps de salida)
            kernel_size:  Tama√±o del kernel (kernel_size √ó kernel_size)
            num_canales:  Canales de entrada (1=gris, 3=RGB)
            stride:       Paso del filtro
            padding:      Relleno de ceros
            seed:         Semilla para reproducibilidad
        """
        self.num_filtros = num_filtros
        self.kernel_size = kernel_size
        self.num_canales = num_canales
        self.stride = stride
        self.padding = padding
        
        # Inicializaci√≥n He (recomendada para ReLU)
        rng = np.random.default_rng(seed)
        escala = np.sqrt(2.0 / (kernel_size * kernel_size * num_canales))
        self.pesos = rng.normal(0, escala, 
                                (num_filtros, num_canales, kernel_size, kernel_size))
        self.bias = np.zeros(num_filtros)
        
    def convolve_canal(self, entrada, kernel, stride, padding):
        """Convoluci√≥n de una entrada 2D con un kernel 2D."""
        H, W = entrada.shape
        K = kernel.shape[0]
        
        if padding > 0:
            entrada = np.pad(entrada, padding, mode='constant')
        
        Oh = (H - K + 2 * padding) // stride + 1
        Ow = (W - K + 2 * padding) // stride + 1
        salida = np.zeros((Oh, Ow))
        
        for i in range(Oh):
            for j in range(Ow):
                region = entrada[i*stride:i*stride+K, j*stride:j*stride+K]
                salida[i, j] = np.sum(region * kernel)
        return salida
    
    def forward(self, entrada):
        """
        Forward pass de la capa convolucional.
        
        Args:
            entrada: np.array de forma (C, H, W) o (H, W) para 1 canal
        Returns:
            salida: np.array de forma (num_filtros, Oh, Ow)
        """
        if entrada.ndim == 2:
            entrada = entrada[np.newaxis, :]   # (1, H, W)
        
        C, H, W = entrada.shape
        K = self.kernel_size
        p = self.padding
        s = self.stride
        
        Oh = (H - K + 2 * p) // s + 1
        Ow = (W - K + 2 * p) // s + 1
        
        salida = np.zeros((self.num_filtros, Oh, Ow))
        
        for f in range(self.num_filtros):
            mapa = np.zeros((Oh, Ow))
            for c in range(C):
                mapa += self.convolve_canal(entrada[c], self.pesos[f, c],
                                            self.stride, self.padding)
            salida[f] = mapa + self.bias[f]
        
        return salida
    
    def info(self):
        """Muestra informaci√≥n de la capa."""
        total_params = self.pesos.size + self.bias.size
        print(f"CapaConvolucional:")
        print(f"  Filtros:     {self.num_filtros}")
        print(f"  Kernel:      {self.kernel_size}√ó{self.kernel_size}")
        print(f"  Canales in:  {self.num_canales}")
        print(f"  Stride:      {self.stride}")
        print(f"  Padding:     {self.padding}")
        print(f"  Par√°metros:  {total_params:,}")


# --- Demostraci√≥n ---
np.random.seed(42)

# Imagen de entrada: 3 canales (RGB), 32√ó32
imagen_rgb = np.random.randn(3, 32, 32)

# Capa conv: 16 filtros de 3√ó3
capa_conv = CapaConvolucional(num_filtros=16, kernel_size=3,
                              num_canales=3, stride=1, padding=1)
capa_conv.info()

salida_conv = capa_conv.forward(imagen_rgb)
print(f"\nEntrada:  {imagen_rgb.shape}  (C, H, W)")
print(f"Salida:   {salida_conv.shape}  (Filtros, Oh, Ow)")

# Verificar dimensiones
Oh_esperado = (32 - 3 + 2*1) // 1 + 1
print(f"Oh esperado: {Oh_esperado}")

# Estad√≠sticas de la salida
print(f"\nEstad√≠sticas de la salida:")
print(f"  Media:  {salida_conv.mean():.4f}")
print(f"  Std:    {salida_conv.std():.4f}")
print(f"  Min:    {salida_conv.min():.4f}")
print(f"  Max:    {salida_conv.max():.4f}")
```

---

### 2.2 Pooling (Max y Average)

El pooling reduce el tama√±o espacial manteniendo las caracter√≠sticas m√°s importantes.

```python
# ============================================================
# CAPAS DE POOLING ‚Äî MAX Y AVERAGE
# ============================================================

class CapaPooling:
    """
    Capa de pooling con soporte para Max y Average pooling.
    """
    
    def __init__(self, pool_size=2, stride=None, modo='max'):
        """
        Args:
            pool_size: Tama√±o de la ventana de pooling
            stride:    Paso (por defecto = pool_size)
            modo:      'max' o 'average'
        """
        self.pool_size = pool_size
        self.stride = stride if stride else pool_size
        self.modo = modo
    
    def _pool_2d(self, entrada):
        """Aplica pooling a una entrada 2D."""
        H, W = entrada.shape
        P = self.pool_size
        S = self.stride
        
        Oh = (H - P) // S + 1
        Ow = (W - P) // S + 1
        salida = np.zeros((Oh, Ow))
        
        for i in range(Oh):
            for j in range(Ow):
                ventana = entrada[i*S:i*S+P, j*S:j*S+P]
                if self.modo == 'max':
                    salida[i, j] = np.max(ventana)
                else:
                    salida[i, j] = np.mean(ventana)
        return salida
    
    def forward(self, entrada):
        """
        Forward pass.
        Args:
            entrada: np.array (C, H, W)
        Returns:
            salida:  np.array (C, Oh, Ow)
        """
        if entrada.ndim == 2:
            return self._pool_2d(entrada)
        
        C = entrada.shape[0]
        salida = [self._pool_2d(entrada[c]) for c in range(C)]
        return np.array(salida)
    
    def info(self):
        print(f"CapaPooling({self.modo}):")
        print(f"  Pool size: {self.pool_size}√ó{self.pool_size}")
        print(f"  Stride:    {self.stride}")
        print(f"  Par√°metros: 0 (sin pesos aprendibles)")


# --- Demostraci√≥n de Max vs Average Pooling ---
mapa_features = np.array([
    [ 1,  3,  2,  4],
    [ 5,  6,  7,  8],
    [ 9,  2,  1,  3],
    [ 4,  5, 10, 11]
], dtype=float)

max_pool = CapaPooling(pool_size=2, modo='max')
avg_pool = CapaPooling(pool_size=2, modo='average')

resultado_max = max_pool.forward(mapa_features)
resultado_avg = avg_pool.forward(mapa_features)

print("Mapa de features (4√ó4):")
print(mapa_features)
print(f"\nMax Pooling 2√ó2 (resultado {resultado_max.shape}):")
print(resultado_max)
print(f"\nAverage Pooling 2√ó2 (resultado {resultado_avg.shape}):")
print(resultado_avg)

# --- Aplicar pooling a la salida de la capa conv anterior ---
max_pool_capa = CapaPooling(pool_size=2, modo='max')
max_pool_capa.info()

salida_pool = max_pool_capa.forward(salida_conv)
print(f"\nDespu√©s de Max Pooling 2√ó2:")
print(f"  Antes del pooling: {salida_conv.shape}")
print(f"  Despu√©s del pooling: {salida_pool.shape}")
print(f"  Reducci√≥n: {salida_conv.shape[1]}√ó{salida_conv.shape[2]} ‚Üí "
      f"{salida_pool.shape[1]}√ó{salida_pool.shape[2]}")

# --- Comparaci√≥n de propiedades ---
print("\n" + "=" * 50)
print("COMPARACI√ìN MAX vs AVERAGE POOLING")
print("=" * 50)
print(f"{'Propiedad':<30} {'Max':<15} {'Average'}")
print("-" * 50)
propiedades = [
    ("Par√°metros aprendibles", "0", "0"),
    ("Preserva caracter√≠sticas fuertes", "‚úÖ S√≠", "‚ùå Suaviza"),
    ("Invariancia a traslaci√≥n", "‚úÖ Alta", "‚úÖ Moderada"),
    ("Uso t√≠pico", "Redes gen.", "GAP final"),
    ("Diferenciable en m√°ximo", "‚ùå No siempre", "‚úÖ S√≠"),
]
for prop, max_val, avg_val in propiedades:
    print(f"{prop:<30} {max_val:<15} {avg_val}")
```

---

### 2.3 Flatten y Capas Densas

Despu√©s de las capas convolucionales, se transforma el tensor 3D en un vector 1D para las capas totalmente conectadas.

```python
# ============================================================
# FLATTEN, RELU Y CAPAS DENSAS
# ============================================================

def relu(x):
    """Funci√≥n de activaci√≥n ReLU."""
    return np.maximum(0, x)

def softmax(x):
    """Funci√≥n softmax estable num√©ricamente."""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

class Flatten:
    """Convierte tensor 3D en vector 1D."""
    
    def __init__(self):
        self.forma_entrada = None
    
    def forward(self, entrada):
        self.forma_entrada = entrada.shape
        return entrada.flatten()
    
    def info(self, forma_entrada=None):
        if forma_entrada:
            C, H, W = forma_entrada
            n = C * H * W
            print(f"Flatten: {C}√ó{H}√ó{W} ‚Üí {n} (vector)")
        else:
            print("Flatten: 3D ‚Üí 1D")


class CapaDensa:
    """Capa totalmente conectada con activaci√≥n opcional."""
    
    def __init__(self, n_entrada, n_salida, activacion=None, seed=42):
        rng = np.random.default_rng(seed)
        escala = np.sqrt(2.0 / n_entrada)  # Inicializaci√≥n He
        self.W = rng.normal(0, escala, (n_salida, n_entrada))
        self.b = np.zeros(n_salida)
        self.activacion = activacion
    
    def forward(self, x):
        z = self.W @ x + self.b
        if self.activacion == 'relu':
            return relu(z)
        elif self.activacion == 'softmax':
            return softmax(z)
        return z
    
    def info(self):
        params = self.W.size + self.b.size
        print(f"CapaDensa: {self.W.shape[1]} ‚Üí {self.W.shape[0]} "
              f"| act={self.activacion} | params={params:,}")


# --- Pipeline completo: Conv ‚Üí Pool ‚Üí Flatten ‚Üí Dense ---
print("PIPELINE CNN COMPLETO")
print("=" * 50)

# Datos de entrada (imagen gris 28√ó28)
entrada = np.random.randn(1, 28, 28)
print(f"Entrada:     {entrada.shape}")

# Capa Conv 1: 8 filtros 3√ó3, padding=1
conv1 = CapaConvolucional(num_filtros=8, kernel_size=3,
                          num_canales=1, padding=1)
x = relu(conv1.forward(entrada))
print(f"Conv1+ReLU:  {x.shape}  (8 filtros √ó 28√ó28)")

# Max Pooling 2√ó2
pool1 = CapaPooling(pool_size=2, modo='max')
x = pool1.forward(x)
print(f"MaxPool:     {x.shape}  (8 √ó 14√ó14)")

# Capa Conv 2: 16 filtros 3√ó3, padding=1
conv2 = CapaConvolucional(num_filtros=16, kernel_size=3,
                          num_canales=8, padding=1)
x = relu(conv2.forward(x))
print(f"Conv2+ReLU:  {x.shape}  (16 filtros √ó 14√ó14)")

# Max Pooling 2√ó2
pool2 = CapaPooling(pool_size=2, modo='max')
x = pool2.forward(x)
print(f"MaxPool:     {x.shape}  (16 √ó 7√ó7)")

# Flatten
flatten = Flatten()
x_flat = flatten.forward(x)
print(f"Flatten:     {x_flat.shape}  ({16*7*7} elementos)")

# Capas densas
dense1 = CapaDensa(16*7*7, 128, activacion='relu')
x = dense1.forward(x_flat)
print(f"Dense+ReLU:  {x.shape}")

dense2 = CapaDensa(128, 10, activacion='softmax')
x = dense2.forward(x)
print(f"Dense+Softmax: {x.shape}")

print(f"\nPredicciones (probabilidades):")
print(x.round(4))
print(f"Clase predicha: {np.argmax(x)}")
print(f"Suma de probs:  {x.sum():.4f}")
```

---

## üî¨ Parte 3: Arquitectura CNN Completa (60 min)

### 3.1 Arquitectura CNN con PyTorch

Ahora construimos una CNN real usando PyTorch, aprovechando GPU, autograd y optimizadores modernos.

```python
# ============================================================
# CNN COMPLETA CON PYTORCH
# ============================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# Configuraci√≥n del dispositivo
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Dispositivo: {device}")

# ---- Definici√≥n de la arquitectura ----
class CNN_MNIST(nn.Module):
    """
    CNN para clasificaci√≥n de d√≠gitos MNIST.
    
    Arquitectura:
      INPUT (1√ó28√ó28)
      CONV1 (32 filtros 3√ó3, padding=1) ‚Üí BN ‚Üí ReLU ‚Üí (32√ó28√ó28)
      MaxPool 2√ó2 ‚Üí (32√ó14√ó14)
      CONV2 (64 filtros 3√ó3, padding=1) ‚Üí BN ‚Üí ReLU ‚Üí (64√ó14√ó14)
      MaxPool 2√ó2 ‚Üí (64√ó7√ó7)
      CONV3 (128 filtros 3√ó3, padding=1) ‚Üí BN ‚Üí ReLU ‚Üí (128√ó7√ó7)
      Flatten ‚Üí (128√ó7√ó7 = 6272)
      FC1 (256) ‚Üí ReLU ‚Üí Dropout(0.5)
      FC2 (10) ‚Üí Output
    """
    
    def __init__(self, num_clases=10, dropout=0.5):
        super(CNN_MNIST, self).__init__()
        
        # Bloque convolucional 1
        self.conv_block1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        # Bloque convolucional 2
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        
        # Bloque convolucional 3
        self.conv_block3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        # Capas clasificadoras
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 7 * 7, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(p=dropout),
            nn.Linear(256, num_clases)
        )
    
    def forward(self, x):
        x = self.conv_block1(x)
        x = self.conv_block2(x)
        x = self.conv_block3(x)
        x = self.classifier(x)
        return x


# ---- Instanciar y revisar el modelo ----
modelo = CNN_MNIST(num_clases=10).to(device)
print("\nArquitectura del modelo:")
print(modelo)

# Contar par√°metros
total_params = sum(p.numel() for p in modelo.parameters())
trainable_params = sum(p.numel() for p in modelo.parameters() if p.requires_grad)
print(f"\nPar√°metros totales:       {total_params:,}")
print(f"Par√°metros entrenables:   {trainable_params:,}")

# Probar con una imagen de ejemplo
imagen_test = torch.randn(1, 1, 28, 28).to(device)
with torch.no_grad():
    salida_test = modelo(imagen_test)
print(f"\nForma de salida (1 imagen): {salida_test.shape}")
print(f"Logits: {salida_test.detach().cpu().numpy().round(3)}")
```

---

### 3.2 Entrenamiento en MNIST

```python
# ============================================================
# ENTRENAMIENTO EN MNIST
# ============================================================

def entrenar_una_epoca(modelo, loader, optimizer, criterion, device):
    modelo.train()
    total_loss = 0
    total_correcto = 0
    total_muestras = 0
    
    for batch_idx, (datos, etiquetas) in enumerate(loader):
        datos, etiquetas = datos.to(device), etiquetas.to(device)
        
        optimizer.zero_grad()
        salida = modelo(datos)
        loss = criterion(salida, etiquetas)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * len(datos)
        predicciones = salida.argmax(dim=1)
        total_correcto += (predicciones == etiquetas).sum().item()
        total_muestras += len(datos)
    
    return total_loss / total_muestras, total_correcto / total_muestras


def evaluar(modelo, loader, criterion, device):
    modelo.eval()
    total_loss = 0
    total_correcto = 0
    total_muestras = 0
    
    with torch.no_grad():
        for datos, etiquetas in loader:
            datos, etiquetas = datos.to(device), etiquetas.to(device)
            salida = modelo(datos)
            loss = criterion(salida, etiquetas)
            
            total_loss += loss.item() * len(datos)
            predicciones = salida.argmax(dim=1)
            total_correcto += (predicciones == etiquetas).sum().item()
            total_muestras += len(datos)
    
    return total_loss / total_muestras, total_correcto / total_muestras


# --- Preparar datos ---
transform_train = transforms.Compose([
    transforms.RandomRotation(10),          # Augmentaci√≥n: rotaci√≥n aleatoria ¬±10¬∞
    transforms.RandomAffine(0, translate=(0.1, 0.1)),  # Traslaci√≥n aleatoria
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # Media y std de MNIST
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)
test_dataset  = datasets.MNIST('./data', train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,  num_workers=2)
test_loader  = DataLoader(test_dataset,  batch_size=256, shuffle=False, num_workers=2)

print(f"Datos de entrenamiento: {len(train_dataset):,} im√°genes")
print(f"Datos de prueba:        {len(test_dataset):,} im√°genes")
print(f"Batches por √©poca:      {len(train_loader)}")

# --- Configurar entrenamiento ---
modelo = CNN_MNIST(num_clases=10, dropout=0.5).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(modelo.parameters(), lr=0.001, weight_decay=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)

# --- Ciclo de entrenamiento ---
NUM_EPOCHS = 10
historial = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

print("\n" + "="*65)
print(f"{'√âpoca':<8} {'LR':<10} {'Train Loss':<12} {'Train Acc':<12} {'Val Acc'}")
print("="*65)

mejor_val_acc = 0.0

for epoch in range(1, NUM_EPOCHS + 1):
    train_loss, train_acc = entrenar_una_epoca(modelo, train_loader, optimizer, criterion, device)
    val_loss, val_acc     = evaluar(modelo, test_loader, criterion, device)
    scheduler.step()
    
    historial['train_loss'].append(train_loss)
    historial['train_acc'].append(train_acc)
    historial['val_loss'].append(val_loss)
    historial['val_acc'].append(val_acc)
    
    if val_acc > mejor_val_acc:
        mejor_val_acc = val_acc
        torch.save(modelo.state_dict(), 'mejor_cnn_mnist.pth')
        marca = "‚≠ê"
    else:
        marca = ""
    
    lr_actual = scheduler.get_last_lr()[0]
    print(f"{epoch:<8} {lr_actual:<10.6f} {train_loss:<12.4f} {train_acc:<12.4f} {val_acc:.4f} {marca}")

print("="*65)
print(f"\n‚úÖ Mejor precisi√≥n de validaci√≥n: {mejor_val_acc:.4f} ({mejor_val_acc*100:.2f}%)")

# --- Graficar curvas de entrenamiento ---
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# P√©rdida
axes[0].plot(historial['train_loss'], 'b-o', label='Entrenamiento', markersize=4)
axes[0].plot(historial['val_loss'], 'r-o', label='Validaci√≥n', markersize=4)
axes[0].set_xlabel('√âpoca')
axes[0].set_ylabel('P√©rdida (Cross-Entropy)')
axes[0].set_title('Curva de P√©rdida')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Precisi√≥n
axes[1].plot([a*100 for a in historial['train_acc']], 'b-o', label='Entrenamiento', markersize=4)
axes[1].plot([a*100 for a in historial['val_acc']], 'r-o', label='Validaci√≥n', markersize=4)
axes[1].set_xlabel('√âpoca')
axes[1].set_ylabel('Precisi√≥n (%)')
axes[1].set_title('Curva de Precisi√≥n')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].set_ylim([90, 100])

plt.tight_layout()
plt.savefig('curvas_entrenamiento_cnn.png', dpi=100, bbox_inches='tight')
plt.show()
print("‚úÖ Figura guardada: curvas_entrenamiento_cnn.png")
```

---

### 3.3 Visualizaci√≥n de Filtros y Feature Maps

Una de las caracter√≠sticas m√°s poderosas de las CNNs es que sus filtros son interpretables. Visualizarlos revela qu√© aprende la red.

```python
# ============================================================
# VISUALIZACI√ìN DE FILTROS Y FEATURE MAPS
# ============================================================

# Cargar el mejor modelo
modelo.load_state_dict(torch.load('mejor_cnn_mnist.pth', map_location=device))
modelo.eval()

# ---- 1. Visualizar filtros de la primera capa convolucional ----
filtros_conv1 = modelo.conv_block1[0].weight.data.cpu().numpy()
print(f"Filtros Conv1: {filtros_conv1.shape}")  # (32, 1, 3, 3)

fig, axes = plt.subplots(4, 8, figsize=(16, 8))
for idx, ax in enumerate(axes.flat):
    if idx < 32:
        filtro = filtros_conv1[idx, 0]
        im = ax.imshow(filtro, cmap='RdBu_r',
                       vmin=-filtro.abs().max(),
                       vmax=filtro.abs().max())
        ax.set_title(f'F{idx+1}', fontsize=8)
    ax.axis('off')

plt.suptitle('Filtros Aprendidos ‚Äî Capa Conv1 (32 filtros 3√ó3)', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig('filtros_conv1.png', dpi=100, bbox_inches='tight')
plt.show()
print("‚úÖ Figura guardada: filtros_conv1.png")

# ---- 2. Visualizar Feature Maps de activaci√≥n ----
# Registrar activaciones con hooks
activaciones = {}

def obtener_activacion(nombre):
    def hook(model, input, output):
        activaciones[nombre] = output.detach()
    return hook

# Registrar hooks en cada bloque conv
hook1 = modelo.conv_block1[2].register_forward_hook(obtener_activacion('relu1'))
hook2 = modelo.conv_block2[2].register_forward_hook(obtener_activacion('relu2'))
hook3 = modelo.conv_block3[2].register_forward_hook(obtener_activacion('relu3'))

# Pasar una imagen de prueba
imagen_muestra = test_dataset[0][0].unsqueeze(0).to(device)
etiqueta_muestra = test_dataset[0][1]

with torch.no_grad():
    pred = modelo(imagen_muestra)

clase_pred = pred.argmax().item()
confianza = F.softmax(pred, dim=1).max().item()

# Eliminar hooks
hook1.remove(); hook2.remove(); hook3.remove()

# Visualizar feature maps de cada bloque
for nombre_bloque, num_mostrar in [('relu1', 32), ('relu2', 32), ('relu3', 16)]:
    maps = activaciones[nombre_bloque][0].cpu().numpy()
    n_cols = 8
    n_rows = num_mostrar // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 2))
    for idx, ax in enumerate(axes.flat):
        if idx < num_mostrar:
            ax.imshow(maps[idx], cmap='viridis')
            ax.set_title(f'Ch{idx+1}', fontsize=7)
        ax.axis('off')
    
    titulo = (f"Feature Maps ‚Äî {nombre_bloque.upper()}\n"
              f"Imagen: d√≠gito={etiqueta_muestra} | "
              f"Pred={clase_pred} | Confianza={confianza:.2%}")
    plt.suptitle(titulo, fontsize=11, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'feature_maps_{nombre_bloque}.png', dpi=90, bbox_inches='tight')
    plt.show()
    print(f"‚úÖ Figura guardada: feature_maps_{nombre_bloque}.png")

# ---- 3. Imagen original + predicciones por clase ----
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Imagen original
img_display = test_dataset[0][0].squeeze().numpy()
axes[0].imshow(img_display, cmap='gray')
axes[0].set_title(f'Imagen original (d√≠gito: {etiqueta_muestra})', fontsize=12)
axes[0].axis('off')

# Probabilidades por clase
probabilidades = F.softmax(pred, dim=1)[0].cpu().numpy()
colores = ['green' if i == clase_pred else 'steelblue' for i in range(10)]
axes[1].bar(range(10), probabilidades * 100, color=colores)
axes[1].set_xlabel('Clase (d√≠gito)')
axes[1].set_ylabel('Probabilidad (%)')
axes[1].set_title(f'Predicci√≥n: {clase_pred} ({confianza:.2%} confianza)', fontsize=12)
axes[1].set_xticks(range(10))
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('prediccion_cnn.png', dpi=100, bbox_inches='tight')
plt.show()
```

---

## üî¨ Parte 4: Arquitecturas Avanzadas (30 min)

### 4.1 Skip Connections al Estilo ResNet

ResNet resolvi√≥ el problema de la degradaci√≥n en redes muy profundas al aprender el **residuo** en lugar de la transformaci√≥n completa.

```python
# ============================================================
# SKIP CONNECTIONS ‚Äî BLOQUES RESIDUALES (ResNet)
# ============================================================

class BloqueResidual(nn.Module):
    """
    Bloque residual b√°sico de ResNet.
    
    Arquitectura:
      x ‚Üí Conv ‚Üí BN ‚Üí ReLU ‚Üí Conv ‚Üí BN ‚Üí (+x) ‚Üí ReLU
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              (skip connection)
    """
    
    def __init__(self, canales, stride=1):
        super(BloqueResidual, self).__init__()
        
        self.bloque = nn.Sequential(
            nn.Conv2d(canales, canales, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(canales),
            nn.ReLU(inplace=True),
            nn.Conv2d(canales, canales, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(canales)
        )
        
        # Proyecci√≥n si el stride cambia las dimensiones
        self.proyeccion = None
        if stride != 1:
            self.proyeccion = nn.Sequential(
                nn.Conv2d(canales, canales, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(canales)
            )
    
    def forward(self, x):
        identidad = x
        salida = self.bloque(x)
        
        if self.proyeccion:
            identidad = self.proyeccion(x)
        
        return F.relu(salida + identidad)  # ‚Üê Suma residual


class BloqueResidualCuello(nn.Module):
    """
    Bloque bottleneck de ResNet-50/101/152.
    Reduce par√°metros con conv 1√ó1 antes y despu√©s de la conv 3√ó3.
    
    Arquitectura:
      x ‚Üí Conv1√ó1 ‚Üí BN ‚Üí ReLU ‚Üí Conv3√ó3 ‚Üí BN ‚Üí ReLU ‚Üí Conv1√ó1 ‚Üí BN ‚Üí (+x) ‚Üí ReLU
    """
    
    expansion = 4
    
    def __init__(self, canales_entrada, canales_cuello, stride=1):
        super(BloqueResidualCuello, self).__init__()
        canales_salida = canales_cuello * self.expansion
        
        self.bloque = nn.Sequential(
            # 1√ó1: reducir canales
            nn.Conv2d(canales_entrada, canales_cuello, kernel_size=1, bias=False),
            nn.BatchNorm2d(canales_cuello),
            nn.ReLU(inplace=True),
            # 3√ó3: convoluci√≥n principal
            nn.Conv2d(canales_cuello, canales_cuello, kernel_size=3,
                      stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(canales_cuello),
            nn.ReLU(inplace=True),
            # 1√ó1: expandir canales
            nn.Conv2d(canales_cuello, canales_salida, kernel_size=1, bias=False),
            nn.BatchNorm2d(canales_salida)
        )
        
        self.proyeccion = nn.Sequential(
            nn.Conv2d(canales_entrada, canales_salida, kernel_size=1,
                      stride=stride, bias=False),
            nn.BatchNorm2d(canales_salida)
        )
    
    def forward(self, x):
        return F.relu(self.bloque(x) + self.proyeccion(x))


# --- Probar bloques residuales ---
batch_x = torch.randn(4, 64, 28, 28)  # Batch de 4 im√°genes

bloque_res = BloqueResidual(canales=64)
salida_res = bloque_res(batch_x)
print(f"BloqueResidual: {batch_x.shape} ‚Üí {salida_res.shape}")

bloque_cuello = BloqueResidualCuello(canales_entrada=64, canales_cuello=16)
salida_cuello = bloque_cuello(batch_x)
print(f"BloqueBottleneck: {batch_x.shape} ‚Üí {salida_cuello.shape}")

# --- Comparar gradientes: con y sin skip connections ---
print("\n" + "="*55)
print("AN√ÅLISIS DE GRADIENTES: Con vs Sin Skip Connections")
print("="*55)

class RedProfundaSinSkip(nn.Module):
    def __init__(self, n_bloques=8):
        super().__init__()
        self.capas = nn.ModuleList([
            nn.Sequential(nn.Linear(64, 64), nn.ReLU())
            for _ in range(n_bloques)
        ])
        self.salida = nn.Linear(64, 10)
    
    def forward(self, x):
        for capa in self.capas:
            x = capa(x)
        return self.salida(x)


class RedProfundaConSkip(nn.Module):
    def __init__(self, n_bloques=8):
        super().__init__()
        self.capas = nn.ModuleList([
            nn.Linear(64, 64) for _ in range(n_bloques)
        ])
        self.salida = nn.Linear(64, 10)
    
    def forward(self, x):
        for capa in self.capas:
            x = F.relu(capa(x)) + x   # ‚Üê Skip connection
        return self.salida(x)


x_test = torch.randn(32, 64)
y_test = torch.randint(0, 10, (32,))
criterion = nn.CrossEntropyLoss()

for nombre, red in [("Sin Skip", RedProfundaSinSkip(8)),
                    ("Con Skip", RedProfundaConSkip(8))]:
    red.zero_grad()
    loss = criterion(red(x_test), y_test)
    loss.backward()
    
    normas = [p.grad.norm().item() for p in red.parameters() if p.grad is not None]
    print(f"\n{nombre} connections:")
    print(f"  Norma del gradiente (primera capa): {normas[0]:.6f}")
    print(f"  Norma del gradiente (√∫ltima capa):  {normas[-1]:.6f}")
    print(f"  Ratio √∫ltima/primera: {normas[-1]/max(normas[0], 1e-10):.2f}")
```

---

### 4.2 Transfer Learning con PyTorch

```python
# ============================================================
# TRANSFER LEARNING ‚Äî ESTRATEGIAS
# ============================================================

from torchvision import models

print("="*55)
print("TRANSFER LEARNING ‚Äî ESTRATEGIAS PRINCIPALES")
print("="*55)

# --- Estrategia 1: Feature Extraction (congelar todo menos la cabeza) ---
def crear_modelo_feature_extraction(num_clases, backbone='resnet18'):
    """
    Usa una red pre-entrenada como extractor de caracter√≠sticas.
    Solo entrena la √∫ltima capa lineal.
    Ideal cuando tienes muy pocos datos de entrenamiento.
    """
    if backbone == 'resnet18':
        modelo = models.resnet18(pretrained=False)  # En prod: pretrained=True
        # Congelar todos los par√°metros
        for param in modelo.parameters():
            param.requires_grad = False
        # Reemplazar s√≥lo la cabeza de clasificaci√≥n
        in_features = modelo.fc.in_features
        modelo.fc = nn.Linear(in_features, num_clases)
    return modelo


# --- Estrategia 2: Fine-Tuning (descongelar algunas capas) ---
def crear_modelo_fine_tuning(num_clases, capas_a_entrenar=2):
    """
    Fine-tuning: descongela s√≥lo las √∫ltimas N capas del backbone.
    Ideal con datos moderados y cuando el dominio es similar a ImageNet.
    """
    modelo = models.resnet18(pretrained=False)  # En prod: pretrained=True
    
    # Congelar todo primero
    for param in modelo.parameters():
        param.requires_grad = False
    
    # Descongelar las √∫ltimas capas
    capas_resnet = [modelo.layer1, modelo.layer2, modelo.layer3, modelo.layer4]
    for capa in capas_resnet[-capas_a_entrenar:]:
        for param in capa.parameters():
            param.requires_grad = True
    
    # Reemplazar la cabeza
    in_features = modelo.fc.in_features
    modelo.fc = nn.Sequential(
        nn.Linear(in_features, 256),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(256, num_clases)
    )
    
    return modelo


# Demostraci√≥n
modelo_fe = crear_modelo_feature_extraction(num_clases=10)
modelo_ft = crear_modelo_fine_tuning(num_clases=10, capas_a_entrenar=2)

def contar_params(modelo):
    total = sum(p.numel() for p in modelo.parameters())
    entrenable = sum(p.numel() for p in modelo.parameters() if p.requires_grad)
    return total, entrenable

total_fe, train_fe = contar_params(modelo_fe)
total_ft, train_ft = contar_params(modelo_ft)

print(f"\nFeature Extraction:")
print(f"  Par√°metros totales:     {total_fe:,}")
print(f"  Par√°metros entrenables: {train_fe:,}")
print(f"  Porcentaje entrenado:   {train_fe/total_fe*100:.1f}%")

print(f"\nFine-Tuning (2 √∫ltimas capas):")
print(f"  Par√°metros totales:     {total_ft:,}")
print(f"  Par√°metros entrenables: {train_ft:,}")
print(f"  Porcentaje entrenado:   {train_ft/total_ft*100:.1f}%")

print("\nGU√çA DE SELECCI√ìN DE ESTRATEGIA:")
print("  Pocos datos + dominio similar  ‚Üí Feature Extraction")
print("  Datos moderados + similar       ‚Üí Fine-Tuning parcial")
print("  Muchos datos + diferente        ‚Üí Entrenar desde cero")
print("  Muchos datos + similar          ‚Üí Fine-Tuning completo")
```

---

## üìä An√°lisis de Rendimiento

### Comparaci√≥n CNN vs Red Densa en MNIST

```python
# ============================================================
# BENCHMARK: CNN vs RED DENSA
# ============================================================

# ---- Red densa equivalente ----
class RedDensa_MNIST(nn.Module):
    """
    Red totalmente conectada para MNIST.
    Misma cantidad aproximada de par√°metros que CNN_MNIST.
    """
    def __init__(self, dropout=0.5):
        super(RedDensa_MNIST, self).__init__()
        self.red = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.red(x)


def entrenar_y_evaluar(modelo, train_loader, test_loader,
                       num_epochs=5, lr=0.001, device='cpu'):
    """Entrena un modelo y retorna m√©tricas finales."""
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(modelo.parameters(), lr=lr)
    
    for epoch in range(num_epochs):
        modelo.train()
        for datos, etiquetas in train_loader:
            datos, etiquetas = datos.to(device), etiquetas.to(device)
            optimizer.zero_grad()
            loss = criterion(modelo(datos), etiquetas)
            loss.backward()
            optimizer.step()
    
    # Evaluaci√≥n final
    modelo.eval()
    correcto = 0
    total = 0
    with torch.no_grad():
        for datos, etiquetas in test_loader:
            datos, etiquetas = datos.to(device), etiquetas.to(device)
            preds = modelo(datos).argmax(dim=1)
            correcto += (preds == etiquetas).sum().item()
            total += len(etiquetas)
    
    return correcto / total


import time

resultados = {}
modelos_comparar = {
    'CNN': CNN_MNIST(num_clases=10, dropout=0.3),
    'Red Densa': RedDensa_MNIST(dropout=0.3)
}

print("="*65)
print(f"{'Modelo':<15} {'Par√°metros':<15} {'Tiempo (s)':<15} {'Precisi√≥n (%)'}")
print("="*65)

for nombre, model in modelos_comparar.items():
    model = model.to(device)
    total_p, _ = contar_params(model)
    
    inicio = time.time()
    acc = entrenar_y_evaluar(model, train_loader, test_loader,
                             num_epochs=5, device=device)
    tiempo = time.time() - inicio
    
    resultados[nombre] = {'params': total_p, 'tiempo': tiempo, 'acc': acc}
    print(f"{nombre:<15} {total_p:<15,} {tiempo:<15.1f} {acc*100:.2f}")

print("="*65)

# --- Gr√°fica de comparaci√≥n ---
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

nombres = list(resultados.keys())
params_vals = [resultados[n]['params'] / 1000 for n in nombres]  # en miles
acc_vals = [resultados[n]['acc'] * 100 for n in nombres]
tiempo_vals = [resultados[n]['tiempo'] for n in nombres]

colores = ['#2196F3', '#FF5722']

axes[0].bar(nombres, params_vals, color=colores)
axes[0].set_ylabel('Par√°metros (miles)')
axes[0].set_title('Par√°metros del Modelo')
for i, v in enumerate(params_vals):
    axes[0].text(i, v + 0.5, f'{v:.0f}K', ha='center', fontweight='bold')

axes[1].bar(nombres, acc_vals, color=colores)
axes[1].set_ylabel('Precisi√≥n (%)')
axes[1].set_title('Precisi√≥n en Test')
axes[1].set_ylim([95, 100])
for i, v in enumerate(acc_vals):
    axes[1].text(i, v + 0.05, f'{v:.2f}%', ha='center', fontweight='bold')

axes[2].bar(nombres, tiempo_vals, color=colores)
axes[2].set_ylabel('Tiempo (segundos)')
axes[2].set_title('Tiempo de Entrenamiento (5 √©pocas)')
for i, v in enumerate(tiempo_vals):
    axes[2].text(i, v + 0.5, f'{v:.0f}s', ha='center', fontweight='bold')

plt.suptitle('CNN vs Red Densa ‚Äî Comparaci√≥n en MNIST', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.savefig('benchmark_cnn_vs_densa.png', dpi=100, bbox_inches='tight')
plt.show()
print("‚úÖ Figura guardada: benchmark_cnn_vs_densa.png")

# --- An√°lisis del campo receptivo ---
print("\n" + "="*55)
print("AN√ÅLISIS DEL CAMPO RECEPTIVO")
print("="*55)

def calcular_campo_receptivo(num_capas, kernel_size=3, stride=1):
    """
    Calcula el campo receptivo acumulado capa por capa.
    RF_l = RF_(l-1) + (kernel_size - 1) * stride_acumulado
    """
    rf = 1
    stride_acum = 1
    print(f"\nKernel={kernel_size}√ó{kernel_size}, Stride={stride}")
    print(f"{'Capa':<8} {'RF':<15} {'Stride acum.'}")
    print("-" * 35)
    print(f"{'Input':<8} {rf}√ó{rf}{'':10} {stride_acum}")
    
    for capa in range(1, num_capas + 1):
        rf += (kernel_size - 1) * stride_acum
        stride_acum *= stride
        print(f"{'Conv '+str(capa):<8} {rf}√ó{rf}{'':10} {stride_acum}")
    return rf

rf_final = calcular_campo_receptivo(num_capas=6, kernel_size=3, stride=1)
print(f"\nDespu√©s de 6 capas conv 3√ó3: campo receptivo = {rf_final}√ó{rf_final}")
print("(Cada neurona 've' una regi√≥n de 13√ó13 p√≠xeles de la imagen original)")
```

---

## üéØ EJERCICIOS PROPUESTOS

### Nivel B√°sico

**Ejercicio B1: Convoluci√≥n Manual con Filtro Personalizado**  
Implementa la funci√≥n `detectar_bordes_diagonales(imagen)` que aplique dos kernels personalizados para detectar bordes en direcci√≥n diagonal (+45¬∞ y -45¬∞). Retorna la magnitud combinada de ambas respuestas.

```python
# Plantilla de inicio
def detectar_bordes_diagonales(imagen):
    """
    Detecta bordes diagonales en una imagen en escala de grises.
    
    Kernel diagonal 1 (+45¬∞):      Kernel diagonal 2 (-45¬∞):
    [ 0  1  2]                     [ 2  1  0]
    [-1  0  1]                     [ 1  0 -1]
    [-2 -1  0]                     [ 0 -1 -2]
    
    Returns:
        magnitud: imagen con la magnitud de bordes diagonales
    """
    kernel_diag1 = # TODO: define el kernel +45¬∞
    kernel_diag2 = # TODO: define el kernel -45¬∞
    
    resp1 = convolve2d_manual(imagen, kernel_diag1)
    resp2 = convolve2d_manual(imagen, kernel_diag2)
    
    return # TODO: combina las respuestas
```

**Ejercicio B2: C√°lculo de Par√°metros**  
Dado el siguiente stack de capas, calcula manualmente el n√∫mero exacto de par√°metros de cada capa y el total. Verifica tu respuesta con PyTorch.

```
Input: 3√ó224√ó224 (imagen RGB)
Conv1: 64 filtros, 7√ó7, stride=2, padding=3
Conv2: 128 filtros, 3√ó3, stride=1, padding=1
Conv3: 256 filtros, 3√ó3, stride=1, padding=1
FC:    1000 clases
```

**Ejercicio B3: Convoluci√≥n Vectorizada**  
Reimplementa `convolve2d_manual` usando `numpy.lib.stride_tricks` para evitar los bucles `for` y hacer la operaci√≥n completamente vectorizada. Mide la diferencia de velocidad con `time.time()`.

---

### Nivel Intermedio

**Ejercicio I1: Clasificaci√≥n CIFAR-10**  
El dataset CIFAR-10 contiene im√°genes a color (3√ó32√ó32) de 10 categor√≠as. Dise√±a y entrena una CNN desde cero que alcance >80% de precisi√≥n en el conjunto de prueba.

```python
# Requisitos m√≠nimos de la arquitectura:
# - Al menos 4 capas convolucionales
# - Batch Normalization despu√©s de cada conv
# - Data Augmentation: flip horizontal, crop aleatorio, jitter de color
# - Dropout antes de la capa de clasificaci√≥n final
# - Scheduler de learning rate

from torchvision import datasets

transform_cifar_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465),
                         (0.2023, 0.1994, 0.2010))
])

# TODO: Define tu arquitectura CNN para CIFAR-10
class CNN_CIFAR10(nn.Module):
    def __init__(self):
        super().__init__()
        # Tu implementaci√≥n aqu√≠
        pass
    
    def forward(self, x):
        pass
```

**Ejercicio I2: Visualizaci√≥n de Saliency Maps**  
Implementa Gradient-based Saliency Maps para visualizar qu√© regiones de una imagen son m√°s importantes para la predicci√≥n de una CNN.

```python
def calcular_saliency_map(modelo, imagen_tensor, clase_objetivo):
    """
    Calcula el saliency map basado en gradientes.
    
    El saliency map indica qu√© p√≠xeles, si se modifican levemente,
    m√°s afectar√≠an la probabilidad de la clase objetivo.
    
    Returns:
        saliency: np.array (H, W) ‚Äî importancia de cada p√≠xel
    """
    imagen_tensor = imagen_tensor.unsqueeze(0).requires_grad_(True)
    
    modelo.zero_grad()
    salida = modelo(imagen_tensor)
    
    # Backprop respecto a la clase objetivo
    salida[0, clase_objetivo].backward()
    
    # El gradiente respecto a la imagen es el saliency
    saliency = # TODO: extrae el gradiente absoluto
    saliency = # TODO: reduce a 2D tomando el m√°ximo sobre los canales
    
    return saliency
```

**Ejercicio I3: Implementar Global Average Pooling (GAP)**  
Modifica la clase `CNN_MNIST` para reemplazar las capas densas con Global Average Pooling seguido de una √∫nica capa lineal. Compara el n√∫mero de par√°metros y la precisi√≥n resultante.

---

### Nivel Avanzado

**Ejercicio A1: Arquitectura ResNet-20 para CIFAR-10**  
Implementa completamente la arquitectura ResNet-20 original (He et al., 2016) para CIFAR-10 e intenta alcanzar >90% de precisi√≥n.

```python
class ResNet20_CIFAR10(nn.Module):
    """
    ResNet-20 para CIFAR-10 siguiendo el paper original.
    
    Estructura:
    - 1 capa conv inicial 3√ó3 con 16 filtros
    - 3 bloques de 3 capas residuales cada uno:
      * Bloque 1: 16 filtros, stride=1
      * Bloque 2: 32 filtros, stride=2
      * Bloque 3: 64 filtros, stride=2
    - Global Average Pooling
    - FC ‚Üí 10 clases
    
    Total: 6√ó3 + 2 = 20 capas aprendibles
    """
    def __init__(self):
        super().__init__()
        # TODO: Implementar la arquitectura completa
        pass
```

**Ejercicio A2: Depthwise Separable Convolutions (MobileNet)**  
Implementa el bloque de convoluci√≥n separable en profundidad (Depthwise Separable Convolution) al estilo MobileNet y mide la reducci√≥n en par√°metros y FLOPs comparado con una conv est√°ndar equivalente.

```python
class BloqueDepthwiseSeparable(nn.Module):
    """
    Convoluci√≥n separable en profundidad:
    
    Convoluci√≥n est√°ndar: H√óW√óCin√óCout√óK√óK par√°metros
    Separable:
      1. Depthwise: H√óW√óCin con kernel K√óK por canal ‚Üí Cin√óK√óK par√°metros
      2. Pointwise: 1√ó1 conv para combinar canales ‚Üí Cin√óCout par√°metros
    
    Reducci√≥n: ~K¬≤ veces menos par√°metros
    """
    def __init__(self, c_in, c_out, kernel_size=3, stride=1):
        super().__init__()
        # TODO: implementar
        pass
```

**Ejercicio A3: Detecci√≥n de Objetos con CNN (Proyecto)**  
Usando una CNN pre-entrenada como backbone, implementa un detector de objetos simple tipo YOLO-tiny para detectar objetos en el dataset Pascal VOC mini (subset de 5 clases). El modelo debe predecir bounding boxes y etiquetas de clase.

Requisitos:
- Dataset: Pascal VOC 2012 (5 clases: persona, gato, perro, coche, avi√≥n)
- Backbone: ResNet-18 pre-entrenado
- Cabeza de detecci√≥n: 3√ó3 conv + FC con salida `(S√óS√ó(5+C))` para grid `S=7`
- Funci√≥n de p√©rdida: MSE para coordenadas + Cross-Entropy para clases
- mAP@0.5 > 0.30

---

## üìù Entregables

### C√≥digo Fuente

| Archivo | Descripci√≥n |
|---|---|
| `convoluciones_numpy.py` | Implementaci√≥n de convoluci√≥n 2D y filtros cl√°sicos |
| `cnn_pytorch.py` | Arquitectura CNN completa con PyTorch |
| `entrenamiento_mnist.py` | Pipeline de entrenamiento en MNIST |
| `visualizaciones.py` | Filtros, feature maps, saliency maps |
| `benchmark.py` | Comparaci√≥n CNN vs red densa |
| `ejercicio_*.py` | Archivos de soluci√≥n de ejercicios |

### Modelos Guardados

- `mejor_cnn_mnist.pth` ‚Äî Pesos del mejor modelo CNN en MNIST
- `cnn_cifar10.pth` ‚Äî Modelo entrenado en CIFAR-10 (Ejercicio I1)

### Documentaci√≥n

- Notebook Jupyter documentado con resultados de cada experimento
- Gr√°ficas de curvas de entrenamiento con comentarios de an√°lisis
- Tabla comparativa de arquitecturas CNN analizadas

### Reporte Final

El reporte debe incluir (2-4 p√°ginas, formato libre):

1. **Experimentos realizados**: descripci√≥n breve de cada parte del laboratorio
2. **Resultados cuantitativos**: tablas con m√©tricas (precisi√≥n, p√©rdida, par√°metros)
3. **An√°lisis de filtros**: ¬øqu√© detectan los filtros aprendidos en la primera capa?
4. **Comparaci√≥n CNN vs Densa**: ¬øcu√°ndo vale la pena usar CNN?
5. **Reflexi√≥n final**: qu√© aprendiste y qu√© te sorprendi√≥ m√°s

---

## üéØ Criterios de Evaluaci√≥n (CDIO)

### Concebir (25%) ‚Äî Comprensi√≥n Conceptual

**Objetivo:** Demostrar comprensi√≥n profunda de los principios que hacen a las CNNs superiores para datos visuales.

‚úÖ **Evidencias esperadas:**
- Explica por qu√© una CNN necesita muchos menos par√°metros que una red densa equivalente
- Describe correctamente la jerarqu√≠a de caracter√≠sticas (bordes ‚Üí texturas ‚Üí objetos)
- Justifica la elecci√≥n de hiperpar√°metros: kernel size, n√∫mero de filtros, stride, padding
- Explica el problema de la degradaci√≥n y c√≥mo ResNet lo resuelve
- Relaciona Transfer Learning con el concepto de jerarqu√≠a de caracter√≠sticas

### Dise√±ar (25%) ‚Äî Dise√±o de Arquitecturas

**Objetivo:** Dise√±ar arquitecturas CNN apropiadas para distintas tareas y datasets.

‚úÖ **Evidencias esperadas:**
- Dise√±a una CNN para CIFAR-10 con justificaci√≥n expl√≠cita de cada capa
- Aplica la regla de duplicar filtros al reducir dimensionalidad espacial
- Incorpora Batch Normalization y Dropout en lugares apropiados
- Selecciona la estrategia correcta de Transfer Learning seg√∫n el tama√±o del dataset
- Calcula manualmente el n√∫mero de par√°metros de su arquitectura

### Implementar (30%) ‚Äî Implementaci√≥n y Entrenamiento

**Objetivo:** Implementar y entrenar CNNs funcionales con c√≥digo limpio y correcto.

‚úÖ **Evidencias esperadas:**
- Convoluci√≥n 2D implementada correctamente desde cero (verificada contra SciPy)
- CNN en PyTorch entrena sin errores y converge en MNIST (>98%)
- Implementa Data Augmentation con al menos 3 transformaciones
- Usa callbacks: guardado del mejor modelo, scheduler de LR
- Visualizaciones de filtros y feature maps generadas correctamente

### Operar (20%) ‚Äî An√°lisis y Operaci√≥n

**Objetivo:** Interpretar resultados, diagnosticar problemas y mejorar modelos.

‚úÖ **Evidencias esperadas:**
- Interpreta las curvas de p√©rdida y precisi√≥n (underfitting, overfitting, convergencia)
- Identifica filtros aprendidos con interpretaci√≥n visual razonable
- Compara CNN vs red densa con justificaci√≥n basada en datos
- Diagnostica y resuelve al menos un problema durante el entrenamiento (overfitting, NaN loss, etc.)
- Propone mejoras con fundamento t√©cnico

---

## üìã R√∫brica de Evaluaci√≥n

| Criterio | Excelente (100%) | Bueno (75%) | Aceptable (50%) | Insuficiente (25%) |
|---|---|---|---|---|
| **Convoluci√≥n desde cero** (15 pts) | Implementada sin errores, vectorizable, verificada contra SciPy; maneja padding/stride correctamente | Funciona correctamente con peque√±os casos pero sin validaci√≥n rigurosa | Implementada con alg√∫n error menor; resultado correcto en casos simples | No implementada o con errores conceptuales graves |
| **Filtros cl√°sicos** (10 pts) | Implementa ‚â•5 filtros, visualizaci√≥n clara, explica la interpretaci√≥n visual de cada uno | ‚â•3 filtros implementados con visualizaci√≥n | 2 filtros implementados | Solo 1 filtro o sin visualizaci√≥n |
| **F√≥rmulas de dimensiones** (10 pts) | Calcula correctamente todas las configuraciones de la tabla; justifica cada paso | Calcula correctamente la mayor√≠a con alg√∫n error puntual | Aplica la f√≥rmula con errores frecuentes | No aplica la f√≥rmula correctamente |
| **CNN PyTorch** (20 pts) | Arquitectura bien documentada, entrena >98% en MNIST, usa BN, Dropout y scheduler | >97% en MNIST con arquitectura razonable | >95% en MNIST | No entrena o <95% |
| **Visualizaciones** (10 pts) | Filtros y feature maps de las 3 capas visualizados e interpretados | 2 capas visualizadas | Solo 1 capa o sin interpretaci√≥n | Sin visualizaciones |
| **Skip Connections** (10 pts) | BloqueResidual implementado correctamente, an√°lisis de gradientes realizado | Implementado correctamente sin an√°lisis | Implementado con errores menores | No implementado |
| **Transfer Learning** (10 pts) | Ambas estrategias implementadas, comparaci√≥n cuantitativa, gu√≠a de selecci√≥n justificada | Una estrategia correcta | C√≥digo de transfer learning sin evaluaci√≥n | No implementado |
| **Benchmark CNN vs Densa** (10 pts) | Tabla completa con m√©tricas, gr√°fica comparativa, an√°lisis escrito de ventajas/desventajas | Comparaci√≥n num√©rica sin an√°lisis | Solo precisi√≥n comparada | Sin comparaci√≥n |
| **Reporte Final** (5 pts) | Reporte completo, an√°lisis profundo, conclusiones basadas en evidencia | Reporte con la mayor√≠a de secciones | Reporte incompleto pero con reflexi√≥n genuina | Sin reporte o copia de la gu√≠a |

**Escala:**
- 90-100: Sobresaliente
- 75-89: Bueno  
- 60-74: Aceptable
- <60: Necesita refuerzo

---

## üìö Referencias Adicionales

### Papers Fundamentales

1. **LeCun et al. (1998)** ‚Äî *Gradient-Based Learning Applied to Document Recognition*  
   El paper original de LeNet-5. Disponible en: [http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

2. **Krizhevsky, Sutskever & Hinton (2012)** ‚Äî *ImageNet Classification with Deep CNNs (AlexNet)*  
   El paper que inici√≥ el auge del Deep Learning moderno.

3. **Simonyan & Zisserman (2015)** ‚Äî *Very Deep CNNs for Large-Scale Image Recognition (VGGNet)*  
   Disponible en: [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)

4. **He et al. (2016)** ‚Äî *Deep Residual Learning for Image Recognition (ResNet)*  
   Disponible en: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)

5. **Howard et al. (2017)** ‚Äî *MobileNets: Efficient CNNs for Mobile Vision Applications*  
   Disponible en: [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)

### Tutoriales y Cursos

6. **CS231n Stanford** ‚Äî *Convolutional Neural Networks for Visual Recognition*  
   [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/) ‚Äî El mejor curso sobre CNNs

7. **CNN Explainer** ‚Äî Visualizaci√≥n interactiva de operaciones CNN  
   [https://poloclub.github.io/cnn-explainer/](https://poloclub.github.io/cnn-explainer/)

8. **PyTorch Tutorials ‚Äî Training a Classifier**  
   [https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)

9. **Distill.pub ‚Äî Feature Visualization**  
   [https://distill.pub/2017/feature-visualization/](https://distill.pub/2017/feature-visualization/) ‚Äî C√≥mo visualizar lo que aprenden las CNNs

10. **FastAI Practical Deep Learning**  
    [https://course.fast.ai/](https://course.fast.ai/) ‚Äî Enfoque pr√°ctico

### Libros

11. **Goodfellow, Bengio & Courville** ‚Äî *Deep Learning* (2016), Cap√≠tulo 9: Convolutional Networks  
    Disponible gratuitamente en: [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)

12. **Chollet** ‚Äî *Deep Learning with Python* (2nd ed., 2021), Manning

13. **Zhang et al.** ‚Äî *Dive into Deep Learning*  
    [https://d2l.ai/](https://d2l.ai/) ‚Äî Libro interactivo con c√≥digo ejecutable

### Herramientas y Repositorios

14. **torchvision.models** ‚Äî Implementaciones oficiales de arquitecturas CNN  
    [https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)

15. **timm (PyTorch Image Models)** ‚Äî Biblioteca con +600 modelos pre-entrenados  
    `pip install timm` | [https://github.com/rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)

16. **Netron** ‚Äî Visualizador de arquitecturas de redes neuronales  
    [https://netron.app/](https://netron.app/)

---

## üéì Notas Finales

### Lo Que Debes Recordar

Las CNNs revolucionaron la visi√≥n computacional por tres principios simples pero poderosos:

```
1. CONECTIVIDAD LOCAL
   Cada neurona ve s√≥lo una peque√±a regi√≥n de la entrada.
   ‚Üí Aprovecha que los p√≠xeles cercanos est√°n correlacionados.

2. COMPARTICI√ìN DE PESOS
   El mismo filtro se aplica a toda la imagen.
   ‚Üí 150M par√°metros (densa) vs 60K par√°metros (CNN) para 224√ó224.

3. JERARQU√çA DE CARACTER√çSTICAS
   Capas apiladas construyen representaciones cada vez m√°s abstractas.
   ‚Üí Capa 1: bordes ‚Üí Capa 5: orejas de gato ‚Üí Capa 10: gatos completos.
```

### Conceptos Clave por Recapitular

| Concepto | F√≥rmula/Definici√≥n |
|---|---|
| Dimensi√≥n salida | `(H - K + 2P) / S + 1` |
| Par√°metros conv | `(Kh √ó Kw √ó Cin + 1) √ó Nfiltros` |
| Padding "same" | `P = (K - 1) / 2` (con stride=1) |
| Campo receptivo | Se duplica aproximadamente con cada capa conv 3√ó3 |
| Skip connection | `F(x) = H(x) - x` ‚Üí Aprende el residuo |
| Global Avg Pool | `C√óH√óW ‚Üí C` (sin par√°metros) |

### Relaci√≥n con Laboratorios Anteriores

Este laboratorio integra conceptos de todos los laboratorios previos:

- **Labs 01-02**: Neuronas y redes ‚Üí ahora organizadas en convoluciones
- **Lab 03**: ReLU, la activaci√≥n m√°s usada en CNNs
- **Lab 04**: Cross-Entropy loss ‚Üí funci√≥n de p√©rdida est√°ndar para clasificaci√≥n
- **Lab 05**: Backpropagation ‚Üí ahora a trav√©s de capas convolucionales
- **Lab 06**: Entrenamiento, SGD, Adam ‚Üí aplicados aqu√≠ con scheduler
- **Lab 07**: M√©tricas ‚Üí precisi√≥n, confusi√≥n matrix, Precision/Recall
- **Lab 08**: PyTorch ‚Üí framework usado en todo este lab
- **Lab 09**: IA Generativa ‚Üí CNNs son el backbone de GANs y Diffusion Models

### Pr√≥ximos Pasos

Despu√©s de dominar las CNNs, el siguiente laboratorio explora:

> üëâ **[Lab 11: Redes Neuronales Recurrentes y LSTM](../Lab11_Redes_Neuronales_Recurrentes_LSTM/)**
> 
> Aprender√°s arquitecturas dise√±adas para **datos secuenciales**: texto, audio, series de tiempo y video. Las RNNs y LSTMs procesan la dimensi√≥n temporal de la misma manera que las CNNs procesan la dimensi√≥n espacial.

---

## ‚úÖ Checklist de Verificaci√≥n

Antes de entregar, verifica que has completado todo:

### Parte 1 ‚Äî Convoluci√≥n
- [ ] Implement√© `convolve2d_manual` con padding y stride funcionando correctamente
- [ ] Apliqu√© al menos 4 filtros cl√°sicos y gener√© visualizaciones
- [ ] Verifiqu√© la f√≥rmula de dimensiones en todas las configuraciones de la tabla
- [ ] Entiendo la diferencia entre padding "valid" y "same"

### Parte 2 ‚Äî Capas CNN
- [ ] Implement√© `CapaConvolucional` con m√∫ltiples filtros y canales
- [ ] Implement√© `CapaPooling` para Max y Average pooling
- [ ] Constru√≠ el pipeline completo: Conv ‚Üí Pool ‚Üí Flatten ‚Üí Dense desde cero
- [ ] Puedo calcular par√°metros manualmente para cualquier configuraci√≥n

### Parte 3 ‚Äî CNN con PyTorch
- [ ] Defin√≠ `CNN_MNIST` con al menos 3 bloques convolucionales
- [ ] El modelo entrena correctamente y alcanza >97% en MNIST
- [ ] Guard√© el mejor modelo con `torch.save`
- [ ] Visualic√© filtros de la primera capa convolucional
- [ ] Visualic√© feature maps de activaci√≥n usando hooks

### Parte 4 ‚Äî Arquitecturas Avanzadas
- [ ] Implement√© `BloqueResidual` con skip connection funcional
- [ ] Analic√© la diferencia en gradientes con y sin skip connections
- [ ] Entiendo las dos estrategias de Transfer Learning
- [ ] S√© cu√°ndo usar Feature Extraction vs Fine-Tuning

### Benchmark y An√°lisis
- [ ] Compar√© CNN vs red densa en MNIST (precisi√≥n, par√°metros, tiempo)
- [ ] Calcul√© el campo receptivo para al menos 3 profundidades
- [ ] Gener√© y guard√© todas las figuras (filtros, feature maps, curvas)
- [ ] Escrib√≠ el reporte final con an√°lisis y conclusiones

### Ejercicios (m√≠nimo 2 por nivel)
- [ ] Ejercicio B√°sico 1: Filtros diagonales ‚úì
- [ ] Ejercicio B√°sico 2: C√°lculo de par√°metros ‚úì
- [ ] Ejercicio Intermedio 1: CNN en CIFAR-10 ‚úì
- [ ] Ejercicio Avanzado: uno a elecci√≥n ‚úì

### Calidad del C√≥digo
- [ ] C√≥digo comentado y organizado en funciones/clases
- [ ] Variables con nombres descriptivos en espa√±ol o ingl√©s (consistente)
- [ ] Sin c√≥digo de debug o celdas con errores en el notebook final
- [ ] Todas las figuras tienen t√≠tulo, etiquetas de ejes y leyenda

---

*Gu√≠a desarrollada para el curso de Redes Neuronales ‚Äî Lab 10: CNNs*  
*Conecta hacia atr√°s con Labs 01-09 y hacia adelante con Lab 11 (RNNs/LSTM) y Lab 12 (Transformers)*
